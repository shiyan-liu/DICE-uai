#!/usr/bin/env python3
"""
DICEå‡†ç¡®ç‡è¯„ä¼°è„šæœ¬
ç”¨äºéªŒè¯DICEç³»ç»Ÿçš„å¯ä¿¡åº¦ï¼Œé€šè¿‡ä¸äººå·¥æ ‡æ³¨çš„"é‡‘æ ‡å‡†"è¿›è¡Œå¯¹æ¯”
"""

import json
import random
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple
from collections import defaultdict
from datetime import datetime
import numpy as np
from sklearn.metrics import cohen_kappa_score
from scipy.stats import kendalltau
import pandas as pd

from src.dice.dice_simplified import SimplifiedDICEEvaluator, SimplifiedDICEConfig
from ragas_evaluator import RagasEvaluator, RagasConfig, RagasValidationEvaluator


class DICEValidationEvaluator:
    """DICEéªŒè¯è¯„ä¼°å™¨ - ç”¨äºè¯„ä¼°DICEæœ¬èº«çš„å‡†ç¡®æ€§"""
    
    def __init__(self, config: SimplifiedDICEConfig, tournament_result_file: str = None):
        self.config = config
        self.logger = logging.getLogger("DICEValidation")
        self.dice_evaluator = SimplifiedDICEEvaluator(config)
        self.tournament_result_file = tournament_result_file
        self.tournament_results = None
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logger()
        
        # å¦‚æœæä¾›äº†tournamentç»“æœæ–‡ä»¶ï¼Œåˆ™åŠ è½½å®ƒ
        if self.tournament_result_file and Path(self.tournament_result_file).exists():
            self._load_tournament_results()
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def _load_tournament_results(self):
        """åŠ è½½tournamentç»“æœæ–‡ä»¶"""
        try:
            self.logger.info(f"å¼€å§‹åŠ è½½tournamentç»“æœæ–‡ä»¶: {self.tournament_result_file}")
            with open(self.tournament_result_file, 'r', encoding='utf-8') as f:
                self.tournament_results = json.load(f)
            self.logger.info(f"æˆåŠŸåŠ è½½tournamentç»“æœæ–‡ä»¶ï¼ŒåŒ…å« {len(self.tournament_results.get('swiss_results', {}).get('match_records', []))} ä¸ªå¯¹å†³è®°å½•")
        except Exception as e:
            self.logger.error(f"åŠ è½½tournamentç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            self.tournament_results = None
    
    def _find_tournament_match(self, system_a: str, system_b: str, question: str) -> Dict[str, Any]:
        """åœ¨tournamentç»“æœä¸­æŸ¥æ‰¾åŒ¹é…çš„å¯¹å†³"""
        if not self.tournament_results:
            return None
        
        # æŸ¥æ‰¾åŒ¹é…çš„ç³»ç»Ÿå¯¹
        match_records = self.tournament_results.get('swiss_results', {}).get('match_records', [])
        
        for match in match_records:
            match_system_a = match.get('system_a', '')
            match_system_b = match.get('system_b', '')
            
            # æ£€æŸ¥ç³»ç»Ÿå¯¹æ˜¯å¦åŒ¹é…ï¼ˆè€ƒè™‘é¡ºåºï¼‰
            if ((match_system_a == system_a and match_system_b == system_b) or 
                (match_system_a == system_b and match_system_b == system_a)):
                
                # åœ¨comparisonç»“æœä¸­æŸ¥æ‰¾åŒ¹é…çš„é—®é¢˜
                comparison = match.get('comparison', {})
                question_results = comparison.get('question_results', [])
                
                for q_result in question_results:
                    if q_result.get('question', '') == question:
                        return q_result
        
        return None
    
    def sample_evaluation_pairs(self, qacg_files: List[str], num_samples: int = 200, 
                               random_seed: int = 42) -> List[Dict[str, Any]]:
        """é‡‡æ ·è¯„ä¼°å¯¹"""
        import random
        random.seed(random_seed)
        
        all_pairs = []
        for file_path in qacg_files:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_pairs.extend(data)
        
        if len(all_pairs) < num_samples:
            self.logger.warning(f"å¯ç”¨æ•°æ®å¯¹æ•°é‡({len(all_pairs)})å°‘äºè¯·æ±‚çš„é‡‡æ ·æ•°é‡({num_samples})")
            return all_pairs
        
        return random.sample(all_pairs, num_samples)
    
    def run_dice_evaluation(self, evaluation_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡ŒDICEè¯„ä¼°"""
        results = []
        
        for i, pair in enumerate(evaluation_pairs):
            try:
                # ä»QACGæ ¼å¼ä¸­æå–é—®ç­”å¯¹
                qa_a = pair.get('qa_a', {})
                qa_b = pair.get('qa_b', {})
                
                question = qa_a.get('question', '')
                system_a = pair.get('system_a', '')
                system_b = pair.get('system_b', '')
                
                # é¦–å…ˆå°è¯•ä»tournamentç»“æœä¸­æŸ¥æ‰¾åŒ¹é…
                tournament_match = self._find_tournament_match(system_a, system_b, question)
                
                if tournament_match:
                    # ä½¿ç”¨tournamentä¸­çš„å·²æœ‰ç»“æœ
                    self.logger.info(f"ä½¿ç”¨tournamentç»“æœ: {system_a} vs {system_b} - {question[:50]}...")
                    
                    passage_judgment = tournament_match.get('passage_judgment', {})
                    score_a = passage_judgment.get('prob_a', 0.0)
                    score_b = passage_judgment.get('prob_b', 0.0)
                    dice_score = score_a - score_b
                    
                    result = {
                        'index': i,
                        'question': question,
                        'system_a': system_a,
                        'system_b': system_b,
                        'answer_a': qa_a.get('rag_answer', ''),
                        'answer_b': qa_b.get('rag_answer', ''),
                        'context_a': qa_a.get('context', []),
                        'context_b': qa_b.get('context', []),
                        'dice_score': dice_score,
                        'dice_explanation': passage_judgment.get('reason', ''),
                        'human_annotation': pair.get('human_annotation', ''),
                        'prob_a': score_a,
                        'prob_b': score_b,
                        'win_type': passage_judgment.get('win_type', 'Unknown'),
                        'source': 'tournament'  # æ ‡è®°æ¥æº
                    }
                else:
                    # æ²¡æœ‰æ‰¾åˆ°tournamentç»“æœï¼Œè¿›è¡Œæ–°çš„æ¨ç†
                    self.logger.info(f"æœªæ‰¾åˆ°tournamentç»“æœï¼Œè¿›è¡Œæ–°æ¨ç†: {system_a} vs {system_b} - {question[:50]}...")
                    
                    # æ„å»ºé—®ç­”å¯¹æ ¼å¼
                    target_qa_a = {
                        'answer': qa_a.get('rag_answer', ''),
                        'context': qa_a.get('context', [])
                    }
                    
                    target_qa_b = {
                        'answer': qa_b.get('rag_answer', ''),
                        'context': qa_b.get('context', [])
                    }
                    
                    # ä½¿ç”¨DICEçš„pairwise judgeè¿›è¡Œè¯„ä¼°
                    judgment = self.dice_evaluator.pairwise_judge.judge_pair(
                        question=question,
                        qa_a=target_qa_a,
                        qa_b=target_qa_b,
                        granularity="passage"  # ä½¿ç”¨passageç²’åº¦è¿›è¡Œè¯„ä¼°
                    )
                    
                    # ä»åˆ¤å†³ç»“æœä¸­æå–åˆ†æ•°
                    passage_judgment = judgment.get('passage_judgment', {})
                    score_a = passage_judgment.get('prob_a', 0.0)
                    score_b = passage_judgment.get('prob_b', 0.0)
                    
                    # è®¡ç®—ç›¸å¯¹åˆ†æ•°ï¼ˆç³»ç»ŸAç›¸å¯¹äºç³»ç»ŸBçš„ä¼˜åŠ¿ï¼‰
                    dice_score = score_a - score_b
                    
                    result = {
                        'index': i,
                        'question': question,
                        'system_a': system_a,
                        'system_b': system_b,
                        'answer_a': qa_a.get('rag_answer', ''),
                        'answer_b': qa_b.get('rag_answer', ''),
                        'context_a': qa_a.get('context', []),
                        'context_b': qa_b.get('context', []),
                        'dice_score': dice_score,
                        'dice_explanation': passage_judgment.get('reason', ''),
                        'human_annotation': pair.get('human_annotation', ''),
                        'prob_a': score_a,
                        'prob_b': score_b,
                        'win_type': passage_judgment.get('win_type', 'Unknown'),
                        'source': 'new_inference'  # æ ‡è®°æ¥æº
                    }
                
                results.append(result)
                
                if (i + 1) % 10 == 0:
                    self.logger.info(f"å·²å®Œæˆ {i + 1}/{len(evaluation_pairs)} ä¸ªè¯„ä¼°")
                    
            except Exception as e:
                self.logger.error(f"è¯„ä¼°ç¬¬{i}ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
                # æ·»åŠ ä¸€ä¸ªé»˜è®¤ç»“æœ
                result = {
                    'index': i,
                    'question': pair.get('qa_a', {}).get('question', ''),
                    'system_a': pair.get('system_a', ''),
                    'system_b': pair.get('system_b', ''),
                    'answer_a': pair.get('qa_a', {}).get('rag_answer', ''),
                    'answer_b': pair.get('qa_b', {}).get('rag_answer', ''),
                    'context_a': pair.get('qa_a', {}).get('context', []),
                    'context_b': pair.get('qa_b', {}).get('context', []),
                    'dice_score': 0.0,
                    'dice_explanation': f'è¯„ä¼°å‡ºé”™: {str(e)}',
                    'human_annotation': pair.get('human_annotation', ''),
                    'prob_a': 0.0,
                    'prob_b': 0.0,
                    'win_type': 'Error',
                    'source': 'error'
                }
                results.append(result)
                continue
        
        return results
    
    def load_human_annotations(self, annotation_file: str) -> Dict[int, str]:
        """åŠ è½½äººå·¥æ ‡æ³¨"""
        annotations = {}
        try:
            with open(annotation_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data:
                    if 'index' in item and 'human_annotation' in item:
                        annotations[item['index']] = item['human_annotation']
        except Exception as e:
            self.logger.error(f"åŠ è½½äººå·¥æ ‡æ³¨æ–‡ä»¶å¤±è´¥: {e}")
        return annotations
    
    def calculate_agreement(self, results: List[Dict[str, Any]], gold_labels: Dict[int, str]) -> Dict[str, float]:
        """è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡"""
        dice_scores = []
        human_scores = []
        
        for result in results:
            if result['index'] in gold_labels:
                dice_scores.append(result['dice_score'])
                # å°†äººå·¥æ ‡æ³¨è½¬æ¢ä¸ºæ•°å€¼åˆ†æ•°
                human_annotation = gold_labels[result['index']]
                if human_annotation.lower() in ['a', 'system_a', 'good', 'correct', 'accurate']:
                    human_scores.append(1.0)  # ç³»ç»ŸAæ›´å¥½
                elif human_annotation.lower() in ['b', 'system_b', 'bad', 'incorrect', 'inaccurate']:
                    human_scores.append(-1.0)  # ç³»ç»ŸBæ›´å¥½
                else:
                    human_scores.append(0.0)  # å¹³å±€æˆ–ä¸­æ€§
        
        if len(dice_scores) == 0:
            return {'correlation': 0.0, 'kappa': 0.0}
        
        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        correlation = np.corrcoef(dice_scores, human_scores)[0, 1] if len(dice_scores) > 1 else 0.0
        
        # è®¡ç®—Cohen's Kappa (å°†åˆ†æ•°è½¬æ¢ä¸ºäºŒåˆ†ç±»)
        dice_binary = [1 if score > 0 else 0 for score in dice_scores]  # æ­£æ•°è¡¨ç¤ºAæ›´å¥½
        human_binary = [1 if score > 0 else 0 for score in human_scores]  # æ­£æ•°è¡¨ç¤ºAæ›´å¥½
        kappa = cohen_kappa_score(dice_binary, human_binary) if len(dice_scores) > 1 else 0.0
        
        return {
            'correlation': correlation,
            'kappa': kappa,
            'sample_size': len(dice_scores)
        }
    
    def calculate_elo_correlation(self, results: List[Dict[str, Any]], gold_labels: Dict[int, str]) -> Dict[str, float]:
        """è®¡ç®—ELOç›¸å…³æ€§"""
        # è¿™é‡Œå¯ä»¥å®ç°ELOè¯„åˆ†ç³»ç»Ÿçš„ç›¸å…³æ€§è®¡ç®—
        # æš‚æ—¶è¿”å›åŸºæœ¬çš„ç›¸å…³æ€§æŒ‡æ ‡
        return self.calculate_agreement(results, gold_labels)
    
    def analyze_disagreement_cases(self, results: List[Dict[str, Any]], gold_labels: Dict[int, str]) -> List[Dict[str, Any]]:
        """åˆ†æä¸ä¸€è‡´æ¡ˆä¾‹"""
        disagreement_cases = []
        
        for result in results:
            if result['index'] in gold_labels:
                dice_score = result['dice_score']
                human_annotation = gold_labels[result['index']]
                
                # åˆ¤æ–­æ˜¯å¦ä¸ä¸€è‡´
                dice_a_better = dice_score > 0  # DICEè®¤ä¸ºç³»ç»ŸAæ›´å¥½
                human_a_better = human_annotation.lower() in ['a', 'system_a', 'good', 'correct', 'accurate']
                
                if dice_a_better != human_a_better:
                    disagreement_cases.append({
                        'index': result['index'],
                        'question': result['question'],
                        'system_a': result.get('system_a', ''),
                        'system_b': result.get('system_b', ''),
                        'answer_a': result.get('answer_a', ''),
                        'answer_b': result.get('answer_b', ''),
                        'dice_score': dice_score,
                        'human_annotation': human_annotation,
                        'disagreement_type': 'dice_a_better_human_b_better' if dice_a_better else 'dice_b_better_human_a_better'
                    })
        
        return disagreement_cases
    
    def print_disagreement_analysis(self, disagreement_cases: List[Dict[str, Any]]) -> None:
        """æ‰“å°ä¸ä¸€è‡´åˆ†æ"""
        if not disagreement_cases:
            self.logger.info("æ²¡æœ‰å‘ç°ä¸ä¸€è‡´æ¡ˆä¾‹")
            return
        
        self.logger.info(f"å‘ç° {len(disagreement_cases)} ä¸ªä¸ä¸€è‡´æ¡ˆä¾‹:")
        
        for case in disagreement_cases[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            self.logger.info(f"æ¡ˆä¾‹ {case['index']}: DICEåˆ†æ•°={case['dice_score']:.3f}, äººå·¥æ ‡æ³¨={case['human_annotation']}")
            self.logger.info(f"é—®é¢˜: {case['question'][:100]}...")
    
    def generate_validation_report(self, results: List[Dict[str, Any]], gold_labels: Dict[int, str]) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        agreement_metrics = self.calculate_agreement(results, gold_labels)
        disagreement_cases = self.analyze_disagreement_cases(results, gold_labels)
        
        report = {
            'total_samples': len(results),
            'annotated_samples': len([r for r in results if r['index'] in gold_labels]),
            'agreement_metrics': agreement_metrics,
            'disagreement_count': len(disagreement_cases),
            'disagreement_rate': len(disagreement_cases) / len(results) if results else 0.0,
            'dice_scores_summary': {
                'mean': np.mean([r['dice_score'] for r in results]) if results else 0.0,
                'std': np.std([r['dice_score'] for r in results]) if results else 0.0,
                'min': min([r['dice_score'] for r in results]) if results else 0.0,
                'max': max([r['dice_score'] for r in results]) if results else 0.0
            }
        }
        
        return report


class UnifiedValidationEvaluator:
    """ç»Ÿä¸€éªŒè¯è¯„ä¼°å™¨ - æ”¯æŒDICEå’ŒRAGASä¸¤ç§è¯„ä¼°æ–¹æ³•"""
    
    def __init__(self, evaluation_method: str = "dice", dice_config: SimplifiedDICEConfig = None, 
                 ragas_config: RagasConfig = None, tournament_result_file: str = None):
        self.evaluation_method = evaluation_method.lower()
        self.logger = logging.getLogger("UnifiedValidation")
        
        # æ ¹æ®è¯„ä¼°æ–¹æ³•åˆå§‹åŒ–ç›¸åº”çš„è¯„ä¼°å™¨
        if self.evaluation_method == "dice":
            if dice_config is None:
                raise ValueError("ä½¿ç”¨DICEæ–¹æ³•æ—¶å¿…é¡»æä¾›dice_config")
            self.evaluator = DICEValidationEvaluator(dice_config, tournament_result_file)
        elif self.evaluation_method == "ragas":
            if ragas_config is None:
                raise ValueError("ä½¿ç”¨RAGASæ–¹æ³•æ—¶å¿…é¡»æä¾›ragas_config")
            self.evaluator = RagasValidationEvaluator(ragas_config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯„ä¼°æ–¹æ³•: {evaluation_method}")
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logger()
        
        self.logger.info(f"åˆå§‹åŒ–ç»Ÿä¸€éªŒè¯è¯„ä¼°å™¨ï¼Œä½¿ç”¨æ–¹æ³•: {self.evaluation_method.upper()}")
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def _derive_dice_label(self, result: Dict[str, Any]) -> str:
        """ç»Ÿä¸€æ¨æ–­DICEæ ‡ç­¾çš„é€»è¾‘ï¼Œé¿å…åˆ†å€¼å°ºåº¦è¯¯åˆ¤å¯¼è‡´ç»Ÿè®¡é”™è¯¯ã€‚"""
        explicit_label = result.get("dice_judgment")
        if explicit_label in {"A wins", "B wins", "Tie"}:
            return explicit_label
        
        score = result.get("dice_score")
        if isinstance(score, (int, float)):
            # è‹¥æ˜¯[0,1]åˆ†å°ºåº¦ï¼Œåˆ™ä»¥0.5ä¸ºä¸­æ€§é˜ˆå€¼ï¼ŒåŠ å…¥è½»å¾®ç¼“å†²
            if 0.0 <= score <= 1.0:
                if score > 0.55:
                    return "A wins"
                if score < 0.45:
                    return "B wins"
                return "Tie"
            # å¦åˆ™è§†ä¸ºå¯¹ç§°åˆ†åˆ¶ï¼ˆå¦‚[-1,1]ï¼‰ï¼Œä»¥0ä¸ºä¸­æ€§é˜ˆå€¼ï¼ŒåŠ å…¥è½»å¾®ç¼“å†²
            if score > 0.1:
                return "A wins"
            if score < -0.1:
                return "B wins"
            return "Tie"
        
        # å›é€€ï¼šè‹¥æœ‰prob_a/prob_bå¯æ¯”è¾ƒ
        prob_a = result.get("prob_a")
        prob_b = result.get("prob_b")
        if isinstance(prob_a, (int, float)) and isinstance(prob_b, (int, float)):
            delta = prob_a - prob_b
            if delta > 0.05:
                return "A wins"
            if delta < -0.05:
                return "B wins"
            return "Tie"
        
        return "Tie"
        
    def sample_evaluation_pairs(self, qacg_files: List[str], num_samples: int = 200, 
                               random_seed: int = 42) -> List[Dict[str, Any]]:
        """
        ä»70é¢˜ä¸­éšæœºæŠ½å–200å¯¹(q, cA, aA, cB, aB)ç”¨äºäººå·¥æ ‡æ³¨
        
        Args:
            qacg_files: QACGæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            num_samples: é‡‡æ ·æ•°é‡
            random_seed: éšæœºç§å­
            
        Returns:
            é‡‡æ ·çš„è¯„ä¼°å¯¹åˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹é‡‡æ · {num_samples} å¯¹è¯„ä¼°æ ·æœ¬")
        random.seed(random_seed)
        
        # åŠ è½½æ‰€æœ‰ç³»ç»Ÿæ•°æ®
        all_systems_data = {}
        for file_path in qacg_files:
            system_name = Path(file_path).stem.replace("qacg_", "")
            with open(file_path, 'r', encoding='utf-8') as f:
                all_systems_data[system_name] = json.load(f)
        
        systems = list(all_systems_data.keys())
        if len(systems) < 2:
            raise ValueError(f"éœ€è¦è‡³å°‘2ä¸ªç³»ç»Ÿï¼Œå®é™…è·å¾—{len(systems)}ä¸ª")
        
        self.logger.info(f"åŠ è½½äº† {len(systems)} ä¸ªç³»ç»Ÿ: {systems}")
        
        # ç¡®å®šæ•°æ®é•¿åº¦ï¼ˆä½¿ç”¨æœ€çŸ­çš„ç³»ç»Ÿæ•°æ®é•¿åº¦ï¼‰
        min_length = min(len(data) for data in all_systems_data.values())
        self.logger.info(f"æ¯ä¸ªç³»ç»Ÿæœ‰ {min_length} é¢˜æ•°æ®")
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç³»ç»Ÿå¯¹å’Œé¢˜ç›®ç»„åˆ
        all_combinations = []
        for i, system_a in enumerate(systems):
            for j, system_b in enumerate(systems):
                if i < j:  # é¿å…é‡å¤å¯¹æ¯”
                    for q_idx in range(min_length):
                        qa_a = all_systems_data[system_a][q_idx]
                        qa_b = all_systems_data[system_b][q_idx]
                        
                        # ç¡®ä¿ä¸¤ä¸ªç³»ç»Ÿå›ç­”çš„æ˜¯åŒä¸€ä¸ªé—®é¢˜
                        if qa_a["question"] == qa_b["question"]:
                            combination = {
                                "question_idx": q_idx,
                                "system_a": system_a,
                                "system_b": system_b,
                                "qa_a": qa_a,
                                "qa_b": qa_b,
                                "question": qa_a["question"],
                                "answer_a": qa_a.get("rag_answer", ""),
                                "answer_b": qa_b.get("rag_answer", ""),
                                "expected_answer": qa_a.get("expected_answer", ""),
                                "context_a": qa_a.get("context", []),
                                "context_b": qa_b.get("context", []),
                                "groundtruth": qa_a.get("groundtruth", qa_a.get("expected_answer", ""))
                            }
                            all_combinations.append(combination)
        
        self.logger.info(f"æ€»å…±æœ‰ {len(all_combinations)} ä¸ªå¯èƒ½çš„ç»„åˆ")
        
        # éšæœºé‡‡æ ·
        if len(all_combinations) < num_samples:
            self.logger.warning(f"å¯ç”¨ç»„åˆæ•° ({len(all_combinations)}) å°‘äºéœ€æ±‚æ ·æœ¬æ•° ({num_samples})")
            sampled_pairs = all_combinations
        else:
            sampled_pairs = random.sample(all_combinations, num_samples)
        
        self.logger.info(f"æˆåŠŸé‡‡æ · {len(sampled_pairs)} å¯¹è¯„ä¼°æ ·æœ¬")
        return sampled_pairs
    
    def run_evaluation(self, evaluation_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡Œç›¸åº”çš„è¯„ä¼°æ–¹æ³•"""
        if self.evaluation_method == "dice":
            return self.run_dice_evaluation(evaluation_pairs)
        elif self.evaluation_method == "ragas":
            return self.evaluator.run_ragas_evaluation(evaluation_pairs)
    
    def load_human_annotations(self, annotation_file: str) -> Dict[int, str]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„æ ‡æ³¨åŠ è½½æ–¹æ³•"""
        return self.evaluator.load_human_annotations(annotation_file)
    
    def calculate_agreement(self, results: List[Dict[str, Any]], 
                          gold_labels: Dict[int, str]) -> Dict[str, float]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„ä¸€è‡´æ€§è®¡ç®—æ–¹æ³•"""
        return self.evaluator.calculate_agreement(results, gold_labels)
    
    def calculate_elo_correlation(self, results: List[Dict[str, Any]], 
                                gold_labels: Dict[int, str]) -> Dict[str, float]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„Eloç›¸å…³æ€§è®¡ç®—æ–¹æ³•"""
        return self.evaluator.calculate_elo_correlation(results, gold_labels)
    
    def analyze_disagreement_cases(self, results: List[Dict[str, Any]], 
                                  gold_labels: Dict[int, str]) -> List[Dict[str, Any]]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„åˆ†æ­§åˆ†ææ–¹æ³•"""
        return self.evaluator.analyze_disagreement_cases(results, gold_labels)
    
    def print_disagreement_analysis(self, disagreement_cases: List[Dict[str, Any]]):
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„åˆ†æ­§æ‰“å°æ–¹æ³•"""
        return self.evaluator.print_disagreement_analysis(disagreement_cases)
    
    def generate_validation_report(self, agreement_metrics: Dict[str, Any], 
                                 correlation_metrics: Dict[str, Any],
                                 results: List[Dict[str, Any]],
                                 gold_labels: Dict[int, str],
                                 output_file: str):
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„æŠ¥å‘Šç”Ÿæˆæ–¹æ³•"""
        return self.evaluator.generate_validation_report(
            agreement_metrics, correlation_metrics, results, gold_labels, output_file
        )
    
    def run_dice_evaluation(self, evaluation_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨DICEè¯„ä¼°æ‰€æœ‰é‡‡æ ·çš„å¯¹æ¯”å¯¹
        
        Args:
            evaluation_pairs: è¯„ä¼°å¯¹åˆ—è¡¨
            
        Returns:
            DICEè¯„ä¼°ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹DICEè¯„ä¼° {len(evaluation_pairs)} å¯¹æ ·æœ¬")
        
        dice_results = []
        for i, pair in enumerate(evaluation_pairs):
            self.logger.info(f"è¯„ä¼°ç¬¬ {i+1}/{len(evaluation_pairs)} å¯¹")
            
            # ä½¿ç”¨DICEè¿›è¡Œè¯„ä¼°
            qa_a = pair["qa_a"]
            qa_b = pair["qa_b"]
            
            # ä½¿ç”¨DICEè¯„ä¼°å™¨çš„_pairwise_comparisonæ–¹æ³•
            result = self.evaluator.dice_evaluator._pairwise_comparison(
                [qa_a], [qa_b], 
                pair["system_a"], pair["system_b"],
                max_questions=1
            )
            
            # æå–å…³é”®ä¿¡æ¯
            if result["question_results"]:
                question_result = result["question_results"][0]
                passage_judgment = question_result.get("passage_judgment", {})
                
                dice_result = {
                    "pair_id": i,  # ä½¿ç”¨ç´¢å¼•ä½œä¸ºpair_idï¼Œä¸æ ‡æ³¨æ¨¡æ¿ä¿æŒä¸€è‡´
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "system_b": pair["system_b"],
                    "dice_judgment": passage_judgment.get("label", "Tie"),
                    "dice_score": passage_judgment.get("score", 0.5),
                    "dice_reason": passage_judgment.get("reason", ""),
                    "dice_margin_score": passage_judgment.get("margin_score", 0.0),
                    "combined_delta": question_result.get("elo_delta", 0.0),
                    "original_pair": pair
                }
            else:
                # å¤‡ç”¨ç»“æœ
                dice_result = {
                    "pair_id": i,  # ä½¿ç”¨ç´¢å¼•ä½œä¸ºpair_idï¼Œä¸æ ‡æ³¨æ¨¡æ¿ä¿æŒä¸€è‡´
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "system_b": pair["system_b"],
                    "dice_judgment": "Tie",
                    "dice_score": 0.5,
                    "dice_reason": "è¯„ä¼°å¤±è´¥",
                    "dice_margin_score": 0.0,
                    "combined_delta": 0.0,
                    "original_pair": pair
                }
            
            dice_results.append(dice_result)
        
        return dice_results
    
    def load_human_annotations(self, annotation_file: str) -> Dict[int, str]:
        """
        åŠ è½½äººå·¥æ ‡æ³¨ç»“æœ
        
        Args:
            annotation_file: äººå·¥æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[pair_id, gold_label]: é‡‘æ ‡å‡†æ ‡æ³¨
        """
        self.logger.info(f"åŠ è½½äººå·¥æ ‡æ³¨: {annotation_file}")
        
        if not Path(annotation_file).exists():
            self.logger.error(f"æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {annotation_file}")
            # åˆ›å»ºç¤ºä¾‹æ ‡æ³¨æ–‡ä»¶
            self._create_annotation_template(annotation_file)
            raise FileNotFoundError(f"è¯·å®Œæˆäººå·¥æ ‡æ³¨åé‡æ–°è¿è¡Œ: {annotation_file}")
        
        with open(annotation_file, 'r', encoding='utf-8') as f:
            annotation_data = json.load(f)
        
        # è·å–å®é™…çš„æ ‡æ³¨æ•°ç»„
        if isinstance(annotation_data, dict) and "annotations" in annotation_data:
            annotations = annotation_data["annotations"]
        else:
            annotations = annotation_data
        
        # è½¬æ¢ä¸ºç®€å•çš„dictæ ¼å¼
        gold_labels = {}
        for item in annotations:
            pair_id = item["pair_id"]
            # ä½¿ç”¨å¤šæ•°ç¥¨å†³å®šé‡‘æ ‡å‡†
            votes = item["expert_votes"]
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŠ•ç¥¨éƒ½ä¸ºç©º
            valid_votes = [vote for vote in votes if vote and vote.strip()]
            if not valid_votes:
                self.logger.warning(f"pair_id {pair_id} çš„expert_votesä¸ºç©ºï¼Œè·³è¿‡æ­¤é¡¹")
                continue
            
            # æ£€æŸ¥æŠ•ç¥¨æ˜¯å¦æœ‰æ•ˆ
            valid_labels = {"A wins", "B wins", "Tie"}
            filtered_votes = [vote for vote in valid_votes if vote in valid_labels]
            if not filtered_votes:
                self.logger.warning(f"pair_id {pair_id} æ²¡æœ‰æœ‰æ•ˆçš„æŠ•ç¥¨æ ‡ç­¾ï¼Œè·³è¿‡æ­¤é¡¹")
                continue
            
            vote_counts = defaultdict(int)
            for vote in filtered_votes:
                vote_counts[vote] += 1
            gold_label = max(vote_counts.items(), key=lambda x: x[1])[0]
            gold_labels[pair_id] = gold_label
        
        self.logger.info(f"åŠ è½½äº† {len(gold_labels)} ä¸ªé‡‘æ ‡å‡†æ ‡æ³¨")
        
        if len(gold_labels) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ ‡æ³¨æ•°æ®ã€‚è¯·ç¡®ä¿ï¼š\n"
                           "1. expert_votesä¸ä¸ºç©º\n"
                           "2. æŠ•ç¥¨å€¼ä¸º 'A wins'ã€'B wins' æˆ– 'Tie'\n"
                           "3. è‡³å°‘æœ‰ä¸€ä½ä¸“å®¶å®Œæˆäº†æ ‡æ³¨")
        
        return gold_labels
    
    def _create_annotation_template(self, annotation_file: str):
        """åˆ›å»ºäººå·¥æ ‡æ³¨æ¨¡æ¿æ–‡ä»¶"""
        self.logger.info(f"åˆ›å»ºæ ‡æ³¨æ¨¡æ¿: {annotation_file}")
        
        template = {
            "instructions": "è¯·3ä½ä¸“å®¶ç‹¬ç«‹å®Œæˆæ ‡æ³¨ï¼Œæ¯ä¸ªpair_idå¯¹åº”ä¸€ä¸ªè¯„ä¼°å¯¹ï¼Œè¯·ä¸ºæ¯ä½ä¸“å®¶åœ¨expert_votesä¸­å¡«å…¥ 'A wins'ã€'B wins' æˆ– 'Tie'",
            "annotation_guide": {
                "A wins": "ç³»ç»ŸAæ˜æ˜¾ä¼˜äºç³»ç»ŸB",
                "B wins": "ç³»ç»ŸBæ˜æ˜¾ä¼˜äºç³»ç»ŸA", 
                "Tie": "ä¸¤ä¸ªç³»ç»Ÿè¡¨ç°ç›¸å½“ï¼Œéš¾ä»¥åŒºåˆ†ä¼˜åŠ£"
            },
            "annotations": [
                {
                    "pair_id": 0,
                    "question": "ç¤ºä¾‹é—®é¢˜",
                    "system_a": "system_a_name",
                    "answer_a": "ç³»ç»ŸAçš„å›ç­”",
                    "system_b": "system_b_name", 
                    "answer_b": "ç³»ç»ŸBçš„å›ç­”",
                    "expert_votes": ["A wins", "B wins", "A wins"]  # 3ä½ä¸“å®¶çš„æŠ•ç¥¨
                }
            ]
        }
        
        with open(annotation_file, 'w', encoding='utf-8') as f:
            json.dump(template, f, ensure_ascii=False, indent=2)
    
    def calculate_agreement(self, dice_results: List[Dict[str, Any]], 
                          gold_labels: Dict[int, str]) -> Dict[str, float]:
        """
        è®¡ç®—DICEä¸é‡‘æ ‡å‡†çš„ä¸€è‡´æ€§
        
        Args:
            dice_results: DICEè¯„ä¼°ç»“æœ
            gold_labels: é‡‘æ ‡å‡†æ ‡æ³¨
            
        Returns:
            ä¸€è‡´æ€§æŒ‡æ ‡å­—å…¸
        """
        self.logger.info("è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡")
        
        # å‡†å¤‡æ•°æ®
        dice_labels = []
        human_labels = []
        
        for result in dice_results:
            # ä½¿ç”¨pair_idå­—æ®µ
            result_index = result.get("pair_id", result.get("index", -1))
            if result_index in gold_labels:
                # ç»Ÿä¸€ä»ç»“æœä¸­æ¨æ–­æ ‡ç­¾ï¼ˆå…¼å®¹ä¸åŒåˆ†æ•°å°ºåº¦ä¸ç»“æ„ï¼‰
                dice_label = self._derive_dice_label(result)
                dice_labels.append(dice_label)
                human_labels.append(gold_labels[result_index])
        
        if len(dice_labels) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ ‡æ³¨æ•°æ®")
        
        self.logger.info(f"åŒ¹é…åˆ° {len(dice_labels)} ä¸ªæ ‡æ³¨å¯¹")
        
        # è®¡ç®—Îºå€¼
        kappa = cohen_kappa_score(human_labels, dice_labels)
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = sum(1 for d, h in zip(dice_labels, human_labels) if d == h) / len(dice_labels)
        
        # åˆ†ç±»ç»Ÿè®¡
        label_stats = {}
        for label in ["A wins", "B wins", "Tie"]:
            human_count = human_labels.count(label)
            dice_count = dice_labels.count(label)
            label_stats[label] = {
                "human_count": human_count,
                "dice_count": dice_count,
                "agreement": sum(1 for d, h in zip(dice_labels, human_labels) 
                              if d == h == label) if human_count > 0 else 0
            }
        
        return {
            "kappa": kappa,
            "accuracy": accuracy,
            "total_pairs": len(dice_labels),
            "label_statistics": label_stats
        }
    
    def calculate_elo_correlation(self, dice_results: List[Dict[str, Any]], 
                                gold_labels: Dict[int, str]) -> Dict[str, float]:
        """
        è®¡ç®—DICE-Eloä¸äººå·¥-Eloæ’åºçš„ç›¸å…³æ€§
        
        Args:
            dice_results: DICEè¯„ä¼°ç»“æœ
            gold_labels: é‡‘æ ‡å‡†æ ‡æ³¨
            
        Returns:
            ç›¸å…³æ€§æŒ‡æ ‡å­—å…¸
        """
        self.logger.info("è®¡ç®—Eloæ’åºç›¸å…³æ€§")
        
        # æ”¶é›†æ‰€æœ‰ç³»ç»Ÿ
        all_systems = set()
        for result in dice_results:
            all_systems.add(result["system_a"])
            all_systems.add(result["system_b"])
        all_systems = list(all_systems)
        
        # è®¡ç®—DICE-Eloåˆ†æ•°
        dice_elo = {system: 1500.0 for system in all_systems}  # åˆå§‹Elo
        human_elo = {system: 1500.0 for system in all_systems}
        
        k_factor = 32
        
        for result in dice_results:
            # å…¼å®¹ä¸¤ç§æ•°æ®ç»“æ„ï¼šä½¿ç”¨pair_idæˆ–indexå­—æ®µ
            pair_id = result.get("pair_id", result.get("index", -1))
            if pair_id not in gold_labels:
                continue
                
            system_a = result["system_a"]
            system_b = result["system_b"]
            
            # DICEç»“æœ - ç»Ÿä¸€æ¨æ–­æ–¹å¼
            dice_judgment = self._derive_dice_label(result)
            
            if dice_judgment == "A wins":
                dice_score_a, dice_score_b = 1.0, 0.0
            elif dice_judgment == "B wins":
                dice_score_a, dice_score_b = 0.0, 1.0
            else:
                dice_score_a, dice_score_b = 0.5, 0.5
            
            # äººå·¥æ ‡æ³¨ç»“æœ
            human_label = gold_labels[pair_id]
            if human_label == "A wins":
                human_score_a, human_score_b = 1.0, 0.0
            elif human_label == "B wins":
                human_score_a, human_score_b = 0.0, 1.0
            else:
                human_score_a, human_score_b = 0.5, 0.5
            
            # æ›´æ–°DICE-Elo
            expected_a = 1 / (1 + 10**((dice_elo[system_b] - dice_elo[system_a]) / 400))
            dice_elo[system_a] += k_factor * (dice_score_a - expected_a)
            dice_elo[system_b] += k_factor * (dice_score_b - (1 - expected_a))
            
            # æ›´æ–°Human-Elo
            expected_a = 1 / (1 + 10**((human_elo[system_b] - human_elo[system_a]) / 400))
            human_elo[system_a] += k_factor * (human_score_a - expected_a)
            human_elo[system_b] += k_factor * (human_score_b - (1 - expected_a))
        
        # è®¡ç®—æ’åº
        dice_ranking = sorted(all_systems, key=lambda x: dice_elo[x], reverse=True)
        human_ranking = sorted(all_systems, key=lambda x: human_elo[x], reverse=True)
        
        # è®¡ç®—Kendall-Ï„ç›¸å…³æ€§
        dice_ranks = [dice_ranking.index(system) for system in all_systems]
        human_ranks = [human_ranking.index(system) for system in all_systems]
        
        tau, p_value = kendalltau(dice_ranks, human_ranks)
        
        return {
            "kendall_tau": tau,
            "p_value": p_value,
            "dice_elo_scores": dice_elo,
            "human_elo_scores": human_elo,
            "dice_ranking": dice_ranking,
            "human_ranking": human_ranking
        }
    
    def analyze_disagreement_cases(self, dice_results: List[Dict[str, Any]], 
                                  gold_labels: Dict[int, str]) -> List[Dict[str, Any]]:
        """
        åˆ†æDICEåˆ¤æ–­ä¸äººå·¥æ ‡æ³¨ä¸ä¸€è‡´çš„case
        
        Args:
            dice_results: DICEè¯„ä¼°ç»“æœ
            gold_labels: é‡‘æ ‡å‡†æ ‡æ³¨
            
        Returns:
            ä¸ä¸€è‡´çš„caseåˆ—è¡¨
        """
        self.logger.info("åˆ†æä¸ä¸€è‡´çš„case")
        
        disagreement_cases = []
        
        for result in dice_results:
            # å…¼å®¹ä¸¤ç§æ•°æ®ç»“æ„ï¼šä½¿ç”¨pair_idæˆ–indexå­—æ®µ
            pair_id = result.get("pair_id", result.get("index", -1))
            if pair_id in gold_labels:
                # ç»Ÿä¸€æ¨æ–­DICEåˆ¤å†³
                dice_judgment = self._derive_dice_label(result)
                
                human_judgment = gold_labels[pair_id]
                
                if dice_judgment != human_judgment:
                    # å…¼å®¹ä¸¤ç§æ•°æ®ç»“æ„ï¼šä½¿ç”¨original_pairæˆ–ç›´æ¥ä»resultè·å–
                    pair_data = result.get("original_pair", result)
                    case = {
                        "pair_id": pair_id,
                        "question": pair_data.get("question", result.get("question", "")),
                        "system_a": result["system_a"],
                        "system_b": result["system_b"],
                        "answer_a": pair_data.get("answer_a", result.get("answer_a", "")),
                        "answer_b": pair_data.get("answer_b", result.get("answer_b", "")),
                        "context_a": pair_data.get("context_a", result.get("context_a", []))[:2],  # åªæ˜¾ç¤ºå‰2ä¸ªcontext
                        "context_b": pair_data.get("context_b", result.get("context_b", []))[:2],
                        "groundtruth": pair_data.get("groundtruth", result.get("groundtruth", "")),
                        "dice_judgment": dice_judgment,
                        "dice_score": result.get("dice_score", 0.0),
                        "dice_reason": result.get("dice_reason", result.get("dice_explanation", "")),
                        "human_judgment": human_judgment,
                        "disagreement_type": f"DICE: {dice_judgment} vs Human: {human_judgment}"
                    }
                    disagreement_cases.append(case)
        
        self.logger.info(f"å‘ç° {len(disagreement_cases)} ä¸ªä¸ä¸€è‡´çš„case")
        return disagreement_cases
    
    def print_disagreement_analysis(self, disagreement_cases: List[Dict[str, Any]]):
        """
        æ‰“å°ä¸ä¸€è‡´caseçš„è¯¦ç»†åˆ†æ
        """
        if not disagreement_cases:
            print("\nâœ… æ‰€æœ‰caseéƒ½ä¸€è‡´ï¼Œæ²¡æœ‰å‘ç°åˆ†æ­§")
            return
        
        print(f"\nğŸ” å‘ç° {len(disagreement_cases)} ä¸ªä¸ä¸€è‡´çš„case:")
        print("="*80)
        
        for i, case in enumerate(disagreement_cases[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"\nğŸ“‹ Case {case['pair_id']+1} (pair_id: {case['pair_id']})")
            print(f"ğŸ”¥ åˆ†æ­§ç±»å‹: {case['disagreement_type']}")
            print(f"â“ é—®é¢˜: {case['question']}")
            print()
            
            print(f"ğŸ¤– ç³»ç»ŸA ({case['system_a']}):")
            print(f"   å›ç­”: {case['answer_a'][:300]}{'...' if len(case['answer_a']) > 200 else ''}")
            print()
            
            print(f"ğŸ¤– ç³»ç»ŸB ({case['system_b']}):")
            print(f"   å›ç­”: {case['answer_b'][:300]}{'...' if len(case['answer_b']) > 200 else ''}")
            print()
            
            print(f"ğŸ“ æ ‡å‡†ç­”æ¡ˆ: {case['groundtruth'][:200]}{'...' if len(case['groundtruth']) > 200 else ''}")
            print()
            
            print(f"ğŸ¯ DICEåˆ¤æ–­: {case['dice_judgment']} (ç½®ä¿¡åº¦: {case['dice_score']:.3f})")
            print(f"   ç†ç”±: {case['dice_reason'][:500]}{'...' if len(case['dice_reason']) > 500 else ''}")
            print()
            
            print(f"ğŸ‘¥ äººå·¥åˆ¤æ–­: {case['human_judgment']}")
            print("-" * 80)
        
        if len(disagreement_cases) > 10:
            print(f"\n... è¿˜æœ‰ {len(disagreement_cases) - 10} ä¸ªä¸ä¸€è‡´çš„caseæœªæ˜¾ç¤º")
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„åˆ†æ­§
        disagreement_stats = {}
        for case in disagreement_cases:
            disagreement_type = case['disagreement_type']
            disagreement_stats[disagreement_type] = disagreement_stats.get(disagreement_type, 0) + 1
        
        print(f"\nğŸ“Š åˆ†æ­§ç±»å‹ç»Ÿè®¡:")
        for disagreement_type, count in sorted(disagreement_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(disagreement_cases)) * 100
            print(f"   {disagreement_type}: {count} ä¸ª ({percentage:.1f}%)")

    def generate_validation_report(self, agreement_metrics: Dict[str, Any], 
                                 correlation_metrics: Dict[str, Any],
                                 dice_results: List[Dict[str, Any]],
                                 gold_labels: Dict[int, str],
                                 output_file: str):
        """
        ç”ŸæˆéªŒè¯æŠ¥å‘Š
        
        Args:
            agreement_metrics: ä¸€è‡´æ€§æŒ‡æ ‡
            correlation_metrics: ç›¸å…³æ€§æŒ‡æ ‡  
            dice_results: DICEè¯„ä¼°ç»“æœ
            gold_labels: é‡‘æ ‡å‡†æ ‡æ³¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self.logger.info(f"ç”ŸæˆéªŒè¯æŠ¥å‘Š: {output_file}")
        
        # åˆ†æä¸ä¸€è‡´çš„case
        disagreement_cases = self.analyze_disagreement_cases(dice_results, gold_labels)
        
        report = {
            "validation_summary": {
                "kappa_score": agreement_metrics["kappa"],
                "accuracy": agreement_metrics["accuracy"],
                "kendall_tau": correlation_metrics["kendall_tau"],
                "validation_passed": (
                    agreement_metrics["kappa"] >= 0.85 and 
                    correlation_metrics["kendall_tau"] >= 0.9
                )
            },
            "detailed_metrics": {
                "agreement_analysis": agreement_metrics,
                "correlation_analysis": correlation_metrics
            },
            "disagreement_analysis": {
                "total_disagreements": len(disagreement_cases),
                "disagreement_rate": len(disagreement_cases) / len(dice_results) if dice_results else 0,
                "sample_cases": disagreement_cases
            },
            "conclusion": self._generate_conclusion(agreement_metrics, correlation_metrics)
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‘˜è¦
        self._print_validation_summary(report)
        
        # æ‰“å°ä¸ä¸€è‡´caseåˆ†æ
        self.print_disagreement_analysis(disagreement_cases)
    
    def _generate_conclusion(self, agreement_metrics: Dict[str, Any], 
                           correlation_metrics: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»“è®º"""
        kappa = agreement_metrics["kappa"]
        tau = correlation_metrics["kendall_tau"]
        
        # æ£€æŸ¥æ˜¯å¦ä¸º2ç³»ç»Ÿçš„ç‰¹æ®Šæƒ…å†µ
        num_systems = len(correlation_metrics.get("dice_ranking", []))
        if num_systems == 2:
            if tau == -1.0:
                conclusion = "ğŸ“Š 2ç³»ç»ŸéªŒè¯ï¼šDICEä¸äººå·¥æ’åºå®Œå…¨ç›¸åï¼ˆÏ„=-1.0ï¼‰ã€‚"
                if kappa >= 0.6:
                    conclusion += f"ä½†Îºå€¼({kappa:.3f})è¡¨æ˜æ€»ä½“ä¸€è‡´æ€§å°šå¯ï¼Œå¯èƒ½å­˜åœ¨ç³»ç»Ÿåå¥½å·®å¼‚ã€‚"
                else:
                    conclusion += f"ä¸”Îºå€¼({kappa:.3f})è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥åˆ¤å†³é€»è¾‘æˆ–å¢åŠ æ›´å¤šç³»ç»Ÿè¿›è¡ŒéªŒè¯ã€‚"
                return conclusion
            elif tau == 1.0:
                return f"âœ… 2ç³»ç»ŸéªŒè¯ï¼šDICEä¸äººå·¥æ’åºå®Œå…¨ä¸€è‡´ï¼ˆÏ„=1.0ï¼‰ï¼ŒÎºå€¼={kappa:.3f}ã€‚"
        
        # æ ‡å‡†çš„å¤šç³»ç»Ÿè¯„ä¼°
        if kappa >= 0.85 and tau >= 0.9:
            return "âœ… DICEç³»ç»ŸéªŒè¯é€šè¿‡ï¼Îºå€¼å’ŒKendall-Ï„å‡è¾¾æ ‡ï¼Œç³»ç»Ÿå¯ä¿¡åº¦é«˜ï¼Œå¯ç”¨äºåç»­è¯„ä¼°ã€‚"
        elif kappa >= 0.85:
            return "âš ï¸ DICEç³»ç»Ÿéƒ¨åˆ†é€šè¿‡ã€‚Îºå€¼è¾¾æ ‡ä½†æ’åºç›¸å…³æ€§ä¸è¶³ï¼Œå»ºè®®æ£€æŸ¥Eloè®¡ç®—é€»è¾‘ã€‚"
        elif tau >= 0.9:
            return "âš ï¸ DICEç³»ç»Ÿéƒ¨åˆ†é€šè¿‡ã€‚æ’åºç›¸å…³æ€§è¾¾æ ‡ä½†ä¸€è‡´æ€§ä¸è¶³ï¼Œå»ºè®®æ£€æŸ¥åˆ¤å†³é€»è¾‘ã€‚"
        else:
            return "âŒ DICEç³»ç»ŸéªŒè¯å¤±è´¥ã€‚Îºå€¼å’ŒKendall-Ï„å‡æœªè¾¾æ ‡ï¼Œéœ€è¦é‡æ–°è°ƒæ•´è¯„ä¼°ç­–ç•¥ã€‚"
    
    def _print_validation_summary(self, report: Dict[str, Any]):
        """æ‰“å°éªŒè¯æ‘˜è¦"""
        summary = report["validation_summary"]
        
        print("\n" + "="*60)
        print("ğŸ”¬ DICEç³»ç»ŸéªŒè¯ç»“æœ")
        print("="*60)
        print(f"Îº å€¼ (ç›®æ ‡â‰¥0.85): {summary['kappa_score']:.3f}")
        print(f"å‡†ç¡®ç‡: {summary['accuracy']:.3f}")
        print(f"Kendall-Ï„ (ç›®æ ‡â‰¥0.9): {summary['kendall_tau']:.3f}")
        print(f"éªŒè¯çŠ¶æ€: {'âœ… é€šè¿‡' if summary['validation_passed'] else 'âŒ æœªé€šè¿‡'}")
        print("\n" + report["conclusion"])
        print("="*60)


def main():
    parser = argparse.ArgumentParser(description="å¤šRAGç³»ç»Ÿå‡†ç¡®ç‡éªŒè¯è¯„ä¼°")
    parser.add_argument("--qacg_files", nargs="+", required=True,
                       help="QACGæ–‡ä»¶è·¯å¾„åˆ—è¡¨")
    parser.add_argument("--num_samples", type=int, default=200,
                       help="é‡‡æ ·è¯„ä¼°å¯¹æ•°é‡")
    parser.add_argument("--annotation_file", type=str, 
                       default="dice_human_annotations.json",
                       help="äººå·¥æ ‡æ³¨æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="dice_validation_output",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--random_seed", type=int, default=42,
                       help="éšæœºç§å­")
    parser.add_argument("--llm_model", type=str, default="deepseek-chat",
                       help="LLMæ¨¡å‹")
    parser.add_argument("--tournament_result_file", type=str, 
                       default="dice_simplified_output/tournament_result.json",
                       help="tournamentç»“æœæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºå¤ç”¨å·²æœ‰åˆ¤æ–­")
    parser.add_argument("--ragas", action="store_true",
                       help="ä½¿ç”¨RAGASæ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼ˆé»˜è®¤ä½¿ç”¨DICEæ–¹æ³•ï¼‰")
    parser.add_argument("--ragas_metrics", nargs="+", 
                       default=["answer_relevancy", "context_precision", "context_recall", "faithfulness", "answer_correctness"],
                       help="RAGASè¯„ä¼°æŒ‡æ ‡åˆ—è¡¨")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # æ ¹æ®è¯„ä¼°æ–¹æ³•åˆ›å»ºé…ç½®å’Œè¯„ä¼°å™¨
    import os
    
    if args.ragas:
        # RAGASé…ç½® - ä½¿ç”¨DeepSeek
        ragas_config = RagasConfig(
            llm_model=args.llm_model,
            metrics=args.ragas_metrics,
            api_key=os.environ.get("DEEPSEEK_API_KEY", "xxxxxxx"),  # ä½¿ç”¨DeepSeek API
            base_url="https://api.deepseek.com"
        )
        evaluator = UnifiedValidationEvaluator(
            evaluation_method="ragas",
            ragas_config=ragas_config
        )
        evaluation_method = "RAGAS"
    else:
        # DICEé…ç½®
        dice_config = SimplifiedDICEConfig(
            llm_model=args.llm_model,
            output_dir=str(output_dir),
            api_key=os.environ.get("DEEPSEEK_API_KEY", ""),
            base_url="https://api.deepseek.com"
        )
        evaluator = UnifiedValidationEvaluator(
            evaluation_method="dice",
            dice_config=dice_config,
            tournament_result_file=args.tournament_result_file
        )
        evaluation_method = "DICE"
    
    print(f"ğŸ”¬ {evaluation_method}ç³»ç»ŸéªŒè¯è¯„ä¼°")
    print(f"ğŸ“ QACGæ–‡ä»¶æ•°é‡: {len(args.qacg_files)}")
    print(f"ğŸ“Š é‡‡æ ·æ•°é‡: {args.num_samples}")
    print(f"ğŸ”§ è¯„ä¼°æ–¹æ³•: {evaluation_method}")
    
    try:
        # æ­¥éª¤1: é‡‡æ ·è¯„ä¼°å¯¹
        print("\nğŸ“‹ æ­¥éª¤1: é‡‡æ ·è¯„ä¼°å¯¹...")
        evaluation_pairs = evaluator.sample_evaluation_pairs(
            args.qacg_files, args.num_samples, args.random_seed
        )
        
        # ä¿å­˜é‡‡æ ·ç»“æœ
        pairs_file = output_dir / "evaluation_pairs.json"
        with open(pairs_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_pairs, f, ensure_ascii=False, indent=2)
        print(f"âœ… é‡‡æ ·å®Œæˆï¼Œä¿å­˜è‡³: {pairs_file}")
        
        # æ­¥éª¤1.5: æ£€æŸ¥æˆ–åˆ›å»ºäººå·¥æ ‡æ³¨æ–‡ä»¶
        print(f"\nğŸ“ æ­¥éª¤1.5: æ£€æŸ¥äººå·¥æ ‡æ³¨æ–‡ä»¶: {args.annotation_file}")
        annotation_file_path = Path(args.annotation_file)
        
        if not annotation_file_path.exists():
            print("âš ï¸  äººå·¥æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ ‡æ³¨æ¨¡æ¿...")
            
            # åˆ›å»ºæ ‡æ³¨æ¨¡æ¿
            annotation_data = []
            for i, pair in enumerate(evaluation_pairs):
                annotation_item = {
                    "pair_id": i,
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "answer_a": pair["answer_a"],
                    "context_a": pair["context_a"][:3],  # åªæ˜¾ç¤ºå‰3ä¸ªcontext
                    "system_b": pair["system_b"],
                    "answer_b": pair["answer_b"],
                    "context_b": pair["context_b"][:3],
                    "groundtruth": pair["groundtruth"],
                    "expert_votes": ["", "", ""]  # 3ä½ä¸“å®¶å¡«å…¥ï¼šA wins/B wins/Tie
                }
                annotation_data.append(annotation_item)
            
            template = {
                "instructions": "è¯·3ä½ä¸“å®¶ç‹¬ç«‹å®Œæˆæ ‡æ³¨ã€‚å¯¹äºæ¯ä¸ªpair_idï¼Œè¯·ä¸ºæ¯ä½ä¸“å®¶åœ¨expert_votesä¸­å¡«å…¥ 'A wins'ã€'B wins' æˆ– 'Tie'",
                "annotation_guide": {
                    "A wins": "ç³»ç»ŸAçš„æ£€ç´¢è´¨é‡å’Œå›ç­”è´¨é‡æ˜æ˜¾ä¼˜äºç³»ç»ŸB",
                    "B wins": "ç³»ç»ŸBçš„æ£€ç´¢è´¨é‡å’Œå›ç­”è´¨é‡æ˜æ˜¾ä¼˜äºç³»ç»ŸA", 
                    "Tie": "ä¸¤ä¸ªç³»ç»Ÿè¡¨ç°ç›¸å½“ï¼Œéš¾ä»¥åŒºåˆ†ä¼˜åŠ£"
                },
                "evaluation_criteria": [
                    "1. æ£€ç´¢è¯æ®çš„ç›¸å…³æ€§å’Œå®Œæ•´æ€§",
                    "2. å›ç­”çš„å‡†ç¡®æ€§å’Œé€»è¾‘æ€§", 
                    "3. è¯æ®ä¸å›ç­”çš„ä¸€è‡´æ€§",
                    "4. ä¸æ ‡å‡†ç­”æ¡ˆçš„ç¬¦åˆç¨‹åº¦"
                ],
                "annotations": annotation_data
            }
            
            with open(args.annotation_file, 'w', encoding='utf-8') as f:
                json.dump(template, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å·²åˆ›å»ºæ ‡æ³¨æ¨¡æ¿: {args.annotation_file}")
            print("ğŸ’¡ æ ‡æ³¨è¯´æ˜:")
            print("   - æ¯ä¸ªpair_idéœ€è¦3ä½ä¸“å®¶ç‹¬ç«‹æŠ•ç¥¨")
            print("   - æŠ•ç¥¨é€‰é¡¹: 'A wins'ã€'B wins'ã€'Tie'")
            print("   - è¯·æ ¹æ®æ£€ç´¢è´¨é‡å’Œå›ç­”è´¨é‡è¿›è¡Œåˆ¤æ–­")
            print("âš ï¸  å¦‚éœ€ç”ŸæˆéªŒè¯æŠ¥å‘Šï¼Œè¯·å…ˆå®Œæˆæ ‡æ³¨åå†è¿è¡Œ")
            print("âœ… ç¨‹åºå°†ç»§ç»­æ‰§è¡ŒDICEè¯„ä¼°...\n")
        else:
            print(f"âœ… äººå·¥æ ‡æ³¨æ–‡ä»¶å·²å­˜åœ¨: {args.annotation_file}")
        
        # æ­¥éª¤2: æ£€æŸ¥æˆ–è¿è¡ŒDICEè¯„ä¼°
        results_file = output_dir / f"{evaluation_method.lower()}_results.json"
        evaluation_results = None
        
        print(f"\nğŸ¤– æ­¥éª¤2: æ£€æŸ¥{evaluation_method}è¯„ä¼°ç»“æœæ–‡ä»¶...")
        if results_file.exists():
            print(f"âœ… å‘ç°å·²æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶: {results_file}")
            print("ğŸ“‚ åŠ è½½å·²æœ‰è¯„ä¼°ç»“æœï¼Œè·³è¿‡é‡æ–°è¯„ä¼°...")
            
            try:
                with open(results_file, 'r', encoding='utf-8') as f:
                    evaluation_results = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½ {len(evaluation_results)} ä¸ªè¯„ä¼°ç»“æœ")
                
                # éªŒè¯è¯„ä¼°ç»“æœæ˜¯å¦ä¸å½“å‰é‡‡æ ·å¯¹åŒ¹é…
                if len(evaluation_results) != len(evaluation_pairs):
                    print(f"âš ï¸  è¯„ä¼°ç»“æœæ•°é‡({len(evaluation_results)})ä¸é‡‡æ ·å¯¹æ•°é‡({len(evaluation_pairs)})ä¸åŒ¹é…")
                    print("ğŸ”„ å°†é‡æ–°è¿è¡Œè¯„ä¼°...")
                    evaluation_results = None
                else:
                    print("âœ… è¯„ä¼°ç»“æœæ•°é‡åŒ¹é…ï¼Œå°†ä½¿ç”¨å·²æœ‰ç»“æœ")
            except Exception as e:
                print(f"âŒ åŠ è½½è¯„ä¼°ç»“æœå¤±è´¥: {e}")
                print("ğŸ”„ å°†é‡æ–°è¿è¡Œè¯„ä¼°...")
                evaluation_results = None
        
        # å¦‚æœæ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆçš„è¯„ä¼°ç»“æœï¼Œåˆ™è¿è¡Œè¯„ä¼°
        if evaluation_results is None:
            print(f"\nğŸ¤– è¿è¡Œ{evaluation_method}ç³»ç»Ÿè¯„ä¼°...")
            evaluation_results = evaluator.run_evaluation(evaluation_pairs)
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… {evaluation_method}è¯„ä¼°å®Œæˆï¼Œä¿å­˜è‡³: {results_file}")
        
        # æ­¥éª¤3: å°è¯•åŠ è½½äººå·¥æ ‡æ³¨å¹¶ç”ŸæˆæŠ¥å‘Š
        print(f"\nğŸ“Š æ­¥éª¤3: æ£€æŸ¥äººå·¥æ ‡æ³¨å®Œæˆæƒ…å†µ...")
        
        try:
            # å°è¯•åŠ è½½äººå·¥æ ‡æ³¨
            gold_labels = evaluator.load_human_annotations(args.annotation_file)
            
            if len(gold_labels) == 0:
                print("âš ï¸  äººå·¥æ ‡æ³¨æ–‡ä»¶å­˜åœ¨ä½†æ²¡æœ‰æœ‰æ•ˆæ ‡æ³¨")
                print("ğŸ’¡ è¯·å®Œæˆæ ‡æ³¨åé‡æ–°è¿è¡Œä»¥ç”ŸæˆéªŒè¯æŠ¥å‘Š")
                print(f"âœ… DICEè¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_file}")
                return
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(gold_labels)} ä¸ªäººå·¥æ ‡æ³¨")
            
            # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
            print("\nğŸ“Š æ­¥éª¤4: è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡...")
            agreement_metrics = evaluator.calculate_agreement(evaluation_results, gold_labels)
            
            # è®¡ç®—Eloç›¸å…³æ€§
            print("ğŸ“Š æ­¥éª¤5: è®¡ç®—Eloæ’åºç›¸å…³æ€§...")
            correlation_metrics = evaluator.calculate_elo_correlation(evaluation_results, gold_labels)
            
            # ç”ŸæˆæŠ¥å‘Š
            print("ğŸ“ æ­¥éª¤6: ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
            timestamp = datetime.now().strftime("%Y%m%d%H%M")
            report_file = output_dir / f"validation_report_{timestamp}.json"
            evaluator.generate_validation_report(
                agreement_metrics, correlation_metrics, evaluation_results, gold_labels, str(report_file)
            )
            
            print(f"\nâœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
            
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ç”ŸæˆéªŒè¯æŠ¥å‘Š: {e}")
            print("ğŸ’¡ è¿™å¯èƒ½æ˜¯å› ä¸º:")
            print("   1. äººå·¥æ ‡æ³¨æ–‡ä»¶å°šæœªå®Œæˆæ ‡æ³¨")
            print("   2. æ ‡æ³¨æ ¼å¼ä¸æ­£ç¡®")
            print("   3. expert_voteså­—æ®µä¸ºç©º")
            print(f"\nâœ… DICEè¯„ä¼°å·²å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {results_file}")
            print("ğŸ“ å®Œæˆäººå·¥æ ‡æ³¨åï¼Œå¯é‡æ–°è¿è¡Œè„šæœ¬ç”ŸæˆéªŒè¯æŠ¥å‘Š")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()