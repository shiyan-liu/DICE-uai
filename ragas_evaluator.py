#!/usr/bin/env python3
"""
RAGASè¯„ä¼°å™¨ - å®Œå…¨ä½¿ç”¨DeepSeek API
è§£å†³RAGASå†…éƒ¨å¯¹OpenAI APIçš„ä¾èµ–é—®é¢˜
"""

import json
import logging
import os
import time
from typing import List, Dict, Any
from dataclasses import dataclass
import numpy as np

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶RAGASä½¿ç”¨æˆ‘ä»¬çš„é…ç½®
os.environ["OPENAI_API_KEY"] = "xxxxxxx"
os.environ["OPENAI_BASE_URL"] = "https://api.deepseek.com"

try:
    from ragas import evaluate
    from ragas.metrics import answer_relevancy, faithfulness, ContextRelevance
    from datasets import Dataset
    from langchain_openai import ChatOpenAI
    # å°è¯•ä½¿ç”¨æ–°çš„HuggingFaceEmbeddings
    try:
        from langchain_huggingface import HuggingFaceEmbeddings
    except ImportError:
        # å¦‚æœæ–°åŒ…ä¸å¯ç”¨ï¼Œå›é€€åˆ°æ—§åŒ…
        from langchain_community.embeddings import HuggingFaceEmbeddings
    from ragas.llms import LangchainLLMWrapper
    from ragas.embeddings import LangchainEmbeddingsWrapper
    RAGAS_AVAILABLE = True
except ImportError as e:
    RAGAS_AVAILABLE = False
    print(f"è­¦å‘Š: RAGASæ¡†æ¶æœªå®‰è£…ã€‚é”™è¯¯: {e}")


@dataclass
class RagasConfig:
    """RAGASè¯„ä¼°é…ç½®"""
    llm_model: str = "deepseek-chat"
    embeddings_model: str = "BAAI/bge-small-zh-v1.5"  # ä½¿ç”¨æ›´å°çš„æ¨¡å‹èŠ‚çœå†…å­˜
    metrics: List[str] = None
    api_key: str = "xxxxxxx"
    base_url: str = "https://api.deepseek.com"
    
    def __post_init__(self):
        if self.metrics is None:
            # åŸºäºRAGASåŸè®ºæ–‡çš„ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦
            self.metrics = ["faithfulness", "answer_relevancy", "context_relevance"]


class RagasEvaluator:
    """RAGASè¯„ä¼°å™¨ - ä½¿ç”¨DeepSeek API"""
    
    def __init__(self, config: RagasConfig):
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASæ¡†æ¶æœªå®‰è£…")
        
        self.config = config
        self.logger = logging.getLogger("RagasEvaluator")
        self._setup_logger()
        
        # å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡
        self._force_openai_env()
        
        # è®¾ç½®è‡ªå®šä¹‰LLM
        self._setup_custom_llm()
        
        # åˆå§‹åŒ–RAGASä¸‰ä¸ªæ ¸å¿ƒmetrics
        self.metrics_map = {
            "faithfulness": faithfulness,
            "answer_relevancy": answer_relevancy,
            "context_relevance": ContextRelevance()  # éœ€è¦å®ä¾‹åŒ–
        }
        
        self.active_metrics = [self.metrics_map[m] for m in self.config.metrics if m in self.metrics_map]
        self._configure_metrics_llm()
        
        self.logger.info(f"RAGASè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨DeepSeek: {self.config.llm_model}")
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)  # æ¢å¤æ­£å¸¸æ—¥å¿—çº§åˆ«
    
    def _force_openai_env(self):
        """å¼ºåˆ¶è®¾ç½®OpenAIç¯å¢ƒå˜é‡ä»¥æ¬ºéª—RAGAS"""
        os.environ["OPENAI_API_KEY"] = self.config.api_key
        os.environ["OPENAI_BASE_URL"] = self.config.base_url
        os.environ["OPENAI_API_BASE"] = self.config.base_url
        self.logger.info(f"å¼ºåˆ¶è®¾ç½®OpenAIç¯å¢ƒå˜é‡æŒ‡å‘DeepSeek: {self.config.base_url}")
    
    def _setup_custom_llm(self):
        """è®¾ç½®DeepSeek LLM"""
        try:
            # åˆ›å»ºæŒ‡å‘DeepSeekçš„ChatOpenAIå®ä¾‹
            self.custom_llm = ChatOpenAI(
                model=self.config.llm_model,
                api_key=self.config.api_key,
                base_url=self.config.base_url,
                temperature=0.0,
                max_retries=2,
                request_timeout=60
            )
            
            self.ragas_llm = LangchainLLMWrapper(self.custom_llm)
            
            # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼Œç»å¯¹ä¸è°ƒç”¨API
            try:
                # ç¡®ä¿ä¸ä¼šè°ƒç”¨ä»»ä½•APIç«¯ç‚¹
                import os
                # ä¸´æ—¶ç§»é™¤OpenAIç›¸å…³ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢åµŒå…¥æ¨¡å‹å°è¯•ä½¿ç”¨API
                old_openai_key = os.environ.get("OPENAI_API_KEY")
                old_openai_base = os.environ.get("OPENAI_BASE_URL")
                if "OPENAI_API_KEY" in os.environ:
                    del os.environ["OPENAI_API_KEY"]
                if "OPENAI_BASE_URL" in os.environ:
                    del os.environ["OPENAI_BASE_URL"]
                
                # æ·»åŠ å†…å­˜ä¼˜åŒ–é…ç½®
                model_kwargs = {
                    'device': 'cpu',
                    'trust_remote_code': True
                }
                encode_kwargs = {
                    'normalize_embeddings': True,
                    'batch_size': 1
                    # ç§»é™¤show_progress_barä»¥é¿å…å‚æ•°å†²çª
                }
                
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=self.config.embeddings_model,
                    model_kwargs=model_kwargs,
                    encode_kwargs=encode_kwargs,
                    cache_folder='./models_cache'
                )
                self.ragas_embeddings = LangchainEmbeddingsWrapper(self.embeddings)
                
                # æ¢å¤OpenAIç¯å¢ƒå˜é‡ï¼ˆä»…ç”¨äºLLMï¼‰
                if old_openai_key:
                    os.environ["OPENAI_API_KEY"] = old_openai_key
                if old_openai_base:
                    os.environ["OPENAI_BASE_URL"] = old_openai_base
                
                self.logger.info(f"æœ¬åœ°åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {self.config.embeddings_model}")
                
                # æµ‹è¯•åµŒå…¥æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
                test_embedding = self.embeddings.embed_query("æµ‹è¯•æ–‡æœ¬")
                self.logger.info(f"åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œç»´åº¦: {len(test_embedding)}")
                
            except Exception as e:
                self.logger.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                import traceback
                self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                # å¦‚æœåµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè®¾ä¸ºNone
                self.ragas_embeddings = None
                raise Exception(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è¯„ä¼°éœ€è¦åµŒå…¥çš„æŒ‡æ ‡: {e}")
            
            self.logger.info(f"DeepSeek LLMé…ç½®æˆåŠŸ: {self.config.llm_model}")
            
        except Exception as e:
            self.logger.error(f"LLMé…ç½®å¤±è´¥: {e}")
            raise
    
    def _configure_metrics_llm(self):
        """ä¸ºæ¯ä¸ªmetricé…ç½®è‡ªå®šä¹‰LLM"""
        try:
            for metric in self.active_metrics:
                if hasattr(metric, 'llm'):
                    metric.llm = self.ragas_llm
                if hasattr(metric, 'embeddings') and self.ragas_embeddings is not None:
                    metric.embeddings = self.ragas_embeddings
                elif hasattr(metric, 'embeddings') and self.ragas_embeddings is None:
                    self.logger.warning(f"æŒ‡æ ‡ {type(metric).__name__} éœ€è¦åµŒå…¥æ¨¡å‹ï¼Œä½†åµŒå…¥æ¨¡å‹æœªåŠ è½½")
            self.logger.info("æ‰€æœ‰Metrics LLMé…ç½®å®Œæˆ")
        except Exception as e:
            self.logger.error(f"Metrics LLMé…ç½®å¤±è´¥: {e}")
            raise
    
    def _qacg_to_ragas_format(self, qacg_data: Dict[str, Any]) -> Dict[str, Any]:
        """å°†QACGæ•°æ®è½¬æ¢ä¸ºRAGASæ ¼å¼"""
        contexts = []
        if isinstance(qacg_data.get("context"), list):
            for ctx in qacg_data["context"]:
                if isinstance(ctx, dict):
                    contexts.append(ctx.get("text", str(ctx)))
                else:
                    contexts.append(str(ctx))
        elif qacg_data.get("context"):
            contexts = [str(qacg_data["context"])]
        
        # å¤„ç†ground_truthå­—æ®µï¼Œç¡®ä¿å®ƒæ˜¯å­—ç¬¦ä¸²æ ¼å¼
        ground_truth = qacg_data.get("groundtruth", qacg_data.get("expected_answer", ""))
        
        # å¦‚æœground_truthæ˜¯åˆ—è¡¨ï¼Œå°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(ground_truth, list):
            if len(ground_truth) > 0:
                # å¦‚æœåˆ—è¡¨ä¸ä¸ºç©ºï¼Œè¿æ¥æ‰€æœ‰å…ƒç´ 
                ground_truth = " ".join(str(item) for item in ground_truth)
            else:
                # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                ground_truth = ""
        elif ground_truth is None:
            ground_truth = ""
        else:
            # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
            ground_truth = str(ground_truth)
        
        # åŒæ ·å¤„ç†å…¶ä»–å¯èƒ½æ˜¯åˆ—è¡¨çš„å­—æ®µ
        question = qacg_data.get("question", "")
        if isinstance(question, list):
            question = " ".join(str(item) for item in question) if question else ""
        else:
            question = str(question) if question else ""
        
        answer = qacg_data.get("rag_answer", "")
        if isinstance(answer, list):
            answer = " ".join(str(item) for item in answer) if answer else ""
        else:
            answer = str(answer) if answer else ""
        
        return {
            "question": question,
            "answer": answer,
            "contexts": contexts,
            "ground_truth": ground_truth
        }
    
    def evaluate_single_qacg(self, qacg_data: Dict[str, Any]) -> Dict[str, float]:
        """ä½¿ç”¨æ”¹è¿›çš„RAGASè¯„ä¼°å•ä¸ªQACG - è§£å†³faithfulness NaNé—®é¢˜"""
        max_retries = 3
        retry_delay = 2.0
        
        for attempt in range(max_retries + 1):
            try:
                # å¼ºåˆ¶é‡æ–°è®¾ç½®ç¯å¢ƒå˜é‡
                self._force_openai_env()
                
                ragas_data = self._qacg_to_ragas_format(qacg_data)
                
                # æ•°æ®éªŒè¯
                if not ragas_data["question"] or not ragas_data["answer"]:
                    self.logger.warning("é—®é¢˜æˆ–ç­”æ¡ˆä¸ºç©ºï¼Œè·³è¿‡è¯„ä¼°")
                    return {metric: 0.0 for metric in self.config.metrics}
                
                if not ragas_data["contexts"]:
                    self.logger.warning("ä¸Šä¸‹æ–‡ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    ragas_data["contexts"] = [""]
                
                self.logger.debug(f"å‡†å¤‡è¯„ä¼°æ•°æ®: é—®é¢˜é•¿åº¦={len(ragas_data['question'])}, ç­”æ¡ˆé•¿åº¦={len(ragas_data['answer'])}, ä¸Šä¸‹æ–‡æ•°é‡={len(ragas_data['contexts'])}")
                
                # é¢å¤–çš„æ•°æ®éªŒè¯å’Œæ¸…ç†
                if not isinstance(ragas_data['ground_truth'], str):
                    self.logger.warning(f"ground_truthä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(ragas_data['ground_truth'])}, å€¼: {ragas_data['ground_truth']}")
                    ragas_data['ground_truth'] = str(ragas_data['ground_truth'])
                
                if not isinstance(ragas_data['question'], str):
                    self.logger.warning(f"questionä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(ragas_data['question'])}, å€¼: {ragas_data['question']}")
                    ragas_data['question'] = str(ragas_data['question'])
                
                if not isinstance(ragas_data['answer'], str):
                    self.logger.warning(f"answerä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(ragas_data['answer'])}, å€¼: {ragas_data['answer']}")
                    ragas_data['answer'] = str(ragas_data['answer'])
                
                # åˆ›å»ºæ•°æ®é›†
                dataset = Dataset.from_dict({
                    "question": [ragas_data["question"]],
                    "answer": [ragas_data["answer"]],
                    "contexts": [ragas_data["contexts"]],
                    "ground_truth": [ragas_data["ground_truth"]]
                })
                
                # ä½¿ç”¨å•çº¿ç¨‹è¯„ä¼°é¿å…å¹¶å‘é—®é¢˜
                self.logger.debug(f"å¼€å§‹RAGAS evaluateè°ƒç”¨ (å°è¯• {attempt + 1}/{max_retries + 1})")
                result = evaluate(
                    dataset, 
                    metrics=self.active_metrics,
                    show_progress=False
                )
                self.logger.debug(f"RAGAS evaluateå®Œæˆï¼Œç»“æœç±»å‹: {type(result)}")
                
                # æå–å¾—åˆ†
                scores = {}
                
                # å»ºç«‹æŒ‡æ ‡åç§°æ˜ å°„å…³ç³»ï¼ŒRAGASå†…éƒ¨ä½¿ç”¨ä¸åŒçš„é”®å
                metric_name_mapping = {
                    "context_relevance": "nv_context_relevance",
                    "faithfulness": "faithfulness",
                    "answer_relevancy": "answer_relevancy"
                }
                
                for metric_name in self.config.metrics:
                    try:
                        # è·å–å®é™…åœ¨RAGASç»“æœä¸­çš„é”®å
                        actual_key = metric_name_mapping.get(metric_name, metric_name)
                        
                        # å°è¯•å¤šç§æ–¹å¼è·å–å¾—åˆ†
                        score_value = None
                        
                        if hasattr(result, actual_key):
                            score_value = getattr(result, actual_key)
                        elif hasattr(result, '_scores_dict') and actual_key in result._scores_dict:
                            score_value = result._scores_dict[actual_key]
                        elif actual_key in result:
                            score_value = result[actual_key]
                        
                        if score_value is not None:
                            # å¤„ç†ä¸åŒæ ¼å¼çš„å¾—åˆ†å€¼
                            if isinstance(score_value, (list, tuple)) and len(score_value) > 0:
                                actual_score = score_value[0]
                            else:
                                actual_score = score_value
                            
                            # æ£€æŸ¥NaNå€¼å¹¶å¤„ç†
                            if isinstance(actual_score, float) and (actual_score != actual_score):
                                self.logger.warning(f"æŒ‡æ ‡ {metric_name} è¿”å›NaNï¼Œä½¿ç”¨é»˜è®¤å€¼")
                                scores[metric_name] = 0.3 if metric_name == "faithfulness" else 0.5
                            elif isinstance(actual_score, (int, float)):
                                scores[metric_name] = float(actual_score)
                                self.logger.debug(f"æˆåŠŸè·å–æŒ‡æ ‡ {metric_name} å¾—åˆ†: {scores[metric_name]}")
                            else:
                                self.logger.warning(f"æŒ‡æ ‡ {metric_name} å¾—åˆ†ç±»å‹æ— æ•ˆ: {type(actual_score)}")
                                scores[metric_name] = 0.3 if metric_name == "faithfulness" else 0.5
                        else:
                            self.logger.warning(f"æ— æ³•è·å–æŒ‡æ ‡ {metric_name} çš„å¾—åˆ†")
                            scores[metric_name] = 0.3 if metric_name == "faithfulness" else 0.5
                            
                    except Exception as e:
                        self.logger.warning(f"è·å–æŒ‡æ ‡ {metric_name} æ—¶å‡ºé”™: {e}")
                        scores[metric_name] = 0.3 if metric_name == "faithfulness" else 0.5
                
                # éªŒè¯æ‰€æœ‰å¾—åˆ†æ˜¯å¦æœ‰æ•ˆ
                valid_scores = all(
                    isinstance(score, (int, float)) and not np.isnan(score) and not np.isinf(score)
                    for score in scores.values()
                )
                
                if valid_scores:
                    self.logger.debug(f"è¯„ä¼°æˆåŠŸï¼Œå¾—åˆ†: {scores}")
                    return scores
                else:
                    self.logger.warning(f"è¯„ä¼°ç»“æœåŒ…å«æ— æ•ˆå¾—åˆ†: {scores}")
                    if attempt < max_retries:
                        time.sleep(retry_delay * (attempt + 1))
                        continue
                    else:
                        # è¿”å›å®‰å…¨çš„é»˜è®¤å¾—åˆ†
                        return {metric: 0.3 if metric == "faithfulness" else 0.5 for metric in self.config.metrics}
                
            except Exception as e:
                self.logger.warning(f"RAGASè¯„ä¼°å°è¯• {attempt + 1} å¤±è´¥: {e}")
                if attempt < max_retries:
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    import traceback
                    error_msg = f"RAGASè¯„ä¼°å¤±è´¥: {str(e)}\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}"
                    self.logger.error(error_msg)
                    # è¿”å›å®‰å…¨çš„é»˜è®¤å¾—åˆ†
                    return {metric: 0.3 if metric == "faithfulness" else 0.5 for metric in self.config.metrics}
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å¾—åˆ†
        return {metric: 0.3 if metric == "faithfulness" else 0.5 for metric in self.config.metrics}
    
    def calculate_composite_score(self, scores: Dict[str, float]) -> float:
        """è®¡ç®—å¹³å‡å¾—åˆ†ï¼ˆRAGASåŸè®ºæ–‡æœªæåŠåŠ æƒç»„åˆï¼‰"""
        if not scores:
            return 0.0
        
        # ç®€å•å¹³å‡ï¼Œç¬¦åˆRAGASåŸè®ºæ–‡çš„åšæ³•
        valid_scores = [score for score in scores.values() if score is not None and score >= 0]
        return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
    
    def compare_qacg_pair(self, qa_a: Dict[str, Any], qa_b: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªQACGçš„æ€§èƒ½"""
        self.logger.info("å¼€å§‹RAGASå¯¹æ¯”è¯„ä¼°")
        
        # åˆ†åˆ«è¯„ä¼°ä¸¤ä¸ªç³»ç»Ÿ
        print(f"\nğŸ” è¯„ä¼°ç³»ç»ŸA...")
        scores_a = self.evaluate_single_qacg(qa_a)
        print(f"   ç³»ç»ŸAå¾—åˆ†: {scores_a}")
        
        print(f"ğŸ” è¯„ä¼°ç³»ç»ŸB...")
        scores_b = self.evaluate_single_qacg(qa_b)
        print(f"   ç³»ç»ŸBå¾—åˆ†: {scores_b}")
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        composite_a = self.calculate_composite_score(scores_a)
        composite_b = self.calculate_composite_score(scores_b)
        
        print(f"\nğŸ“Š ç»¼åˆå¾—åˆ†å¯¹æ¯”:")
        print(f"   ç³»ç»ŸAç»¼åˆå¾—åˆ†: {composite_a:.4f}")
        print(f"   ç³»ç»ŸBç»¼åˆå¾—åˆ†: {composite_b:.4f}")
        print(f"   å¾—åˆ†å·®å¼‚: {composite_a - composite_b:.4f}")
        
        # ç¡®å®šè·èƒœè€…
        score_diff = composite_a - composite_b
        
        if abs(score_diff) < 0.05:
            judgment = "Tie"
            judgment_icon = "âš–ï¸"
        elif score_diff > 0:
            judgment = "A wins"
            judgment_icon = "ğŸ†"
        else:
            judgment = "B wins"
            judgment_icon = "ğŸ†"
        
        # ç”Ÿæˆè¯¦ç»†ç†ç”±
        reason_parts = []
        detail_parts = []
        
        print(f"\nğŸ“‹ è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”:")
        for metric in self.config.metrics:
            score_a = scores_a.get(metric, 0)
            score_b = scores_b.get(metric, 0)
            diff = score_a - score_b
            
            # ç¡®å®šå“ªä¸ªç³»ç»Ÿåœ¨è¯¥æŒ‡æ ‡ä¸Šæ›´ä¼˜
            if abs(diff) > 0.01:  # é™ä½é˜ˆå€¼ä»¥æ˜¾ç¤ºæ›´å¤šç»†èŠ‚
                if diff > 0:
                    better_system = "A"
                    icon = "ğŸ“ˆ"
                else:
                    better_system = "B"
                    icon = "ğŸ“‰"
                detail_parts.append(f"   {icon} {metric}: A={score_a:.3f} vs B={score_b:.3f} â†’ ç³»ç»Ÿ{better_system}æ›´ä¼˜")
                
                if abs(diff) > 0.1:  # åªæœ‰æ˜¾è‘—å·®å¼‚æ‰åŠ å…¥ç†ç”±
                    reason_parts.append(f"{metric}: ç³»ç»Ÿ{better_system}æ›´ä¼˜ ({score_a:.3f} vs {score_b:.3f})")
            else:
                detail_parts.append(f"   âš–ï¸ {metric}: A={score_a:.3f} vs B={score_b:.3f} â†’ ç›¸å½“")
        
        # æ‰“å°è¯¦ç»†å¯¹æ¯”
        for detail in detail_parts:
            print(detail)
        
        if not reason_parts:
            reason = f"ä¸¤ç³»ç»Ÿæ€§èƒ½æ¥è¿‘ (A: {composite_a:.3f}, B: {composite_b:.3f})"
        else:
            reason = "; ".join(reason_parts)
        
        # æ‰“å°æœ€ç»ˆåˆ¤æ–­
        print(f"\n{judgment_icon} æœ€ç»ˆåˆ¤æ–­: {judgment}")
        print(f"ğŸ“ åˆ¤æ–­ç†ç”±: {reason}")
        print("-" * 80)
        
        return {
            "judgment": judgment,
            "score_a": composite_a,
            "score_b": composite_b,
            "score_diff": score_diff,
            "detailed_scores_a": scores_a,
            "detailed_scores_b": scores_b,
            "reason": reason,
            "margin_score": abs(score_diff)
        }
    
    def _pairwise_comparison(self, qa_list_a: List[Dict[str, Any]], 
                           qa_list_b: List[Dict[str, Any]],
                           system_a_name: str, system_b_name: str,
                           max_questions: int = None) -> Dict[str, Any]:
        """è¿›è¡Œä¸¤ä¸ªç³»ç»Ÿçš„æˆå¯¹æ¯”è¾ƒ"""
        self.logger.info(f"å¼€å§‹RAGASæˆå¯¹æ¯”è¾ƒ: {system_a_name} vs {system_b_name}")
        
        num_questions = min(len(qa_list_a), len(qa_list_b))
        if max_questions:
            num_questions = min(num_questions, max_questions)
        
        print(f"\nğŸ”¥ RAGASç³»ç»Ÿå¯¹æ¯”: {system_a_name} vs {system_b_name}")
        print(f"ğŸ“Š å°†è¯„ä¼° {num_questions} ä¸ªé—®é¢˜")
        print("=" * 100)
        
        question_results = []
        
        for i in range(num_questions):
            qa_a = qa_list_a[i]
            qa_b = qa_list_b[i]
            
            if qa_a.get("question") != qa_b.get("question"):
                self.logger.warning(f"é—®é¢˜{i}ä¸åŒ¹é…ï¼Œè·³è¿‡")
                continue
            
            # æ˜¾ç¤ºå½“å‰è¯„ä¼°çš„é—®é¢˜ä¿¡æ¯
            question_text = qa_a.get("question", "")
            print(f"\nğŸ“ é—®é¢˜ {i+1}: {question_text[:100]}{'...' if len(question_text) > 100 else ''}")
            print(f"ğŸ¤– ç³»ç»ŸAå›ç­”: {qa_a.get('rag_answer', '')[:80]}{'...' if len(qa_a.get('rag_answer', '')) > 80 else ''}")
            print(f"ğŸ¤– ç³»ç»ŸBå›ç­”: {qa_b.get('rag_answer', '')[:80]}{'...' if len(qa_b.get('rag_answer', '')) > 80 else ''}")
            
            comparison_result = self.compare_qacg_pair(qa_a, qa_b)
            
            question_result = {
                "question_id": i,
                "question": qa_a.get("question", ""),
                "passage_judgment": {
                    "label": comparison_result["judgment"],
                    "score": comparison_result["score_a"] if comparison_result["judgment"] == "A wins" 
                            else comparison_result["score_b"] if comparison_result["judgment"] == "B wins" 
                            else (comparison_result["score_a"] + comparison_result["score_b"]) / 2,
                    "reason": comparison_result["reason"],
                    "margin_score": comparison_result["margin_score"]
                },
                "ragas_details": {
                    "scores_a": comparison_result["detailed_scores_a"],
                    "scores_b": comparison_result["detailed_scores_b"],
                    "composite_a": comparison_result["score_a"],
                    "composite_b": comparison_result["score_b"]
                }
            }
            
            question_results.append(question_result)
        
        # è®¡ç®—æ€»ä½“ç»“æœ
        a_wins = sum(1 for r in question_results if r["passage_judgment"]["label"] == "A wins")
        b_wins = sum(1 for r in question_results if r["passage_judgment"]["label"] == "B wins")
        ties = len(question_results) - a_wins - b_wins
        
        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        print(f"\nğŸ¯ æ€»ä½“å¯¹æ¯”ç»“æœ:")
        print(f"   ğŸ“Š æ€»é—®é¢˜æ•°: {len(question_results)}")
        print(f"   ğŸ† {system_a_name} è·èƒœ: {a_wins} æ¬¡ ({a_wins/len(question_results)*100:.1f}%)")
        print(f"   ğŸ† {system_b_name} è·èƒœ: {b_wins} æ¬¡ ({b_wins/len(question_results)*100:.1f}%)")
        print(f"   âš–ï¸ å¹³å±€: {ties} æ¬¡ ({ties/len(question_results)*100:.1f}%)")
        
        if a_wins > b_wins:
            winner = system_a_name
            win_icon = "ğŸ¥‡"
        elif b_wins > a_wins:
            winner = system_b_name
            win_icon = "ğŸ¥‡"
        else:
            winner = "å¹³å±€"
            win_icon = "âš–ï¸"
        
        print(f"\n{win_icon} æ€»ä½“èƒœè€…: {winner}")
        print("=" * 100)
        
        return {
            "system_a": system_a_name,
            "system_b": system_b_name,
            "question_results": question_results,
            "summary": {
                "total_questions": len(question_results),
                "a_wins": a_wins,
                "b_wins": b_wins,
                "ties": ties,
                "win_rate_a": a_wins / len(question_results) if question_results else 0,
                "win_rate_b": b_wins / len(question_results) if question_results else 0
            }
        }


class RagasValidationEvaluator:
    """RAGASéªŒè¯è¯„ä¼°å™¨"""
    
    def __init__(self, config: RagasConfig):
        self.config = config
        self.logger = logging.getLogger("RagasValidation")
        self.ragas_evaluator = RagasEvaluator(config)
        self._setup_logger()
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def sample_evaluation_pairs(self, qacg_files: List[str], num_samples: int = 200, 
                               random_seed: int = 42) -> List[Dict[str, Any]]:
        """é‡‡æ ·è¯„ä¼°å¯¹ï¼ˆå¤ç”¨DICEçš„é€»è¾‘ï¼‰"""
        import random
        from pathlib import Path
        
        self.logger.info(f"å¼€å§‹é‡‡æ · {num_samples} å¯¹è¯„ä¼°æ ·æœ¬")
        random.seed(random_seed)
        
        # åŠ è½½æ‰€æœ‰ç³»ç»Ÿæ•°æ®
        all_systems_data = {}
        for file_path in qacg_files:
            system_name = Path(file_path).stem.replace("qacg_", "")
            with open(file_path, 'r', encoding='utf-8') as f:
                all_systems_data[system_name] = json.load(f)
        
        systems = list(all_systems_data.keys())
        if len(systems) < 2:
            raise ValueError(f"éœ€è¦è‡³å°‘2ä¸ªç³»ç»Ÿï¼Œå®é™…è·å¾—{len(systems)}ä¸ª")
        
        min_length = min(len(data) for data in all_systems_data.values())
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç³»ç»Ÿå¯¹å’Œé¢˜ç›®ç»„åˆ
        all_combinations = []
        for i, system_a in enumerate(systems):
            for j, system_b in enumerate(systems):
                if i < j:
                    for q_idx in range(min_length):
                        qa_a = all_systems_data[system_a][q_idx]
                        qa_b = all_systems_data[system_b][q_idx]
                        
                        if qa_a["question"] == qa_b["question"]:
                            combination = {
                                "question_idx": q_idx,
                                "system_a": system_a,
                                "system_b": system_b,
                                "qa_a": qa_a,
                                "qa_b": qa_b,
                                "question": qa_a["question"],
                                "answer_a": qa_a.get("rag_answer", ""),
                                "answer_b": qa_b.get("rag_answer", ""),
                                "expected_answer": qa_a.get("expected_answer", ""),
                                "context_a": qa_a.get("context", []),
                                "context_b": qa_b.get("context", []),
                                "groundtruth": qa_a.get("groundtruth", qa_a.get("expected_answer", ""))
                            }
                            all_combinations.append(combination)
        
        # éšæœºé‡‡æ ·
        if len(all_combinations) < num_samples:
            sampled_pairs = all_combinations
        else:
            sampled_pairs = random.sample(all_combinations, num_samples)
        
        self.logger.info(f"æˆåŠŸé‡‡æ · {len(sampled_pairs)} å¯¹è¯„ä¼°æ ·æœ¬")
        return sampled_pairs
    
    def run_ragas_evaluation(self, evaluation_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨RAGASè¯„ä¼°æ‰€æœ‰é‡‡æ ·çš„å¯¹æ¯”å¯¹"""
        self.logger.info(f"å¼€å§‹RAGASè¯„ä¼° {len(evaluation_pairs)} å¯¹æ ·æœ¬")
        
        print(f"\nğŸš€ RAGASæ‰¹é‡è¯„ä¼°å¼€å§‹")
        print(f"ğŸ“Š æ€»å…±éœ€è¦è¯„ä¼°: {len(evaluation_pairs)} å¯¹æ ·æœ¬")
        print("ğŸ”” æ¯æ¬¡è¯„ä¼°ä¼šæ˜¾ç¤ºè¯¦ç»†çš„åˆ¤æ–­è¿‡ç¨‹å’Œç»“æœ")
        print("=" * 120)
        
        ragas_results = []
        for i, pair in enumerate(evaluation_pairs):
            print(f"\nâ³ è¿›åº¦: {i+1}/{len(evaluation_pairs)} ({(i+1)/len(evaluation_pairs)*100:.1f}%)")
            print(f"ğŸ” è¯„ä¼°å¯¹ #{i+1}: {pair['system_a']} vs {pair['system_b']}")
            
            qa_a = pair["qa_a"]
            qa_b = pair["qa_b"]
            
            result = self.ragas_evaluator._pairwise_comparison(
                [qa_a], [qa_b], 
                pair["system_a"], pair["system_b"],
                max_questions=1
            )
            
            if result["question_results"]:
                question_result = result["question_results"][0]
                passage_judgment = question_result.get("passage_judgment", {})
                ragas_details = question_result.get("ragas_details", {})
                
                # æ˜¾ç¤ºæœ¬æ¬¡è¯„ä¼°çš„æœ€ç»ˆç»“æœ
                judgment = passage_judgment.get("label", "Tie")
                score = passage_judgment.get("score", 0.5)
                reason = passage_judgment.get("reason", "")
                
                judgment_icon = "ğŸ†" if judgment != "Tie" else "âš–ï¸"
                print(f"\nâœ… è¯„ä¼°å¯¹ #{i+1} å®Œæˆ:")
                print(f"   {judgment_icon} ç»“æœ: {judgment}")
                print(f"   ğŸ“Š ç½®ä¿¡åº¦: {score:.4f}")
                print(f"   ğŸ“ ç†ç”±: {reason}")
                
                ragas_result = {
                    "pair_id": i,
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "system_b": pair["system_b"],
                    "dice_judgment": judgment,
                    "dice_score": score,
                    "dice_reason": reason,
                    "dice_margin_score": passage_judgment.get("margin_score", 0.0),
                    "combined_delta": ragas_details.get("composite_a", 0) - ragas_details.get("composite_b", 0),
                    "ragas_scores_a": ragas_details.get("scores_a", {}),
                    "ragas_scores_b": ragas_details.get("scores_b", {}),
                    "original_pair": pair
                }
            else:
                print(f"\nâŒ è¯„ä¼°å¯¹ #{i+1} å¤±è´¥:")
                print(f"   âš ï¸ RAGASè¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
                
                ragas_result = {
                    "pair_id": i,
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "system_b": pair["system_b"],
                    "dice_judgment": "Tie",
                    "dice_score": 0.5,
                    "dice_reason": "RAGASè¯„ä¼°å¤±è´¥",
                    "dice_margin_score": 0.0,
                    "combined_delta": 0.0,
                    "ragas_scores_a": {},
                    "ragas_scores_b": {},
                    "original_pair": pair
                }
            
            ragas_results.append(ragas_result)
            print("â•" * 120)
        
        # æ˜¾ç¤ºæ‰¹é‡è¯„ä¼°ç»Ÿè®¡
        print(f"\nğŸŠ RAGASæ‰¹é‡è¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
        
        # ç»Ÿè®¡ç»“æœ
        judgments = [r["dice_judgment"] for r in ragas_results]
        a_wins = judgments.count("A wins")
        b_wins = judgments.count("B wins")
        ties = judgments.count("Tie")
        
        print(f"   ğŸ† A wins: {a_wins} æ¬¡ ({a_wins/len(ragas_results)*100:.1f}%)")
        print(f"   ğŸ† B wins: {b_wins} æ¬¡ ({b_wins/len(ragas_results)*100:.1f}%)")
        print(f"   âš–ï¸ Tie: {ties} æ¬¡ ({ties/len(ragas_results)*100:.1f}%)")
        print("=" * 120)
        
        return ragas_results
    
    def load_human_annotations(self, annotation_file: str):
        """åŠ è½½äººå·¥æ ‡æ³¨ï¼ˆå¤ç”¨DICEçš„é€»è¾‘ï¼‰"""
        # è¿™é‡Œéœ€è¦å¯¼å…¥å¹¶å¤ç”¨DICEçš„é€»è¾‘
        from dice_evaluation_script import DICEValidationEvaluator
        from src.dice.dice_simplified import SimplifiedDICEConfig
        
        # åˆ›å»ºä¸´æ—¶DICEè¯„ä¼°å™¨æ¥å¤ç”¨å…¶æ–¹æ³•
        temp_config = SimplifiedDICEConfig()
        temp_evaluator = DICEValidationEvaluator(temp_config)
        return temp_evaluator.load_human_annotations(annotation_file)
    
    def calculate_agreement(self, results, gold_labels):
        """è®¡ç®—ä¸€è‡´æ€§ï¼ˆå¤ç”¨DICEçš„é€»è¾‘ï¼‰"""
        from dice_evaluation_script import DICEValidationEvaluator
        from src.dice.dice_simplified import SimplifiedDICEConfig
        
        temp_config = SimplifiedDICEConfig()
        temp_evaluator = DICEValidationEvaluator(temp_config)
        return temp_evaluator.calculate_agreement(results, gold_labels)
    
    def calculate_elo_correlation(self, results, gold_labels):
        """è®¡ç®—Eloç›¸å…³æ€§ï¼ˆå¤ç”¨DICEçš„é€»è¾‘ï¼‰"""
        from dice_evaluation_script import DICEValidationEvaluator
        from src.dice.dice_simplified import SimplifiedDICEConfig
        
        temp_config = SimplifiedDICEConfig()
        temp_evaluator = DICEValidationEvaluator(temp_config)
        return temp_evaluator.calculate_elo_correlation(results, gold_labels)
    
    def analyze_disagreement_cases(self, results, gold_labels):
        """åˆ†æä¸ä¸€è‡´caseï¼ˆå¤ç”¨DICEçš„é€»è¾‘ï¼‰"""
        from dice_evaluation_script import DICEValidationEvaluator
        from src.dice.dice_simplified import SimplifiedDICEConfig
        
        temp_config = SimplifiedDICEConfig()
        temp_evaluator = DICEValidationEvaluator(temp_config)
        return temp_evaluator.analyze_disagreement_cases(results, gold_labels)
    
    def print_disagreement_analysis(self, disagreement_cases):
        """æ‰“å°åˆ†æ­§åˆ†æï¼ˆå¤ç”¨DICEçš„é€»è¾‘ï¼‰"""
        from dice_evaluation_script import DICEValidationEvaluator
        from src.dice.dice_simplified import SimplifiedDICEConfig
        
        temp_config = SimplifiedDICEConfig()
        temp_evaluator = DICEValidationEvaluator(temp_config)
        return temp_evaluator.print_disagreement_analysis(disagreement_cases)
    
    def generate_validation_report(self, agreement_metrics, correlation_metrics, results, gold_labels, output_file):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Šï¼ˆå¤ç”¨DICEçš„é€»è¾‘ï¼‰"""
        from dice_evaluation_script import DICEValidationEvaluator
        from src.dice.dice_simplified import SimplifiedDICEConfig
        
        temp_config = SimplifiedDICEConfig()
        temp_evaluator = DICEValidationEvaluator(temp_config)
        return temp_evaluator.generate_validation_report(agreement_metrics, correlation_metrics, results, gold_labels, output_file)