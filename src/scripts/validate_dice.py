#!/usr/bin/env python3
"""
DICEå‡†ç¡®ç‡è¯„ä¼°è„šæœ¬
ç”¨äºéªŒè¯DICEç³»ç»Ÿçš„å¯ä¿¡åº¦ï¼Œé€šè¿‡ä¸äººå·¥æ ‡æ³¨çš„"é‡‘æ ‡å‡†"è¿›è¡Œå¯¹æ¯”
"""

import argparse
import logging
import sys
import os
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from src.evaluation.dice_engine import SimplifiedDICEConfig
from src.utils.ragas_impl import RagasConfig
from src.evaluation.validator import UnifiedValidationEvaluator

def main():
    parser = argparse.ArgumentParser(description="å¤šRAGç³»ç»Ÿå‡†ç¡®ç‡éªŒè¯è¯„ä¼°")
    parser.add_argument("--qacg_files", nargs="+", required=True,
                       help="QACGæ–‡ä»¶è·¯å¾„åˆ—è¡¨")
    parser.add_argument("--num_samples", type=int, default=200,
                       help="é‡‡æ ·è¯„ä¼°å¯¹æ•°é‡")
    parser.add_argument("--annotation_file", type=str, 
                       default="dice_human_annotations.json",
                       help="äººå·¥æ ‡æ³¨æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="dice_validation_output",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--random_seed", type=int, default=42,
                       help="éšæœºç§å­")
    parser.add_argument("--llm_model", type=str, default="deepseek-chat",
                       help="LLMæ¨¡å‹")
    parser.add_argument("--tournament_result_file", type=str, 
                       default="dice_simplified_output/tournament_result.json",
                       help="tournamentç»“æœæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºå¤ç”¨å·²æœ‰åˆ¤æ–­")
    parser.add_argument("--ragas", action="store_true",
                       help="ä½¿ç”¨RAGASæ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼ˆé»˜è®¤ä½¿ç”¨DICEæ–¹æ³•ï¼‰")
    parser.add_argument("--ragas_metrics", nargs="+", 
                       default=["answer_relevancy", "context_precision", "context_recall", "faithfulness", "answer_correctness"],
                       help="RAGASè¯„ä¼°æŒ‡æ ‡åˆ—è¡¨")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # æ ¹æ®è¯„ä¼°æ–¹æ³•åˆ›å»ºé…ç½®å’Œè¯„ä¼°å™¨
    if args.ragas:
        # RAGASé…ç½® - ä½¿ç”¨DeepSeek
        ragas_config = RagasConfig(
            llm_model=args.llm_model,
            metrics=args.ragas_metrics,
            api_key=os.environ.get("DEEPSEEK_API_KEY", "xxxxxxx"),  # ä½¿ç”¨DeepSeek API
            base_url="https://api.deepseek.com"
        )
        evaluator = UnifiedValidationEvaluator(
            evaluation_method="ragas",
            ragas_config=ragas_config
        )
        evaluation_method = "RAGAS"
    else:
        # DICEé…ç½®
        dice_config = SimplifiedDICEConfig(
            llm_model=args.llm_model,
            output_dir=str(output_dir),
            api_key=os.environ.get("DEEPSEEK_API_KEY", ""),
            base_url="https://api.deepseek.com"
        )
        evaluator = UnifiedValidationEvaluator(
            evaluation_method="dice",
            dice_config=dice_config,
            tournament_result_file=args.tournament_result_file
        )
        evaluation_method = "DICE"
    
    print(f"ğŸ”¬ {evaluation_method}ç³»ç»ŸéªŒè¯è¯„ä¼°")
    print(f"ğŸ“ QACGæ–‡ä»¶æ•°é‡: {len(args.qacg_files)}")
    print(f"ğŸ“Š é‡‡æ ·æ•°é‡: {args.num_samples}")
    print(f"ğŸ”§ è¯„ä¼°æ–¹æ³•: {evaluation_method}")
    
    try:
        # æ­¥éª¤1: é‡‡æ ·è¯„ä¼°å¯¹
        print("\nğŸ“‹ æ­¥éª¤1: é‡‡æ ·è¯„ä¼°å¯¹...")
        evaluation_pairs = evaluator.sample_evaluation_pairs(
            args.qacg_files, args.num_samples, args.random_seed
        )
        
        # ä¿å­˜é‡‡æ ·ç»“æœ
        pairs_file = output_dir / "evaluation_pairs.json"
        with open(pairs_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_pairs, f, ensure_ascii=False, indent=2)
        print(f"âœ… é‡‡æ ·å®Œæˆï¼Œä¿å­˜è‡³: {pairs_file}")
        
        # æ­¥éª¤1.5: æ£€æŸ¥æˆ–åˆ›å»ºäººå·¥æ ‡æ³¨æ–‡ä»¶
        print(f"\nğŸ“ æ­¥éª¤1.5: æ£€æŸ¥äººå·¥æ ‡æ³¨æ–‡ä»¶: {args.annotation_file}")
        annotation_file_path = Path(args.annotation_file)
        
        if not annotation_file_path.exists():
            print("âš ï¸  äººå·¥æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ ‡æ³¨æ¨¡æ¿...")
            evaluator._create_annotation_template(args.annotation_file)
            print(f"âœ… å·²åˆ›å»ºæ ‡æ³¨æ¨¡æ¿: {args.annotation_file}")
            print("ğŸ’¡ æ ‡æ³¨è¯´æ˜:")
            print("   - æ¯ä¸ªpair_idéœ€è¦3ä½ä¸“å®¶ç‹¬ç«‹æŠ•ç¥¨")
            print("   - æŠ•ç¥¨é€‰é¡¹: 'A wins'ã€'B wins'ã€'Tie'")
            print("   - è¯·æ ¹æ®æ£€ç´¢è´¨é‡å’Œå›ç­”è´¨é‡è¿›è¡Œåˆ¤æ–­")
            print("âš ï¸  å¦‚éœ€ç”ŸæˆéªŒè¯æŠ¥å‘Šï¼Œè¯·å…ˆå®Œæˆæ ‡æ³¨åå†è¿è¡Œ")
            print("âœ… ç¨‹åºå°†ç»§ç»­æ‰§è¡ŒDICEè¯„ä¼°...\n")
        else:
            print(f"âœ… äººå·¥æ ‡æ³¨æ–‡ä»¶å·²å­˜åœ¨: {args.annotation_file}")
        
        # æ­¥éª¤2: æ£€æŸ¥æˆ–è¿è¡ŒDICEè¯„ä¼°
        results_file = output_dir / f"{evaluation_method.lower()}_results.json"
        evaluation_results = None
        
        print(f"\nğŸ¤– æ­¥éª¤2: æ£€æŸ¥{evaluation_method}è¯„ä¼°ç»“æœæ–‡ä»¶...")
        if results_file.exists():
            print(f"âœ… å‘ç°å·²æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶: {results_file}")
            print("ğŸ“‚ åŠ è½½å·²æœ‰è¯„ä¼°ç»“æœï¼Œè·³è¿‡é‡æ–°è¯„ä¼°...")
            
            try:
                with open(results_file, 'r', encoding='utf-8') as f:
                    evaluation_results = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½ {len(evaluation_results)} ä¸ªè¯„ä¼°ç»“æœ")
                
                # éªŒè¯è¯„ä¼°ç»“æœæ˜¯å¦ä¸å½“å‰é‡‡æ ·å¯¹åŒ¹é…
                if len(evaluation_results) != len(evaluation_pairs):
                    print(f"âš ï¸  è¯„ä¼°ç»“æœæ•°é‡({len(evaluation_results)})ä¸é‡‡æ ·å¯¹æ•°é‡({len(evaluation_pairs)})ä¸åŒ¹é…")
                    print("ğŸ”„ å°†é‡æ–°è¿è¡Œè¯„ä¼°...")
                    evaluation_results = None
                else:
                    print("âœ… è¯„ä¼°ç»“æœæ•°é‡åŒ¹é…ï¼Œå°†ä½¿ç”¨å·²æœ‰ç»“æœ")
            except Exception as e:
                print(f"âŒ åŠ è½½è¯„ä¼°ç»“æœå¤±è´¥: {e}")
                print("ğŸ”„ å°†é‡æ–°è¿è¡Œè¯„ä¼°...")
                evaluation_results = None
        
        # å¦‚æœæ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆçš„è¯„ä¼°ç»“æœï¼Œåˆ™è¿è¡Œè¯„ä¼°
        if evaluation_results is None:
            print(f"\nğŸ¤– è¿è¡Œ{evaluation_method}ç³»ç»Ÿè¯„ä¼°...")
            evaluation_results = evaluator.run_evaluation(evaluation_pairs)
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… {evaluation_method}è¯„ä¼°å®Œæˆï¼Œä¿å­˜è‡³: {results_file}")
        
        # æ­¥éª¤3: å°è¯•åŠ è½½äººå·¥æ ‡æ³¨å¹¶ç”ŸæˆæŠ¥å‘Š
        print(f"\nğŸ“Š æ­¥éª¤3: æ£€æŸ¥äººå·¥æ ‡æ³¨å®Œæˆæƒ…å†µ...")
        
        try:
            # å°è¯•åŠ è½½äººå·¥æ ‡æ³¨
            gold_labels = evaluator.load_human_annotations(args.annotation_file)
            
            if len(gold_labels) == 0:
                print("âš ï¸  äººå·¥æ ‡æ³¨æ–‡ä»¶å­˜åœ¨ä½†æ²¡æœ‰æœ‰æ•ˆæ ‡æ³¨")
                print("ğŸ’¡ è¯·å®Œæˆæ ‡æ³¨åé‡æ–°è¿è¡Œä»¥ç”ŸæˆéªŒè¯æŠ¥å‘Š")
                print(f"âœ… DICEè¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_file}")
                return
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(gold_labels)} ä¸ªäººå·¥æ ‡æ³¨")
            
            # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
            print("\nğŸ“Š æ­¥éª¤4: è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡...")
            agreement_metrics = evaluator.calculate_agreement(evaluation_results, gold_labels)
            
            # è®¡ç®—Eloç›¸å…³æ€§
            print("ğŸ“Š æ­¥éª¤5: è®¡ç®—Eloæ’åºç›¸å…³æ€§...")
            correlation_metrics = evaluator.calculate_elo_correlation(evaluation_results, gold_labels)
            
            # ç”ŸæˆæŠ¥å‘Š
            print("ğŸ“ æ­¥éª¤6: ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
            timestamp = datetime.now().strftime("%Y%m%d%H%M")
            report_file = output_dir / f"validation_report_{timestamp}.json"
            evaluator.generate_validation_report(
                agreement_metrics, correlation_metrics, evaluation_results, gold_labels, str(report_file)
            )
            
            print(f"\nâœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
            
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ç”ŸæˆéªŒè¯æŠ¥å‘Š: {e}")
            print("ğŸ’¡ è¿™å¯èƒ½æ˜¯å› ä¸º:")
            print("   1. äººå·¥æ ‡æ³¨æ–‡ä»¶å°šæœªå®Œæˆæ ‡æ³¨")
            print("   2. æ ‡æ³¨æ ¼å¼ä¸æ­£ç¡®")
            print("   3. expert_voteså­—æ®µä¸ºç©º")
            print(f"\nâœ… DICEè¯„ä¼°å·²å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {results_file}")
            print("ğŸ“ å®Œæˆäººå·¥æ ‡æ³¨åï¼Œå¯é‡æ–°è¿è¡Œè„šæœ¬ç”ŸæˆéªŒè¯æŠ¥å‘Š")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        print(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()
