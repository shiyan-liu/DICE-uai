#!/usr/bin/env python3
"""
åŸºäºæœ¬åœ°DeepSeek-R1æ¨¡å‹çš„Pairwiseåˆ¤å†³å™¨ - æ˜¾å­˜ä¼˜åŒ–ç‰ˆ
å®ç°æ·±åº¦æ€è€ƒ(800 tokens) + å¼ºåˆ¶å­—æ¯è¾“å‡º + å…¨é¢è¯æ®åˆ†æ
"""

import torch
import logging
import gc
import numpy as np
from typing import Dict, Any, Tuple, List, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM


class LocalPairwiseJudge:
    """åŸºäºæœ¬åœ°DeepSeek-R1çš„Pairwiseåˆ¤å†³å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger("DICE.LocalPairwise")
        
        # æœ¬åœ°æ¨¡å‹è·¯å¾„
        self.model_path = "/root/autodl-tmp/deepseek-deployment/deepseek-r1-8b"
        
        # æ§åˆ¶æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
        self.enable_deep_thinking = getattr(config, 'enable_deep_thinking', True)  # é»˜è®¤å¼€å¯æ·±åº¦æ€è€ƒ
        
        # æ¨¡å‹å’Œtokenizer
        self.model = None
        self.tokenizer = None
        self.choice_tokens = {}
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹ - æ˜¾å­˜ä¼˜åŒ–ç‰ˆ"""
        self.logger.info(f"ğŸš€ åŠ è½½æœ¬åœ°DeepSeek-R1æ¨¡å‹: {self.model_path}")
        
        try:
            # æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨float16å’Œhigh_cpu_mem_usage=False
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=True
            )
            
            # æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨float16ï¼Œdevice_map autoï¼Œä½CPUå†…å­˜ä½¿ç”¨
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                device_map="auto",
                torch_dtype=torch.float16,  # å‡å°‘ä¸€åŠæ˜¾å­˜
                trust_remote_code=True,
                low_cpu_mem_usage=True      # å‡å°‘CPUå†…å­˜ä½¿ç”¨
            )
            
            # æ˜¾å­˜ä¼˜åŒ–ï¼šå…³é—­ç¼“å­˜
            if hasattr(self.model.config, 'use_cache'):
                self.model.config.use_cache = False
            
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            
            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # é¢„è®¡ç®—é€‰æ‹©tokençš„ID (A/B/T ä¸‰é€‰é¡¹)
            self.choice_tokens = {
                "A": self.tokenizer.convert_tokens_to_ids("A"),
                "B": self.tokenizer.convert_tokens_to_ids("B"), 
                "T": self.tokenizer.convert_tokens_to_ids("T")
            }
            
            self.logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            self.logger.info(f"ğŸ¯ é€‰æ‹©token IDs: {self.choice_tokens}")
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def judge_pair(
        self, 
        question: str, 
        qa_a: Dict[str, Any], 
        qa_b: Dict[str, Any], 
        granularity: str,
        atoms: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """æ‰§è¡Œpairwiseåˆ¤å†³"""
        
        self.logger.info(f"æ‰§è¡Œ{granularity}ç²’åº¦åˆ¤å†³")
        
        try:
            # è°ƒç”¨logitsåˆ¤å†³ 
            logits_result = self._get_logits_judgment(question, qa_a, qa_b)
            
            if logits_result is None:
                return self._create_default_judgment("åˆ¤å†³å¤±è´¥: æ— æ³•è·å–logitsç»“æœ")
            
            # è§£æåˆ¤å†³ç»“æœ
            parsed_judgment = self._parse_logits_result(logits_result, granularity)
            if parsed_judgment is None:
                return self._create_default_judgment("åˆ¤å†³å¤±è´¥: æ— æ³•è§£ælogitsç»“æœ")
            
            return parsed_judgment
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ¤å†³å¤±è´¥: {e}")
            return self._create_default_judgment(f"åˆ¤å†³å¤±è´¥: {str(e)}")
    
    def _get_logits_judgment(self, question: str, qa_a: Dict[str, Any], qa_b: Dict[str, Any]) -> Dict[str, Any]:
        """DeepSeek-R1åˆ¤å†³æ¨¡å¼ - å¯é€‰æ·±åº¦æ€è€ƒæˆ–ç›´æ¥è¾“å‡º"""
        
        try:
            if self.enable_deep_thinking:
                self.logger.info("ğŸ§  ä½¿ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼")
                return self._get_logits_with_deep_thinking(question, qa_a, qa_b)
            else:
                self.logger.info("âš¡ ä½¿ç”¨ç›´æ¥è¾“å‡ºæ¨¡å¼")
                return self._get_logits_direct_mode(question, qa_a, qa_b)
                
        except Exception as e:
            self.logger.error(f"âŒ Logitsåˆ¤å†³å¤±è´¥: {e}")
            return None
    
    def _get_logits_with_deep_thinking(self, question: str, qa_a: Dict[str, Any], qa_b: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦æ€è€ƒæ¨¡å¼"""
        # ç¬¬ä¸€é˜¶æ®µï¼šæ·±åº¦æ€è€ƒ
        thinking_result = self._generate_deep_thinking(question, qa_a, qa_b)
        if thinking_result is None:
            return None
        
        # æ˜¾å­˜æ¸…ç†
        torch.cuda.empty_cache()
        gc.collect()
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¼ºåˆ¶å­—æ¯è¾“å‡º (A/B/T)
        choice_result = self._generate_final_choice(thinking_result["full_context"])
        if choice_result is None:
            return None
        
        # æ˜¾å­˜æ¸…ç†
        torch.cuda.empty_cache()
        gc.collect()
        
        # éªŒè¯ä¸€è‡´æ€§
        reasoning_choice = self._extract_choice_from_reasoning(thinking_result["reasoning"])
        consistent = (reasoning_choice == choice_result["choice"]) if reasoning_choice else True
        
        # ç®€åŒ–æ—¥å¿—è¾“å‡º
        self.logger.info(f"âœ… æ·±åº¦æ€è€ƒåˆ¤å†³å®Œæˆ: {choice_result['choice']} (æ¦‚ç‡: A={choice_result['prob_a']:.3f}, B={choice_result['prob_b']:.3f}, T={choice_result['prob_t']:.3f})")
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        return {
            "reasoning": thinking_result["reasoning"],
            "choice": choice_result["choice"],
            "logit_a": choice_result["logit_a"],
            "logit_b": choice_result["logit_b"],
            "logit_t": choice_result["logit_t"],
            "prob_a": choice_result["prob_a"],
            "prob_b": choice_result["prob_b"],
            "prob_t": choice_result["prob_t"],
            "raw_response": choice_result.get("final_answer", ""),
            "generated_token": choice_result["generated_token"],
            "verification_consistent": consistent,
            "reasoning_choice": reasoning_choice
        }
    
    def _get_logits_direct_mode(self, question: str, qa_a: Dict[str, Any], qa_b: Dict[str, Any]) -> Dict[str, Any]:
        """ç›´æ¥è¾“å‡ºæ¨¡å¼ - å®Œæ•´è¾“å‡ºåˆ¤å†³åæå–æœ€åtokençš„logits"""
        
        # æ„å»ºç›´æ¥åˆ¤å†³prompt
        prompt = self._build_direct_judgment_prompt(question, qa_a, qa_b)
        
        try:
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True,
                max_length=4096,
                padding=False
            )
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # è®©æ¨¡å‹å®Œæ•´è¾“å‡ºåˆ¤å†³è¿‡ç¨‹ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´å®Œæˆåˆ†æ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1500,  # å¢åŠ åˆ°1500ï¼Œç¡®ä¿åˆ†æå®Œæ•´
                    do_sample=True,
                    temperature=0.2,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.05,
                    return_dict_in_generate=True,
                    output_scores=True  # è·å–æ¯ä¸ªstepçš„logits
                )
            
            # æå–å®Œæ•´ç”Ÿæˆçš„åˆ¤å†³
            input_length = inputs["input_ids"].shape[1]
            generated_tokens = outputs.sequences[0][input_length:]
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            print(f"\nâš¡ DeepSeek-R1 ç›´æ¥åˆ¤å†³:")
            print(f"   å®Œæ•´è¾“å‡º: {generated_text}")
            
            # å…³é”®ï¼šä»æœ€åä¸€ä¸ªtokenè·å–logitsï¼ˆè¿™æ˜¯æ¨¡å‹å®Œæ•´æ€è€ƒåçš„å†³ç­–ï¼‰
            if outputs.scores and len(outputs.scores) > 0 and len(generated_tokens) > 0:
                # è·å–æœ€åä¸€ä¸ªç”Ÿæˆæ­¥éª¤çš„logits
                last_step_logits = outputs.scores[-1][0]  # æœ€åä¸€æ­¥çš„logits
                last_token_id = generated_tokens[-1].item()  # æœ€åä¸€ä¸ªtoken ID
                last_token_text = self.tokenizer.decode([last_token_id], skip_special_tokens=True)
                
                print(f"   æœ€åä¸€ä¸ªtoken: '{last_token_text}' (ID: {last_token_id})")
                
                # åŸºäºæœ€åä¸€æ­¥çš„logitsè®¡ç®—A/B/Tæ¦‚ç‡
                logits_dict = self._compute_logits_directly(last_step_logits)
                
                # ç¡®å®šé€‰æ‹©ï¼šä¼˜å…ˆä»æœ€åtokenåˆ¤æ–­ï¼Œå…¶æ¬¡ä»æ–‡æœ¬åˆ†æ
                if last_token_text.strip().upper() in ["A", "B", "T"]:
                    choice = last_token_text.strip().upper()
                    print(f"   âœ… æœ€åtokenç›´æ¥æ˜¯é€‰æ‹©: {choice}")
                else:
                    # ä»å®Œæ•´æ–‡æœ¬ä¸­æå–é€‰æ‹©
                    choice = self._extract_choice_from_text(generated_text)
                    if not choice:
                        # å¦‚æœæ–‡æœ¬åˆ†æä¹Ÿå¤±è´¥ï¼ŒåŸºäºlogitsæ¦‚ç‡é€‰æ‹©
                        if logits_dict["prob_a"] > logits_dict["prob_b"] and logits_dict["prob_a"] > logits_dict["prob_t"]:
                            choice = "A"
                        elif logits_dict["prob_b"] > logits_dict["prob_a"] and logits_dict["prob_b"] > logits_dict["prob_t"]:
                            choice = "B"
                        else:
                            choice = "T"
                    print(f"   ğŸ” ä»æ–‡æœ¬/logitsæ¨æ–­é€‰æ‹©: {choice}")
                
            else:
                # æ— æ³•è·å–logitsï¼Œä½¿ç”¨æ–‡æœ¬åˆ†æ
                choice = self._extract_choice_from_text(generated_text)
                if not choice:
                    choice = "T"
                logits_dict = self._create_fallback_logits(choice)
                print(f"   âš ï¸ æ— æ³•è·å–logitsï¼Œä½¿ç”¨æ–‡æœ¬åˆ†æ: {choice}")
            
            print(f"   åŸå§‹Logits: A={logits_dict['logit_a']:.3f}, B={logits_dict['logit_b']:.3f}, T={logits_dict['logit_t']:.3f}")
            print(f"   æ¦‚ç‡åˆ†å¸ƒ: A={logits_dict['prob_a']:.3f}, B={logits_dict['prob_b']:.3f}, T={logits_dict['prob_t']:.3f}")
            print(f"   æœ€ç»ˆé€‰æ‹©: {choice}\n")
            
            # æ¸…ç†
            del inputs, outputs
            
            self.logger.info(f"âœ… ç›´æ¥åˆ¤å†³å®Œæˆ: {choice} (æ¦‚ç‡: A={logits_dict['prob_a']:.3f}, B={logits_dict['prob_b']:.3f}, T={logits_dict['prob_t']:.3f})")
            
            return {
                "reasoning": generated_text,
                "choice": choice,
                "logit_a": logits_dict["logit_a"],
                "logit_b": logits_dict["logit_b"],
                "logit_t": logits_dict["logit_t"],
                "prob_a": logits_dict["prob_a"],
                "prob_b": logits_dict["prob_b"],
                "prob_t": logits_dict["prob_t"],
                "raw_response": generated_text,
                "generated_token": choice,
                "verification_consistent": True,
                "reasoning_choice": choice
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç›´æ¥åˆ¤å†³å¤±è´¥: {e}")
            return None
    
    def _generate_deep_thinking(self, question: str, qa_a: Dict[str, Any], qa_b: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ç¬¬ä¸€é˜¶æ®µï¼šDeepSeek-R1æ·±åº¦æ€è€ƒç”Ÿæˆï¼ˆ800 tokensï¼‰"""
        
        # æ„å»ºå®Œæ•´çš„åˆ†æprompt
        prompt = self._build_analysis_prompt(question, qa_a, qa_b)
        
        try:
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True,
                max_length=3072,  # å‡å°‘è¾“å…¥é•¿åº¦ï¼Œä¸ºæ€è€ƒç•™å‡ºæ›´å¤šç©ºé—´ï¼ˆ3072 + 2048 = 5120 < 8192æ¨¡å‹ä¸Šä¸‹æ–‡ï¼‰
                padding=False
            )
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # DeepSeek-R1æ·±åº¦æ€è€ƒç”Ÿæˆï¼šç¡®ä¿ä¸è¢«æˆªæ–­
            with torch.no_grad():
                reasoning_outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1400,
                    do_sample=True,    
                    temperature=0.1,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.05,
                    eos_token_id=self.tokenizer.eos_token_id
                )
                
            # æå–ç”Ÿæˆçš„æ¨ç†æ–‡æœ¬
            input_length = inputs["input_ids"].shape[1]
            reasoning_generated = reasoning_outputs[0][input_length:]
            reasoning_text = self.tokenizer.decode(reasoning_generated, skip_special_tokens=True)
            
            # ğŸ” æ£€æŸ¥æ·±åº¦æ€è€ƒæ˜¯å¦å®Œæ•´
            is_complete = self._check_reasoning_completeness(reasoning_text)
            if not is_complete:
                self.logger.warning("âš ï¸ æ·±åº¦æ€è€ƒå¯èƒ½æœªå®Œæˆï¼Œä½†ç»§ç»­è¿›è¡Œåˆ¤å†³")
            
            # ğŸ–¨ï¸ è°ƒè¯•ï¼šæ·±åº¦æ€è€ƒå®Œæˆ
            print(f"\n{'='*60}")
            print(f"ğŸ§  DeepSeek-R1 æ·±åº¦æ€è€ƒå†…å®¹ ({'å®Œæ•´' if is_complete else 'å¯èƒ½æˆªæ–­'}):")
            print(f"{'='*60}")
            print(reasoning_text)
            print(f"{'='*60}")
            print(f"ğŸ“Š æ€è€ƒé•¿åº¦: {len(reasoning_text)} å­—ç¬¦, {len(reasoning_generated)} tokens")
            print(f"âœ… æ€è€ƒå®Œæ•´æ€§: {'å®Œæ•´' if is_complete else 'å¯èƒ½æˆªæ–­'}\n")
            
            # æ¸…ç†
            del inputs, reasoning_outputs, reasoning_generated
            
            # æ„å»ºç¬¬äºŒé˜¶æ®µçš„ä¸Šä¸‹æ–‡       
            full_context = prompt + reasoning_text + "\n\nåŸºäºä»¥ä¸Šæ·±åº¦åˆ†æï¼Œæˆ‘çš„æœ€ç»ˆåˆ¤å†³æ˜¯ï¼š"
            
            return {
                "reasoning": reasoning_text,
                "full_context": full_context
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ·±åº¦æ€è€ƒç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _generate_final_choice(self, full_context: str) -> Optional[Dict[str, Any]]:
        """ç¬¬äºŒé˜¶æ®µï¼šå¼ºåˆ¶ç”Ÿæˆå•å­—æ¯é€‰æ‹©A/B/T"""
        
        # æ„å»ºéå¸¸æ˜ç¡®çš„å•å­—æ¯é€‰æ‹©prompt
        choice_prompt = full_context + "\n\nç°åœ¨è¯·ç»™å‡ºä½ çš„æœ€ç»ˆåˆ¤å†³ï¼Œåªè¾“å‡ºä¸€ä¸ªå­—æ¯ï¼š\n\nå¦‚æœç³»ç»ŸAæ›´å¥½ï¼Œè¾“å‡ºï¼šA\nå¦‚æœç³»ç»ŸBæ›´å¥½ï¼Œè¾“å‡ºï¼šB\nå¦‚æœä¸¤è€…ç›¸å½“ï¼Œè¾“å‡ºï¼šT\n\næˆ‘çš„é€‰æ‹©æ˜¯ï¼š"
        
        try:
            inputs = self.tokenizer(
                choice_prompt, 
                return_tensors="pt",
                truncation=True,
                max_length=4096,
                padding=False
            )
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # å¼ºåˆ¶çº¦æŸç”Ÿæˆï¼šåªå…è®¸A/B/T
            choice_found = None
            original_logits = None
            generated_tokens = []
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits[0, -1, :]  # è·å–æœ€åä½ç½®çš„logits
                original_logits = logits.clone()  # ä¿å­˜åŸå§‹logits
                
                # åˆ›å»ºçº¦æŸlogitsï¼šåªå…è®¸Aã€Bã€T token
                constrained_logits = torch.full_like(logits, -float('inf'))
                constrained_logits[self.choice_tokens["A"]] = logits[self.choice_tokens["A"]]
                constrained_logits[self.choice_tokens["B"]] = logits[self.choice_tokens["B"]]
                constrained_logits[self.choice_tokens["T"]] = logits[self.choice_tokens["T"]]
                
                # ä½¿ç”¨çº¦æŸåçš„logitsè¿›è¡Œé‡‡æ ·
                next_token_id = torch.multinomial(
                    torch.softmax(constrained_logits / 0.3, dim=-1), 
                    num_samples=1
                ).item()
                
                # ç¡®å®šé€‰æ‹©
                if next_token_id == self.choice_tokens["A"]:
                    choice_found = "A"
                elif next_token_id == self.choice_tokens["B"]:
                    choice_found = "B"
                elif next_token_id == self.choice_tokens["T"]:
                    choice_found = "T"
                else:
                    # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä½œä¸ºå®‰å…¨æªæ–½
                    choice_found = None
                
                generated_tokens = [next_token_id]
            
            # æ¸…ç†inputs
            del inputs
            
            if choice_found and original_logits is not None:
                # ç”Ÿæˆçš„æœ€ç»ˆå›ç­”ï¼ˆä»…æ˜¾ç¤ºç®€çŸ­å†…å®¹ï¼‰
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                
                # ğŸ–¨ï¸ è°ƒè¯•ï¼šæ‰“å°æœ€ç»ˆå›ç­”
                print(f"\nğŸ¯ DeepSeek-R1 æœ€ç»ˆå›ç­”:")
                print(f"   ç”Ÿæˆå†…å®¹: '{generated_text}'")
                print(f"   æœ€åä¸€ä¸ªtoken: '{choice_found}'")
                
                # è®¡ç®—A/B/Tçš„logitså’Œæ¦‚ç‡ï¼ˆä½¿ç”¨åŸå§‹æœªçº¦æŸçš„logitsï¼‰
                logits_dict = self._compute_logits_directly(original_logits)
                
                print(f"   Logits: A={logits_dict['logit_a']:.3f}, B={logits_dict['logit_b']:.3f}, T={logits_dict['logit_t']:.3f}")
                print(f"   æ¦‚ç‡: A={logits_dict['prob_a']:.3f}, B={logits_dict['prob_b']:.3f}, T={logits_dict['prob_t']:.3f}")
                print(f"   ç¡®å®šé€‰æ‹©: {choice_found}\n")
                
                return {
                    "choice": choice_found,
                    "generated_token": choice_found,
                    "final_answer": generated_text,
                    **logits_dict
                }
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé€‰æ‹©ï¼Œåˆ†æç”Ÿæˆçš„å†…å®¹
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                
                # ğŸ–¨ï¸ è°ƒè¯•ï¼šæ‰“å°æœªæ‰¾åˆ°é€‰æ‹©çš„æƒ…å†µ
                print(f"\nâš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆA/B/Té€‰æ‹©!")
                print(f"   ç”Ÿæˆçš„å†…å®¹: '{generated_text}'")
                print(f"   ç”Ÿæˆçš„Token IDs: {generated_tokens}")
                
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆé€‰æ‹©A/B/Tï¼Œç”Ÿæˆå†…å®¹: '{generated_text}'")
                
                # å°è¯•ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–A/B/T
                fallback_choice = self._extract_choice_from_text(generated_text)
                if fallback_choice:
                    print(f"   ğŸ”„ ä»æ–‡æœ¬æå–åˆ°é€‰æ‹©: {fallback_choice}\n")
                    self.logger.info(f"ğŸ”„ ä»æ–‡æœ¬ä¸­æå–åˆ°é€‰æ‹©: {fallback_choice}")
                    # è¿”å›æ¨¡æ‹Ÿçš„logits
                    return self._create_fallback_logits(fallback_choice)
                else:
                    print(f"   âŒ æ— æ³•æå–ä»»ä½•æœ‰æ•ˆé€‰æ‹©\n")
                    self.logger.warning(f"âš ï¸ æ— æ³•ä»æ¨ç†æˆ–logitsä¸­æå–æ˜ç¡®é€‰æ‹©")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ æœ€ç»ˆå›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _check_reasoning_completeness(self, reasoning_text: str) -> bool:
        """æ£€æŸ¥æ¨ç†æ˜¯å¦å®Œæ•´ - åˆ¤æ–­æ˜¯å¦è¢«æˆªæ–­"""
        if not reasoning_text:
            return False
        
        # æ£€æŸ¥æ¨ç†æ˜¯å¦æœ‰æ˜ç¡®çš„ç»“è®ºæ ‡å¿—
        completion_indicators = [
            "æœ€ç»ˆåˆ¤å†³", "ç»“è®º", "å› æ­¤", "æ€»ç»“", "ç»¼ä¸Šæ‰€è¿°", 
            "ç³»ç»ŸA", "ç³»ç»ŸB", "æ›´å¥½", "è·èƒœ", "ä¼˜äº",
            "åˆ¤æ–­", "é€‰æ‹©", "å†³å®š"
        ]
        
        reasoning_lower = reasoning_text.lower()
        has_conclusion = any(indicator in reasoning_lower for indicator in completion_indicators)
        
        # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦çªç„¶æˆªæ–­ï¼ˆä»¥ä¸å®Œæ•´çš„å¥å­ç»“å°¾ï¼‰
        text_stripped = reasoning_text.strip()
        if not text_stripped:
            return False
        
        # æ£€æŸ¥æœ€åçš„å­—ç¬¦æ˜¯å¦è¡¨æ˜å®Œæ•´æ€§
        last_chars = text_stripped[-50:].lower()  # æ£€æŸ¥æœ€å50ä¸ªå­—ç¬¦
        
        # å¦‚æœä»¥å¥å·ã€æ„Ÿå¹å·ã€é—®å·ç»“å°¾ï¼Œä¸”æœ‰ç»“è®ºæ€§å†…å®¹ï¼Œè®¤ä¸ºæ˜¯å®Œæ•´çš„
        ends_properly = text_stripped.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ'))
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç³»ç»Ÿå¯¹æ¯”çš„å†…å®¹
        has_comparison = ("ç³»ç»Ÿa" in reasoning_lower and "ç³»ç»Ÿb" in reasoning_lower) or \
                        ("system a" in reasoning_lower and "system b" in reasoning_lower)
        
        # ç»¼åˆåˆ¤æ–­
        is_complete = has_conclusion and (ends_properly or len(reasoning_text) > 1000) and has_comparison
        
        return is_complete
    
    def _compute_logits_directly(self, logits_tensor: torch.Tensor) -> Dict[str, float]:
        """ç›´æ¥è®¡ç®—logits - è·å–A/B/Tçš„çœŸå®æ¦‚ç‡"""
        
        # è·å–A/B/Tçš„logits
        logit_a = float(logits_tensor[self.choice_tokens["A"]].cpu())
        logit_b = float(logits_tensor[self.choice_tokens["B"]].cpu())
        logit_t = float(logits_tensor[self.choice_tokens["T"]].cpu())
        
        # è®¡ç®—ä¸‰é€‰é¡¹æ¦‚ç‡
        logits_abc = torch.tensor([logit_a, logit_b, logit_t])
        probs = torch.softmax(logits_abc, dim=0)
        
        return {
                "logit_a": logit_a,
                "logit_b": logit_b,
                "logit_t": logit_t,
            "prob_a": float(probs[0]),
            "prob_b": float(probs[1]),
            "prob_t": float(probs[2])
        }
    
    def _extract_choice_from_reasoning(self, reasoning: str) -> Optional[str]:
        """ä»æ¨ç†æ–‡æœ¬ä¸­æå–é€‰æ‹©"""
        if not reasoning:
            return None
        
        # å¯»æ‰¾æ˜ç¡®çš„é€‰æ‹©è¡¨è¾¾
        choice_patterns = [
            "é€‰æ‹©A", "é€‰æ‹©B", "é€‰æ‹©T",
            "åˆ¤å†³A", "åˆ¤å†³B", "åˆ¤å†³T", 
            "ç­”æ¡ˆA", "ç­”æ¡ˆB", "ç­”æ¡ˆT",
            "ç³»ç»ŸAæ›´å¥½", "ç³»ç»ŸBæ›´å¥½", "ä¸¤ç³»ç»Ÿç›¸å½“"
        ]
        
        reasoning_lower = reasoning.lower()
        
        for pattern in choice_patterns:
            if pattern.lower() in reasoning_lower:
                if "a" in pattern.lower():
                    return "A"
                elif "b" in pattern.lower():
                    return "B"
                elif "t" in pattern.lower() or "ç›¸å½“" in pattern:
                    return "T"
        
        return None
    
    def _extract_choice_from_text(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–A/B/Té€‰æ‹©"""
        if not text:
            return None
        
        text = text.strip().upper()
        if "A" in text:
            return "A"
        elif "B" in text:
            return "B"
        elif "T" in text:
            return "T"
        
        return None
    
    def _create_fallback_logits(self, choice: str) -> Dict[str, Any]:
        """åˆ›å»ºå›é€€logitsï¼ˆåŸºäºæ–‡æœ¬åˆ†æçš„é€‰æ‹©ï¼‰"""
        # æ¨¡æ‹Ÿlogitsï¼šç»™é€‰ä¸­çš„é€‰é¡¹é«˜åˆ†ï¼Œå…¶ä»–ä½åˆ†
        if choice == "A":
            logit_a, logit_b, logit_t = 2.0, -1.0, -1.0
        elif choice == "B":
            logit_a, logit_b, logit_t = -1.0, 2.0, -1.0
        else:  # T
            logit_a, logit_b, logit_t = -1.0, -1.0, 2.0
        
        # è®¡ç®—æ¦‚ç‡
        logits_tensor = torch.tensor([logit_a, logit_b, logit_t])
        probs = torch.softmax(logits_tensor, dim=0)
        
        return {
            "choice": choice,
            "generated_token": choice,
            "logit_a": logit_a,
            "logit_b": logit_b,
            "logit_t": logit_t,
            "prob_a": float(probs[0]),
            "prob_b": float(probs[1]),
            "prob_t": float(probs[2])
        }
    
    def _build_analysis_prompt(self, question: str, qa_a: Dict[str, Any], qa_b: Dict[str, Any]) -> str:
        """æ„å»ºæ·±åº¦åˆ†æprompt - åŒ…å«å…¨é¢çš„è¯æ®å’Œç­”æ¡ˆä¿¡æ¯"""
        
        # å®‰å…¨è·å–æ•°æ®ï¼Œå¤„ç†ç©ºå­—æ®µ
        answer_a = qa_a.get("rag_answer", qa_a.get("answer", "æ— å›ç­”"))
        answer_b = qa_b.get("rag_answer", qa_b.get("answer", "æ— å›ç­”"))
        expected_answer = qa_a.get("expected_answer", "")
        groundtruth = qa_a.get("groundtruth", qa_b.get("groundtruth", ""))
        
        # è·å–æ£€ç´¢è¯æ®
        evidence_a = qa_a.get("retrieved_docs", qa_a.get("context", []))
        evidence_b = qa_b.get("retrieved_docs", qa_b.get("context", []))
        
        # æ ¼å¼åŒ–è¯æ®
        evidence_text_a = self._format_evidence(evidence_a)
        evidence_text_b = self._format_evidence(evidence_b)
        
        # # è°ƒè¯•è¾“å‡º
        # self.logger.info(f"ğŸ” answer_a: {answer_a[:100]}...")
        # self.logger.info(f"ğŸ” answer_b: {answer_b[:100]}...")
        # self.logger.info(f"ğŸ” expected_answer: {expected_answer[:100]}...")
        # self.logger.info(f"ğŸ” groundtruth: {groundtruth[:100]}...")
        
        # æ„å»ºå®Œæ•´çš„è¯„ä¼°prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„RAGç³»ç»Ÿå›ç­”è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹æ¯”åˆ†æä¸¤ä¸ªRAGç³»ç»Ÿå¯¹åŒä¸€é—®é¢˜çš„å›ç­”è´¨é‡ã€‚

é—®é¢˜ï¼š{question}

æ ‡å‡†ç­”æ¡ˆï¼š{expected_answer}
æ ‡å‡†ç­”æ¡ˆå¯¹åº”çš„çŸ¥è¯†åº“é‡Œçš„è¯æ®ï¼š{groundtruth}

ç³»ç»ŸAçš„å›ç­”ï¼š
{answer_a}

ç³»ç»ŸAçš„æ£€ç´¢è¯æ®ï¼š
{evidence_text_a}

ç³»ç»ŸBçš„å›ç­”ï¼š
{answer_b}

ç³»ç»ŸBçš„æ£€ç´¢è¯æ®ï¼š
{evidence_text_b}

ç‰¹åˆ«æ³¨æ„ï¼šè¯·åŠ¡å¿…åœ¨1000tokenä»¥å†…ç»™å‡ºç­”æ¡ˆï¼ï¼ï¼

è¯„ä¼°æ ‡å‡†ï¼š
1.å…ˆæ¯”ç³»ç»ŸABçš„ç­”æ¡ˆç›¸è¾ƒäº<æ ‡å‡†ç­”æ¡ˆ>çš„å‡†ç¡®æ€§ï¼Œåœ¨è¦†ç›–äº†æ ‡ç­”åŸºç¡€ä¸Šå¢åŠ çš„é¢å¤–ä¿¡æ¯ä¸èƒ½ç®—ä½œåŠ åˆ†é¡¹ï¼Œä¸€åˆ‡ä»¥æ ‡å‡†ç­”æ¡ˆä¸ºå‡†ï¼Œéƒ½è¦†ç›–äº†æ ‡å‡†ç­”æ¡ˆçš„å…³é”®æ„æ€çš„ç­”æ¡ˆå¿…é¡»åˆ¤å¹³å±€ï¼Œç­”æ¡ˆè´¨é‡ç±»ä¼¼ç›´æ¥åˆ¤å¹³å±€å³å¯ï¼Œä¸ç”¨ç®¡åé¢ä¸¤æ¡è§„åˆ™
2.å¦‚æœç­”æ¡ˆå‡†ç¡®æ€§ç›¸å·®æ— å‡ ï¼Œä½ åªéœ€æ¯”è¾ƒ<æ ‡å‡†ç­”æ¡ˆå¯¹åº”çš„çŸ¥è¯†åº“é‡Œçš„è¯æ®>æ˜¯å¦å®Œæ•´/éƒ¨åˆ†åŒ…å«åœ¨ç³»ç»Ÿæ£€ç´¢å‡ºçš„è¯æ®ä¸­ï¼Œä¸ç”¨çº ç»“äºè¯æ®è´¨é‡
3.å¦‚æœä¸Šé¢ä¸¤ç‚¹éƒ½ç›¸å·®æ— å‡ ï¼Œé‚£ä¹ˆå°±åˆ¤å¹³å±€

æ³¨æ„ï¼š
1.ç”±äºtokenæœ‰é™ï¼Œè¯·ä½ åœ¨800tokenä»¥å†…å®Œæˆæ·±åº¦æ€è€ƒçš„å…¨è¿‡ç¨‹ï¼Œå¹¶ç»™å‡ºç­”æ¡ˆï¼Œ<ä¸€å®š>ä¸è¦è¶…å‡º1000tokené™åˆ¶ï¼Œæ‰€ä»¥ä¸ºäº†èŠ‚çœtokenï¼Œè¦æ±‚ä½ ä¸èƒ½é‡å¤æ€è€ƒç›¸åŒçš„å†…å®¹
2.åœ¨è¯„ä¼°çš„æœ€åï¼Œæ˜ç¡®è¯´æ˜æ˜¯Aè·èƒœ/Bè·èƒœ/å¹³å±€

ç‰¹æ®Šåˆ¤å†³è§„åˆ™ï¼š
- å¦‚æœä¸€æ–¹ç»™å‡ºç­”æ¡ˆï¼Œå¦ä¸€æ–¹å›ç­”"ä¿¡æ¯ä¸è¶³"ï¼Œè¦åˆ¤æ–­ç»™å‡ºç­”æ¡ˆçš„ä¸€æ–¹æ˜¯å¦æ­£ç¡®ï¼Œæ˜¯å¦èƒ¡ç¼–ï¼ˆæŒ‡çš„æ˜¯å®Œå…¨é”™è¯¯ï¼Œä¸æ ‡å‡†ç­”æ¡ˆå®Œå…¨ä¸ä¸€è‡´ï¼Œè€Œééƒ¨åˆ†é”™è¯¯ï¼‰ï¼Œè‹¥èƒ¡ç¼–åˆ™åˆ¤å¦ä¸€æ–¹ï¼ˆè¯šå®çš„ä¸€æ–¹ï¼‰èµ¢ã€‚è‹¥ä¸€æ–¹éƒ¨åˆ†æ­£ç¡®ï¼Œå¦ä¸€æ–¹ä¿¡æ¯ä¸è¶³ï¼Œåˆ™åˆ¤éƒ¨åˆ†æ­£ç¡®çš„ä¸€æ–¹è·èƒœ
- å®Œå…¨ç­”å¯¹ > éƒ¨åˆ†ç­”å¯¹ > ä¿¡æ¯ä¸è¶³ > å®Œå…¨é”™è¯¯

è¯·è¿›è¡Œæ·±åº¦åˆ†æï¼š"""

        self.logger.info(f"ğŸ“ æ„é€ promptå®Œæˆï¼Œé•¿åº¦: {len(prompt)}")
        return prompt
    
    def _build_direct_judgment_prompt(self, question: str, qa_a: Dict[str, Any], qa_b: Dict[str, Any]) -> str:
        """æ„å»ºç›´æ¥åˆ¤å†³prompt - æ— æ·±åº¦æ€è€ƒæ¨¡å¼"""
        
        # å®‰å…¨è·å–æ•°æ®ï¼Œå¤„ç†ç©ºå­—æ®µ
        answer_a = qa_a.get("rag_answer", qa_a.get("answer", "æ— å›ç­”"))
        answer_b = qa_b.get("rag_answer", qa_b.get("answer", "æ— å›ç­”"))
        expected_answer = qa_a.get("expected_answer", "")
        groundtruth = qa_a.get("groundtruth", qa_b.get("groundtruth", ""))
        
        # è·å–æ£€ç´¢è¯æ®
        evidence_a = qa_a.get("retrieved_docs", qa_a.get("context", []))
        evidence_b = qa_b.get("retrieved_docs", qa_b.get("context", []))
        
        # æ ¼å¼åŒ–è¯æ®
        evidence_text_a = self._format_evidence(evidence_a)
        evidence_text_b = self._format_evidence(evidence_b)
        
        # æ„å»ºç›´æ¥åˆ¤å†³prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„RAGç³»ç»Ÿå›ç­”è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹æ¯”ä¸¤ä¸ªRAGç³»ç»Ÿçš„å›ç­”è´¨é‡ï¼Œç»™å‡ºæœ€ç»ˆåˆ¤å†³ã€‚

é—®é¢˜ï¼š{question}

æ ‡å‡†ç­”æ¡ˆï¼š{expected_answer}

ç³»ç»ŸAçš„å›ç­”ï¼š{answer_a}
ç³»ç»ŸAçš„æ£€ç´¢è¯æ®ï¼š{evidence_text_a}

ç³»ç»ŸBçš„å›ç­”ï¼š{answer_b}
ç³»ç»ŸBçš„æ£€ç´¢è¯æ®ï¼š{evidence_text_b}

è¯„ä¼°æ ‡å‡†ï¼š
1. å‡†ç¡®æ€§ï¼šå›ç­”æ˜¯å¦æ­£ç¡®ï¼Œæ˜¯å¦åŒ…å«æ ‡å‡†ç­”æ¡ˆçš„å…³é”®ä¿¡æ¯
2. å®Œæ•´æ€§ï¼šå›ç­”æ˜¯å¦å…¨é¢ï¼Œæ˜¯å¦é—æ¼é‡è¦ä¿¡æ¯
3. ç›¸å…³æ€§ï¼šå›ç­”æ˜¯å¦é’ˆå¯¹é—®é¢˜ï¼Œæ˜¯å¦åŒ…å«æ— å…³ä¿¡æ¯

ç‰¹æ®Šè§„åˆ™ï¼š
- å¦‚æœä¸€æ–¹ç»™å‡ºç­”æ¡ˆï¼Œå¦ä¸€æ–¹å›ç­”"ä¿¡æ¯ä¸è¶³"ï¼Œè¦åˆ¤æ–­ç»™å‡ºç­”æ¡ˆçš„ä¸€æ–¹æ˜¯å¦æ­£ç¡®
- å®Œå…¨ç­”å¯¹ > éƒ¨åˆ†ç­”å¯¹ > éƒ¨åˆ†ç­”é”™ > ä¿¡æ¯ä¸è¶³ > å®Œå…¨é”™è¯¯

è¯·è¯¦ç»†åˆ†ææ¯”è¾ƒä¸¤ä¸ªç³»ç»Ÿçš„å›ç­”è´¨é‡ï¼Œç„¶ååœ¨åˆ†æçš„æœ€åä¸€è¡Œæ˜ç¡®ç»™å‡ºä½ çš„æœ€ç»ˆåˆ¤å†³ã€‚

æœ€ç»ˆåˆ¤å†³æ ¼å¼è¦æ±‚ï¼š
- å¦‚æœç³»ç»ŸAæ›´å¥½ï¼Œæœ€åä¸€è¡Œè¾“å‡ºï¼šA
- å¦‚æœç³»ç»ŸBæ›´å¥½ï¼Œæœ€åä¸€è¡Œè¾“å‡ºï¼šB  
- å¦‚æœä¸¤è€…ç›¸å½“ï¼Œæœ€åä¸€è¡Œè¾“å‡ºï¼šT

å¼€å§‹åˆ†æï¼š"""

        self.logger.info(f"ğŸ“ æ„é€ ç›´æ¥åˆ¤å†³promptå®Œæˆï¼Œé•¿åº¦: {len(prompt)}")
        return prompt
    
    def _format_evidence(self, evidence_list: List) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢è¯æ®ï¼Œå¤„ç†ç©ºå€¼"""
        if not evidence_list:
            return "æ— æ£€ç´¢è¯æ®"
        
        formatted = []
        for i, doc in enumerate(evidence_list[:3]):  # åªæ˜¾ç¤ºå‰3æ¡
            if isinstance(doc, dict):
                content = doc.get("content", doc.get("text", str(doc)))
            else:
                content = str(doc)
            
            # å¤„ç†ç©ºå†…å®¹
            if not content or content.strip() == "":
                content = "ç©ºå†…å®¹"
            
            formatted.append(f"[è¯æ®{i+1}] {content[:200]}...")
        
        return "\n".join(formatted) if formatted else "æ— æœ‰æ•ˆæ£€ç´¢è¯æ®"

    def _extract_choice_from_text(self, text: str) -> Optional[str]:
        """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–A/B/Té€‰æ‹©"""
        if not text:
            return None
        
        # å»é™¤å¤šä½™ç©ºæ ¼ï¼Œè½¬ä¸ºå¤§å†™
        text = text.strip().upper()
        
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾æ–‡æœ¬æœ€åä¸€è¡Œå•ç‹¬çš„A/B/T
        lines = text.split('\n')
        for line in reversed(lines):
            line = line.strip()
            if line in ['A', 'B', 'T']:
                return line
        
        # æ–¹æ³•2ï¼šæŸ¥æ‰¾æœ€åå‡ è¡Œä¸­åŒ…å«é€‰æ‹©æ¨¡å¼çš„å†…å®¹
        last_lines = lines[-3:]  # æ£€æŸ¥æœ€å3è¡Œ
        for line in reversed(last_lines):
            line = line.strip()
            # æ¨¡å¼å¦‚ï¼š"æœ€ç»ˆé€‰æ‹©ï¼šA" æˆ– "æˆ‘çš„åˆ¤å†³ï¼šB" ç­‰
            if 'ï¼šA' in line or ':A' in line:
                return 'A'
            elif 'ï¼šB' in line or ':B' in line:
                return 'B'
            elif 'ï¼šT' in line or ':T' in line:
                return 'T'
        
        # æ–¹æ³•3ï¼šæŸ¥æ‰¾æ•´ä¸ªæ–‡æœ¬ä¸­æœ€åä¸€ä¸ªå•ç‹¬çš„A/B/T
        import re
        # æŸ¥æ‰¾ç‹¬ç«‹çš„A/B/Tå­—æ¯ï¼ˆå‰åæœ‰ç©ºæ ¼ã€æ¢è¡Œæˆ–æ ‡ç‚¹ï¼‰
        matches = re.findall(r'(?:^|\s|ï¼š|:)([ABT])(?:\s|$|ã€‚|ï¼Œ|ï¼)', text)
        if matches:
            return matches[-1]  # è¿”å›æœ€åä¸€ä¸ªåŒ¹é…
        
        # æ–¹æ³•4ï¼šç®€å•åœ°æŸ¥æ‰¾æœ€åå‡ºç°çš„A/B/T
        for choice in ['A', 'B', 'T']:
            if choice in text:
                last_pos = text.rfind(choice)
                # ç¡®ä¿ä¸æ˜¯åœ¨å…¶ä»–å•è¯ä¸­é—´
                if (last_pos == 0 or not text[last_pos-1].isalnum()) and \
                   (last_pos == len(text)-1 or not text[last_pos+1].isalnum()):
                    return choice
        
        return None

    def _parse_logits_result(self, logits_result: Dict[str, Any], granularity: str) -> Dict[str, Any]:
        """è§£ælogitsç»“æœä¸ºæ ‡å‡†åˆ¤å†³æ ¼å¼"""
        
        try:
            choice = logits_result.get("choice", "Unknown")
            prob_a = logits_result.get("prob_a", 0.333)
            prob_b = logits_result.get("prob_b", 0.333)
            prob_t = logits_result.get("prob_t", 0.333)
            
            # ç¡®å®šè·èƒœè€…å’Œæ ‡ç­¾
            if choice == "A":
                winner = "A wins"
                confidence = prob_a
            elif choice == "B":
                winner = "B wins"
                confidence = prob_b
            elif choice == "T":
                winner = "Tie"
                confidence = prob_t
            else:
                winner = "Unknown"
                confidence = max(prob_a, prob_b, prob_t)
            
            # è®¡ç®—margin
            max_prob = max(prob_a, prob_b, prob_t)
            second_prob = sorted([prob_a, prob_b, prob_t])[-2]
            margin = max_prob - second_prob
            
            self.logger.info(f"âœ… è§£æåˆ¤å†³å®Œæˆ: {choice == 'A' or choice == 'B' or choice == 'T'}")
            
            # è¿”å›å®Œæ•´ç»“æœ
            result = {
                "label": winner,
                "reason": logits_result.get("reasoning", "åŸºäºDeepSeek-R1æ·±åº¦åˆ†æ"),
                "granularity": granularity,
                "confidence": confidence,
                "margin_score": margin,
                
                # ç›´æ¥å­—æ®µï¼ˆdice_simplified.pyéœ€è¦ï¼‰
                "prob_a": prob_a,
                "prob_b": prob_b,
                "prob_t": prob_t,
                "logit_a": logits_result.get("logit_a", 0.0),
                "logit_b": logits_result.get("logit_b", 0.0),
                "logit_t": logits_result.get("logit_t", 0.0),
                
                # å…¶ä»–å­—æ®µ
                "raw_response": logits_result.get("raw_response", ""),
                "generated_token": logits_result.get("generated_token", choice),
                "verification_consistent": logits_result.get("verification_consistent", True),
                "reasoning_choice": logits_result.get("reasoning_choice", choice)
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ è§£ælogitsç»“æœå¤±è´¥: {e}")
            return None
    
    def _create_default_judgment(self, reason: str) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤åˆ¤å†³ç»“æœ"""
        return {
            "label": "Soft tie",
            "reason": reason,
            "granularity": "passage",
            "confidence": 0.333,
            "margin_score": 0.0,
            "prob_a": 0.333,
            "prob_b": 0.333,
            "prob_t": 0.333,
            "logit_a": 0.0,
            "logit_b": 0.0,
            "logit_t": 0.0,
            "raw_response": "",
            "generated_token": "T",
            "verification_consistent": False,
            "reasoning_choice": "Unknown"
        }