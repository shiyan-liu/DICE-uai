#!/usr/bin/env python3
"""
DICE Pairwiseåˆ¤å†³æ¨¡å—
å®žçŽ°æ£€ç´¢-è¯æ®åŒé€šé“pairwiseåˆ¤å†³å’ŒMargin-Aware Tieåˆ†è§£
"""

import json
import logging
import math
import os
from typing import Dict, Any, Tuple, List
import numpy as np
from openai import OpenAI

class PairwiseJudge:
    """Pairwiseåˆ¤å†³å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger("DICE.Pairwise")
        
        # åˆå§‹åŒ–OpenAIå…¼å®¹çš„APIå®¢æˆ·ç«¯
        api_key = config.api_key or os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½®DEEPSEEK_API_KEYçŽ¯å¢ƒå˜é‡æˆ–åœ¨configä¸­æä¾›api_key")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url=config.base_url
        )
    
    def judge_pair(
        self, 
        question: str, 
        qa_a: Dict[str, Any], 
        qa_b: Dict[str, Any], 
        granularity: str,
        atoms: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œpairwiseåˆ¤å†³
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            qa_a: ç³»ç»ŸAçš„QAæ•°æ®
            qa_b: ç³»ç»ŸBçš„QAæ•°æ®
            granularity: å½“å‰ç²’åº¦
            atoms: è¯¥ç²’åº¦çš„åŽŸå­å•å…ƒ
            
        Returns:
            åˆ¤å†³ç»“æžœ
        """
        self.logger.info(f"æ‰§è¡Œ{granularity}ç²’åº¦åˆ¤å†³")
        
        # æž„é€ prompt
        prompt = self._build_prompt(question, qa_a, qa_b, granularity, atoms)
        
        # è°ƒç”¨LLMè¿›è¡Œåˆ¤å†³
        raw_judgment = self._call_llm(prompt)
        
        # è§£æžåˆ¤å†³ç»“æžœ
        parsed_judgment = self._parse_judgment(raw_judgment, granularity)
        
        # Margin-Aware Tieå¤„ç†
        if parsed_judgment["label"] == "Tie":
            margin_score = self._compute_margin_aware_tie(
                question, qa_a, qa_b, granularity, atoms
            )
            parsed_judgment["margin_score"] = margin_score
            parsed_judgment["score"] = 0.5 + margin_score  # è°ƒæ•´åˆ°[0,1]åŒºé—´
        else:
            parsed_judgment["margin_score"] = 0.0
            parsed_judgment["score"] = 1.0 if parsed_judgment["label"] == "A wins" else 0.0
        
        return parsed_judgment
    
    def _build_prompt(
        self, 
        question: str, 
        qa_a: Dict[str, Any], 
        qa_b: Dict[str, Any], 
        granularity: str,
        atoms: Dict[str, Any]
    ) -> str:
        """æž„å»ºpairwiseåˆ¤å†³prompt"""
        
        # èŽ·å–æ ‡å‡†ç­”æ¡ˆ
        groundtruth = qa_a.get("groundtruth", qa_a.get("expected_answer", ""))
        
        base_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„RAGç³»ç»Ÿè¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä¸¤ä¸ªç³»ç»Ÿåœ¨{granularity}ç²’åº¦ä¸Šçš„è¡¨çŽ°è¿›è¡Œæ¯”è¾ƒã€‚

é—®é¢˜: {question}

æ ‡å‡†ç­”æ¡ˆ: {groundtruth}

ç³»ç»ŸA:
è¯æ®: {self._format_evidence(qa_a.get('context', []))}
å›žç­”: {qa_a.get('rag_answer', '')}

ç³»ç»ŸB:
è¯æ®: {self._format_evidence(qa_b.get('context', []))}
å›žç­”: {qa_b.get('rag_answer', '')}

"""
        
        # æ ¹æ®ç²’åº¦æ·»åŠ ç‰¹å®šæŒ‡å¯¼
        granularity_guide = self._get_granularity_guide(granularity, atoms)
        
        full_prompt = base_prompt + granularity_guide + """

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›žç­”:
åˆ¤å†³: [A wins/B wins/Tie]
ç†ç”±: [ä¸€å¥è¯è¯´æ˜Žåˆ¤å†³åŽŸå› ï¼Œä¸è¶…è¿‡30å­—]

æ³¨æ„:
1. å¿…é¡»è€ƒè™‘æ£€ç´¢è¯æ®çš„è´¨é‡å’Œç­”æ¡ˆçš„å‡†ç¡®æ€§
2. å¦‚æžœä¸¤è€…å·®å¼‚å¾ˆå°æˆ–å„æœ‰ä¼˜åŠ£ï¼Œé€‰æ‹©Tie
3. ä¼˜å…ˆè€ƒè™‘äº‹å®žå‡†ç¡®æ€§ï¼Œå…¶æ¬¡è€ƒè™‘å®Œæ•´æ€§
"""
        
        return full_prompt
    
    def _format_evidence(self, contexts: List[str]) -> str:
        """æ ¼å¼åŒ–è¯æ®æ–‡æœ¬"""
        if not contexts:
            return "[æ— æ£€ç´¢è¯æ®]"
        
        formatted = []
        for i, ctx in enumerate(contexts[:3]):  # æœ€å¤šæ˜¾ç¤º3ä¸ªè¯æ®
            # æˆªæ–­è¿‡é•¿çš„è¯æ®
            truncated = ctx[:200] + "..." if len(ctx) > 200 else ctx
            formatted.append(f"[è¯æ®{i+1}] {truncated}")
        
        return "\n".join(formatted)
    
    def _get_granularity_guide(self, granularity: str, atoms: Dict[str, Any]) -> str:
        """èŽ·å–ç²’åº¦ç‰¹å®šçš„è¯„ä¼°æŒ‡å¯¼"""
        
        if granularity == "token":
            return f"""
Tokenç²’åº¦è¯„ä¼°æŒ‡å¯¼:
- é‡ç‚¹å…³æ³¨å…³é”®è¯æ±‡ã€æ•°å­—ã€ä¸“æœ‰åè¯çš„å‡†ç¡®æ€§
- æ£€æŸ¥é‡è¦äº‹å®žæ€§tokenæ˜¯å¦å‡†ç¡®æå–
- Tokençº§åˆ«å·®å¼‚: {self._summarize_token_atoms(atoms)}

è¯„ä¼°æ ‡å‡†: å“ªä¸ªç³»ç»Ÿåœ¨å…³é”®tokençš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ä¸Šæ›´ä¼˜ï¼Ÿ
"""
        
        elif granularity == "sentence":
            return f"""
Sentenceç²’åº¦è¯„ä¼°æŒ‡å¯¼:
- é‡ç‚¹å…³æ³¨å¥å­çš„è¯­ä¹‰å®Œæ•´æ€§å’Œé€»è¾‘æ€§
- æ£€æŸ¥å¥å­æ˜¯å¦æœ‰è¯æ®æ”¯æ’‘
- å¥å­çº§åˆ«å·®å¼‚: {self._summarize_sentence_atoms(atoms)}

è¯„ä¼°æ ‡å‡†: å“ªä¸ªç³»ç»Ÿçš„å¥å­è¡¨è¾¾æ›´å‡†ç¡®ã€æ›´æœ‰è¯æ®æ”¯æ’‘ï¼Ÿ
"""
        
        elif granularity == "passage":
            return f"""
Passageç²’åº¦è¯„ä¼°æŒ‡å¯¼:
- é‡ç‚¹å…³æ³¨æ£€ç´¢è¯æ®çš„è¦†ç›–åº¦å’Œç›¸å…³æ€§
- æ£€æŸ¥è¯æ®ä¸Žç­”æ¡ˆçš„ä¸€è‡´æ€§
- æ®µè½çº§åˆ«å·®å¼‚: {self._summarize_passage_atoms(atoms)}

è¯„ä¼°æ ‡å‡†: å“ªä¸ªç³»ç»Ÿçš„æ£€ç´¢è¯æ®æ›´å…¨é¢ã€æ›´ç›¸å…³ï¼Ÿ
"""
        
        elif granularity == "kg":
            return f"""
KGç²’åº¦è¯„ä¼°æŒ‡å¯¼:
- é‡ç‚¹å…³æ³¨çŸ¥è¯†ä¸‰å…ƒç»„çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
- æ£€æŸ¥å®žä½“å…³ç³»æ˜¯å¦æ­£ç¡®
- çŸ¥è¯†å›¾è°±å·®å¼‚: {self._summarize_kg_atoms(atoms)}

è¯„ä¼°æ ‡å‡†: å“ªä¸ªç³»ç»Ÿçš„çŸ¥è¯†ç»“æž„æ›´å‡†ç¡®ã€æ›´å®Œæ•´ï¼Ÿ
"""
        
        return ""
    
    def _summarize_token_atoms(self, atoms: Dict[str, Any]) -> str:
        """æ±‡æ€»tokenåŽŸå­ä¿¡æ¯"""
        comparisons = atoms.get("comparison_units", [])
        unique_a = sum(1 for c in comparisons if c["in_a"] and not c["in_b"])
        unique_b = sum(1 for c in comparisons if c["in_b"] and not c["in_a"])
        common = sum(1 for c in comparisons if c["in_a"] and c["in_b"])
        
        return f"Aç‹¬æœ‰{unique_a}ä¸ªtoken, Bç‹¬æœ‰{unique_b}ä¸ªtoken, å…±åŒ{common}ä¸ªtoken"
    
    def _summarize_sentence_atoms(self, atoms: Dict[str, Any]) -> str:
        """æ±‡æ€»sentenceåŽŸå­ä¿¡æ¯"""
        sentences_a = atoms.get("sentences_a", [])
        sentences_b = atoms.get("sentences_b", [])
        
        return f"Aæœ‰{len(sentences_a)}ä¸ªå¥å­, Bæœ‰{len(sentences_b)}ä¸ªå¥å­"
    
    def _summarize_passage_atoms(self, atoms: Dict[str, Any]) -> str:
        """æ±‡æ€»passageåŽŸå­ä¿¡æ¯"""
        comparison = atoms.get("comparison_units", {})
        
        return f"Aæœ‰{comparison.get('passage_count_a', 0)}ä¸ªæ®µè½, Bæœ‰{comparison.get('passage_count_b', 0)}ä¸ªæ®µè½, å®žä½“é‡å åº¦{comparison.get('entity_overlap', 0):.2f}"
    
    def _summarize_kg_atoms(self, atoms: Dict[str, Any]) -> str:
        """æ±‡æ€»KGåŽŸå­ä¿¡æ¯"""
        comparison = atoms.get("comparison_units", {})
        
        return f"Aæœ‰{comparison.get('triple_count_a', 0)}ä¸ªä¸‰å…ƒç»„, Bæœ‰{comparison.get('triple_count_b', 0)}ä¸ªä¸‰å…ƒç»„, é‡å åº¦{comparison.get('overlap_ratio', 0):.2f}"
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMè¿›è¡Œåˆ¤å†³"""
        try:
            self.logger.info("ðŸ”„ æ­£åœ¨è°ƒç”¨LLM...")
            self.logger.debug(f"ðŸ“ å‘é€çš„prompt: {prompt[:200]}...")
            
            response = self.client.chat.completions.create(
                model=self.config.llm_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„RAGç³»ç»Ÿè¯„ä¼°ä¸“å®¶ï¼Œè¯·å®¢è§‚å…¬æ­£åœ°è¿›è¡Œè¯„ä¼°ã€‚"
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=self.config.judge_temperature,
                max_tokens=self.config.max_tokens,
                top_p=0.9
            )
            
            content = response.choices[0].message.content
            self.logger.info(f"âœ… LLMå“åº”æˆåŠŸ")
            self.logger.info(f"ðŸ“„ åŽŸå§‹å“åº”å†…å®¹: {repr(content)}")
            self.logger.info(f"ðŸ“„ æ ¼å¼åŒ–å“åº”å†…å®¹:\n{content}")
            
            return content
            
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›žé»˜è®¤åˆ¤å†³
            return "åˆ¤å†³: Tie\nç†ç”±: LLMè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ¤å†³"
    
    def _parse_llm_response(self, response: str) -> Tuple[str, str]:
        """ä»ŽLLMçš„åŽŸå§‹å“åº”ä¸­è§£æžåˆ¤å†³å’Œç†ç”±"""
        self.logger.info("ðŸ” å¼€å§‹è§£æžLLMå“åº”...")
        self.logger.debug(f"å¾…è§£æžçš„å“åº”: {repr(response)}")
        
        lines = response.strip().split('\n')
        judgment = "Tie"  # é»˜è®¤å€¼
        reasoning = "æœªèƒ½ä»ŽLLMå“åº”ä¸­è§£æžå‡ºç†ç”±ã€‚"
        
        self.logger.debug(f"åˆ†å‰²åŽçš„è¡Œæ•°: {len(lines)}")
        for i, line in enumerate(lines):
            self.logger.debug(f"ç¬¬{i+1}è¡Œ: {repr(line)}")
            
            # ðŸ”§ æ”¹è¿›ï¼šå¤„ç†å¤šç§åˆ¤å†³æ ¼å¼ï¼ŒåŒ…æ‹¬å¸¦###å‰ç¼€çš„
            if ("åˆ¤å†³:" in line.lower() or "judgement:" in line.lower() or 
                "åˆ¤å†³ï¼š" in line.lower() or "judgment:" in line.lower()):
                # ðŸ”§ å¤„ç†ä¸­è‹±æ–‡å†’å·
                if ":" in line:
                    decision_part = line.split(":", 1)[-1].strip()
                elif "ï¼š" in line:
                    decision_part = line.split("ï¼š", 1)[-1].strip()
                else:
                    decision_part = line.strip()
                self.logger.info(f"ðŸŽ¯ æ‰¾åˆ°åˆ¤å†³è¡Œ: {repr(line)}")
                self.logger.info(f"ðŸŽ¯ æå–çš„åˆ¤å†³éƒ¨åˆ†: {repr(decision_part)}")
                
                if "a wins" in decision_part.lower():
                    judgment = "A wins"
                elif "b wins" in decision_part.lower():
                    judgment = "B wins"
                else:
                    judgment = "Tie"
                self.logger.info(f"ðŸŽ¯ æœ€ç»ˆåˆ¤å†³: {judgment}")
                
            elif line.lower().startswith("ç†ç”±:") or line.lower().startswith("reason:"):
                reasoning = line.split(":", 1)[-1].strip()
                self.logger.info(f"ðŸ’­ æ‰¾åˆ°ç†ç”±è¡Œ: {repr(line)}")
                self.logger.info(f"ðŸ’­ æå–çš„ç†ç”±: {repr(reasoning)}")
        
        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°æ˜Žç¡®çš„ç†ç”±ï¼Œä½¿ç”¨æ•´ä¸ªå“åº”ä½œä¸ºç†ç”±
        if reasoning == "æœªèƒ½ä»ŽLLMå“åº”ä¸­è§£æžå‡ºç†ç”±ã€‚":
            reasoning = response
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æ˜Žç¡®ç†ç”±ï¼Œä½¿ç”¨æ•´ä¸ªå“åº”")
        
        self.logger.info(f"âœ… è§£æžå®Œæˆ - åˆ¤å†³: {judgment}, ç†ç”±: {reasoning[:50]}...")
        return judgment, reasoning

    def _call_llm_with_logits(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨LLMå¹¶è¿”å›žlogitsä¿¡æ¯"""
        # å¦‚æžœæ˜¯åœ¨çº¿APIæ¨¡å¼ï¼Œå®ƒå¯èƒ½ä¸æ”¯æŒlogprobsï¼Œè¿›è¡Œä¼˜é›…é™çº§
        if "https://api.deepseek.com" in self.config.base_url:
            self.logger.warning("åœ¨çº¿APIæ¨¡å¼ä¸æ”¯æŒlogprobsï¼Œé™çº§ä¸ºå¸¸è§„è°ƒç”¨ã€‚")
            raw_response = self._call_llm(prompt)
            judgment, reasoning = self._parse_llm_response(raw_response)
            
            # ðŸ”§ ä¿®å¤ï¼šæ ¹æ®å®žé™…åˆ¤å†³ç»“æžœè®¾ç½®logitsï¼Œè€Œä¸æ˜¯å›ºå®šä¸º0.0
            # è¿™æ ·å¯ä»¥é¿å…æ‰€æœ‰åˆ¤å†³éƒ½è¢«å¼ºåˆ¶ä¸ºTie
            if judgment == "A wins":
                logit_a, logit_b = 2.0, -2.0  # Aæ˜Žæ˜¾èƒœå‡º
            elif judgment == "B wins":
                logit_a, logit_b = -2.0, 2.0  # Bæ˜Žæ˜¾èƒœå‡º
            else:  # Tie
                logit_a, logit_b = 0.0, 0.0   # å¹³å±€
            
            result = {
                "content": raw_response,
                "choice": judgment,
                "logit_a": logit_a,
                "logit_b": logit_b,
                "raw_response": {"message": {"content": raw_response}} # æ¨¡æ‹Ÿç»“æž„
            }
            
            self.logger.info("ðŸ“Š å¸¸è§„è°ƒç”¨ç»“æžœ:")
            self.logger.info(f"   ðŸ“„ å†…å®¹: {repr(result['content'])}")
            self.logger.info(f"   ðŸŽ¯ é€‰æ‹©: {result['choice']}")
            self.logger.info(f"   ðŸ“ˆ logit_a: {result['logit_a']} (åŸºäºŽåˆ¤å†³è°ƒæ•´)")
            self.logger.info(f"   ðŸ“ˆ logit_b: {result['logit_b']} (åŸºäºŽåˆ¤å†³è°ƒæ•´)")
            
            return result

        try:
            # åˆ›å»ºé€‰æ‹©é¢˜æ ¼å¼æ¥èŽ·å–logits
            choice_prompt = f"""{prompt}

è¯·ä»Žä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ï¼š
A. A wins
B. B wins  
C. Tie

ä½ çš„é€‰æ‹©ï¼š"""

            response = self.client.chat.completions.create(
                model=self.config.llm_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„RAGç³»ç»Ÿè¯„ä¼°ä¸“å®¶ï¼Œè¯·å®¢è§‚å…¬æ­£åœ°è¿›è¡Œè¯„ä¼°ã€‚"
                    },
                    {
                        "role": "user", 
                        "content": choice_prompt
                    }
                ],
                temperature=self.config.judge_temperature,
                max_tokens=self.config.max_tokens,
                top_p=0.9,
                logprobs=True,  # è¯·æ±‚logprobs
                top_logprobs=3  # è¿”å›žå‰3ä¸ªtokençš„logprobs
            )
            
            # æå–logitsä¿¡æ¯
            content = response.choices[0].message.content
            
            # æ¨¡æ‹Ÿlogitsè®¡ç®—ï¼ˆå¦‚æžœAPIä¸ç›´æŽ¥æ”¯æŒlogprobsï¼‰
            # åŸºäºŽLLMçš„æ¸©åº¦å’Œè¾“å‡ºæ¦‚çŽ‡ä¼°ç®—logits
            choice_map = {"A": "A wins", "B": "B wins", "C": "Tie"}
            
            # è§£æžé€‰æ‹©
            choice = None
            for key, value in choice_map.items():
                if key in content.upper() or value in content:
                    choice = value
                    break
            
            if not choice:
                choice = "Tie"
            
            # ä¼°ç®—logitsï¼ˆåŸºäºŽæ¸©åº¦å’Œé€‰æ‹©ç¡®å®šæ€§ï¼‰
            # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ–¹æ³•ï¼Œå®žé™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨çœŸå®žçš„logprobs
            base_logit = 0.0
            if "æ˜Žæ˜¾" in content or "æ˜¾ç„¶" in content or "clearly" in content.lower():
                confidence_logit = 2.0  # é«˜ç¡®å®šæ€§
            elif "å¯èƒ½" in content or "æˆ–è®¸" in content or "maybe" in content.lower():
                confidence_logit = -1.0  # ä½Žç¡®å®šæ€§
            else:
                confidence_logit = 0.5  # ä¸­ç­‰ç¡®å®šæ€§
            
            # æ ¹æ®é€‰æ‹©åˆ†é…logits
            if choice == "A wins":
                logit_a, logit_b = base_logit + confidence_logit, base_logit - confidence_logit
            elif choice == "B wins":
                logit_a, logit_b = base_logit - confidence_logit, base_logit + confidence_logit
            else:  # Tie
                logit_a, logit_b = base_logit, base_logit
            
            return {
                "content": content,
                "choice": choice,
                "logit_a": logit_a,
                "logit_b": logit_b,
                "raw_response": response
            }
            
        except Exception as e:
            self.logger.error(f"LLM logitsè°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›žé»˜è®¤å€¼
            return {
                "content": "åˆ¤å†³: Tie\nç†ç”±: LLMè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ¤å†³",
                "choice": "Tie",
                "logit_a": 0.0,
                "logit_b": 0.0,
                "raw_response": None
            }
    
    def _parse_judgment(self, raw_judgment: str, granularity: str) -> Dict[str, Any]:
        """è§£æžLLMçš„åˆ¤å†³ç»“æžœ"""
        lines = raw_judgment.strip().split('\n')
        
        label = "Tie"  # é»˜è®¤å€¼
        reason = "è§£æžå¤±è´¥"
        
        for line in lines:
            line = line.strip()
            if line.startswith("åˆ¤å†³:") or line.startswith("åˆ¤å†³ï¼š"):
                # æå–åˆ¤å†³
                decision_part = line.split(":", 1)[-1].split("ï¼š", 1)[-1].strip()
                if "A wins" in decision_part or "Aèƒœ" in decision_part or "Aæ›´å¥½" in decision_part:
                    label = "A wins"
                elif "B wins" in decision_part or "Bèƒœ" in decision_part or "Bæ›´å¥½" in decision_part:
                    label = "B wins"
                else:
                    label = "Tie"
            
            elif line.startswith("ç†ç”±:") or line.startswith("ç†ç”±ï¼š"):
                reason = line.split(":", 1)[-1].split("ï¼š", 1)[-1].strip()
        
        return {
            "label": label,
            "reason": reason,
            "granularity": granularity,
            "raw_response": raw_judgment
        }
    
    def _compute_margin_aware_tie(
        self, 
        question: str, 
        qa_a: Dict[str, Any], 
        qa_b: Dict[str, Any], 
        granularity: str,
        atoms: Dict[str, Any]
    ) -> float:
        """
        è®¡ç®—Margin-Aware Tieçš„è½¯å¾—åˆ†
        
        Returns:
            è½¯å¾—åˆ† [-0.05, 0.05]ï¼Œæ­£å€¼è¡¨ç¤ºåå‘Aï¼Œè´Ÿå€¼è¡¨ç¤ºåå‘B
        """
        try:
            # æž„é€ æ¯”è¾ƒpromptä»¥èŽ·å–ç½®ä¿¡åº¦
            confidence_prompt = f"""è¯·å¯¹ä»¥ä¸‹ä¸¤ä¸ªç³»ç»Ÿåœ¨{granularity}ç²’åº¦ä¸Šçš„è¡¨çŽ°è¿›è¡Œç²¾ç»†æ¯”è¾ƒï¼Œç»™å‡ºä½ çš„ç½®ä¿¡åº¦è¯„ä¼°ã€‚

é—®é¢˜: {question}

ç³»ç»ŸA: {qa_a.get('rag_answer', '')}
ç³»ç»ŸB: {qa_b.get('rag_answer', '')}

è¯·å›žç­”: å¦‚æžœå¿…é¡»é€‰æ‹©ä¸€ä¸ªæ›´å¥½çš„ç³»ç»Ÿï¼Œä½ ä¼šé€‰æ‹©å“ªä¸ªï¼Ÿè¯·ç”¨1-10çš„æ•°å­—è¡¨ç¤ºä½ çš„ç½®ä¿¡åº¦(1=éžå¸¸ä¸ç¡®å®šï¼Œ10=éžå¸¸ç¡®å®š)ã€‚

æ ¼å¼: é€‰æ‹©: [A/B], ç½®ä¿¡åº¦: [1-10]"""
            
            response = self._call_llm(confidence_prompt)
            
            # è§£æžç½®ä¿¡åº¦
            choice, confidence = self._parse_confidence(response)
            
            # è½¬æ¢ä¸ºmargin score
            if choice == "A":
                margin_score = (confidence - 5) / 100  # æ˜ å°„åˆ°[-0.05, 0.05]
            elif choice == "B":
                margin_score = -(confidence - 5) / 100
            else:
                margin_score = 0.0
            
            # é™åˆ¶èŒƒå›´
            margin_score = max(-0.05, min(0.05, margin_score))
            
            return margin_score
            
        except Exception as e:
            self.logger.warning(f"Margin-Aware Tieè®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _parse_confidence(self, response: str) -> Tuple[str, int]:
        """è§£æžç½®ä¿¡åº¦å“åº”"""
        lines = response.strip().split('\n')
        choice = "A"  # é»˜è®¤
        confidence = 5  # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦
        
        for line in lines:
            line = line.strip()
            if "é€‰æ‹©:" in line or "é€‰æ‹©ï¼š" in line:
                if "B" in line:
                    choice = "B"
                else:
                    choice = "A"
            
            if "ç½®ä¿¡åº¦:" in line or "ç½®ä¿¡åº¦ï¼š" in line:
                # æå–æ•°å­—
                import re
                numbers = re.findall(r'\d+', line)
                if numbers:
                    confidence = int(numbers[0])
                    confidence = max(1, min(10, confidence))  # é™åˆ¶èŒƒå›´
        
        return choice, confidence
    
    def get_judgment_statistics(self, judgments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èŽ·å–åˆ¤å†³ç»Ÿè®¡ä¿¡æ¯"""
        if not judgments:
            return {}
        
        a_wins = sum(1 for j in judgments if j["label"] == "A wins")
        b_wins = sum(1 for j in judgments if j["label"] == "B wins") 
        ties = sum(1 for j in judgments if j["label"] == "Tie")
        
        total = len(judgments)
        
        return {
            "total_judgments": total,
            "a_wins": a_wins,
            "b_wins": b_wins,
            "ties": ties,
            "a_win_rate": a_wins / total if total > 0 else 0,
            "b_win_rate": b_wins / total if total > 0 else 0,
            "tie_rate": ties / total if total > 0 else 0,
            "avg_margin_score": np.mean([j.get("margin_score", 0) for j in judgments])
        } 