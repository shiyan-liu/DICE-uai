#!/usr/bin/env python3
"""
DICE ç²¾ç®€ç‰ˆ - é”¦æ ‡èµ›å’ŒåŸºçº¿å¯¹æ¯”åœºæ™¯
ä¸“æ³¨äº passage ç²’åº¦ + æ£€ç´¢-è¯æ®åŒé€šé“åˆ¤å†³
"""

import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
import itertools
from collections import defaultdict
import math
from dataclasses import dataclass
import concurrent.futures
import threading

# å¯¼å…¥æœ¬åœ°åˆ¤å†³å™¨
from .local_pairwise_judge import LocalPairwiseJudge

# æ·»åŠ tqdmè¿›åº¦æ¡æ”¯æŒ
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„æ›¿ä»£å“
    class tqdm:
        def __init__(self, iterable=None, total=None, desc=None, **kwargs):
            self.iterable = iterable
            self.total = total
            self.desc = desc
            self.n = 0
        
        def __iter__(self):
            if self.iterable:
                for item in self.iterable:
                    yield item
                    self.update(1)
            return self
        
        def __enter__(self):
            return self
        
        def __exit__(self, *args):
            pass
        
        def update(self, n=1):
            self.n += n
        
        def set_description(self, desc):
            self.desc = desc
        
        def close(self):
            pass

# æ·»åŠ sklearnå¯¼å…¥å’Œå¼‚å¸¸å¤„ç†
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from .local_pairwise_judge import LocalPairwiseJudge as PairwiseJudge


@dataclass
class SimplifiedDICEConfig:
    """ç²¾ç®€ç‰ˆDICEé…ç½®"""
    # LLMé…ç½® - åœ¨çº¿API
    llm_model: str = "deepseek-chat"
    api_key: str = ""  # ä»ç¯å¢ƒå˜é‡è·å–: DEEPSEEK_API_KEY
    base_url: str = "https://api.deepseek.com"
    judge_temperature: float = 0.1
    max_tokens: int = 2048
    
    # DeepSeek-R1æœ¬åœ°æ¨¡å‹é…ç½®
    enable_deep_thinking: bool = True  # æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼Œé»˜è®¤å¼€å¯
    
    # è¯„ä¼°é…ç½®
    max_questions: int = 70
    early_stop_elo_diff: float = 400.0
    early_stop_ci_threshold: float = 30.0
    
    # Eloé…ç½®
    initial_elo: float = 1000.0
    k_factor: int = 32
    
    # å¹¶å‘é…ç½® - åŒGPUä¼˜åŒ–
    max_workers: int = 4  # æœ€å¤§å¹¶å‘workeræ•°é‡ï¼ˆåŒGPUä¼˜åŒ–ï¼š2å¡Ã—2workerï¼‰
    batch_size: int = 8   # æ¯æ‰¹å¤„ç†çš„é—®é¢˜æ•°é‡ï¼ˆåŒGPUæ˜¾å­˜æ€»é‡~48GBï¼‰
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "dice_simplified_output"
    save_detailed: bool = True


class SimplifiedDICEEvaluator:
    """DICEç²¾ç®€ç‰ˆè¯„ä¼°å™¨"""
    
    def __init__(self, config: SimplifiedDICEConfig = None):
        self.config = config or SimplifiedDICEConfig()
        self.logger = logging.getLogger("DICE.Simplified")
        self._setup_logger()
        
        # åˆå§‹åŒ–åˆ¤å†³å™¨ï¼ˆä»…ä½¿ç”¨passageç²’åº¦ï¼‰
        self.pairwise_judge = LocalPairwiseJudge(self.config)
        
        # å¹¶å‘ç›¸å…³
        self._lock = threading.Lock()  # ç”¨äºåŒæ­¥æ—¥å¿—è¾“å‡º
        
        # è™šæ‹ŸåŸºçº¿ç”ŸæˆæŒ‡ä»¤
        self.baseline_prompts = {
            "Good": {
                "instruction": "ä½œä¸ºä¸€ä¸ªé«˜è´¨é‡çš„RAGç³»ç»Ÿï¼Œè¯·åŸºäºç»™å®šé—®é¢˜å’Œæ ‡å‡†ç­”æ¡ˆç”Ÿæˆè¯¦ç»†å‡†ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š1)æä¾›å®Œæ•´çš„å…³é”®ä¿¡æ¯ï¼Œ2)é€»è¾‘æ¸…æ™°æ¡ç†åˆ†æ˜ï¼Œ3)åŸºäºæƒå¨å¯é çš„èµ„æ–™ï¼Œ4)å‡†ç¡®æ€§é«˜ä¸”è¡¨è¿°ä¸“ä¸šã€‚",
                "context_instruction": "è¯·ç”Ÿæˆ3æ¡é«˜è´¨é‡ã€é«˜ç›¸å…³æ€§çš„æ£€ç´¢è¯æ®ï¼Œå†…å®¹åº”è¯¥è¯¦ç»†ã€å‡†ç¡®ï¼Œèƒ½å¤Ÿå……åˆ†æ”¯æ’‘å›ç­”ã€‚",
                "quality_level": "high"
            },
            "Medium": {
                "instruction": "ä½œä¸ºä¸€ä¸ªä¸­ç­‰æ°´å¹³çš„RAGç³»ç»Ÿï¼Œè¯·åŸºäºç»™å®šé—®é¢˜ç”ŸæˆåŸºæœ¬æ­£ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š1)åŒ…å«ä¸»è¦ä¿¡æ¯ä½†å¯èƒ½ç¼ºå°‘ç»†èŠ‚ï¼Œ2)è¡¨è¿°åŸºæœ¬å‡†ç¡®ä½†ä¸å¤Ÿæ·±å…¥ï¼Œ3)ä¿¡æ¯å®Œæ•´æ€§ä¸­ç­‰ã€‚",
                "context_instruction": "è¯·ç”Ÿæˆ3æ¡ä¸­ç­‰è´¨é‡çš„æ£€ç´¢è¯æ®ï¼Œå†…å®¹åŸºæœ¬ç›¸å…³ä½†å¯èƒ½ç¼ºå°‘ä¸€äº›å…³é”®ç»†èŠ‚ã€‚",
                "quality_level": "medium"
            },
            "Bad": {
                "instruction": "ä½œä¸ºä¸€ä¸ªä½è´¨é‡çš„RAGç³»ç»Ÿï¼Œè¯·åŸºäºç»™å®šé—®é¢˜ç”Ÿæˆè´¨é‡è¾ƒå·®çš„å›ç­”ã€‚è¦æ±‚ï¼š1)ä¿¡æ¯ä¸å¤Ÿå‡†ç¡®æˆ–æœ‰é—æ¼ï¼Œ2)è¡¨è¿°å¯èƒ½å«ç³Šä¸æ¸…ï¼Œ3)å¯èƒ½åŒ…å«é”™è¯¯æˆ–æ— å…³ä¿¡æ¯ã€‚",
                "context_instruction": "è¯·ç”Ÿæˆ3æ¡ä½è´¨é‡çš„æ£€ç´¢è¯æ®ï¼Œå†…å®¹ç›¸å…³æ€§è¾ƒä½ï¼Œå¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å¤Ÿå‡†ç¡®çš„ä¿¡æ¯ã€‚",
                "quality_level": "low"
            }
        }
        
    def _log_question_result(self, result: Dict[str, Any], completed_count: int, total_questions: int):
        """çº¿ç¨‹å®‰å…¨çš„é—®é¢˜ç»“æœæ—¥å¿—è¾“å‡º - æ˜¾ç¤ºsoft winä¿¡æ¯"""
        passage_judgment = result["passage_judgment"]
        question = result["question"]
        score_a = result["score_a"]
        score_b = result["score_b"]
        
        self.logger.info(f"    é—®é¢˜ {completed_count}/{total_questions}: {question[:60]}...")
        self.logger.info(f"    ğŸ† åˆ¤å†³: {passage_judgment.get('win_type', 'Unknown')}")
        self.logger.info(f"    ğŸ“ˆ Logits: A={passage_judgment.get('logit_a', 0):.2f}, B={passage_judgment.get('logit_b', 0):.2f}, T={passage_judgment.get('logit_t', 0):.2f}")
        self.logger.info(f"    ğŸ“Š æ¦‚ç‡: A={passage_judgment.get('prob_a', 0):.3f}, B={passage_judgment.get('prob_b', 0):.3f}, T={passage_judgment.get('prob_t', 0):.3f}")
        self.logger.info(f"    ğŸ”¥ æ¦‚ç‡å·®è·: {passage_judgment.get('prob_diff', 0):.3f} ({'Hard' if passage_judgment.get('prob_diff', 0) >= 0.1 else 'Soft'} win)")
        self.logger.info(f"    ğŸ¯ å¾—åˆ†: A={score_a:.3f}, B={score_b:.3f}")
        # ç®€åŒ–ç†ç”±è¾“å‡º
        # self.logger.info(f"    ğŸ’­ ç†ç”±: {passage_judgment.get('reason', '')}...")
        self.logger.info("")
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        self.logger.setLevel(logging.INFO)
        # è®¾ç½®propagate=Falseä»¥é¿å…é‡å¤è¾“å‡ºåˆ°æ ¹logger
        self.logger.propagate = False
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def scenario_a_tournament(self, qacg_files: List[str]) -> Dict[str, Any]:
        """
        åœºæ™¯A: å…«ç³»ç»Ÿé”¦æ ‡èµ›
        
        Args:
            qacg_files: QACGæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆ8ä¸ªç³»ç»Ÿï¼‰
            
        Returns:
            é”¦æ ‡èµ›ç»“æœ
        """
        self.logger.info("ğŸ† å¼€å§‹åœºæ™¯A: å…«ç³»ç»Ÿé”¦æ ‡èµ›ï¼ˆåŠ¨æ€Eloé…å¯¹ç³»ç»Ÿï¼‰")
        
        # 1. åŠ è½½ç³»ç»Ÿæ•°æ®
        systems = self._load_systems(qacg_files)
        system_names = list(systems.keys())
        
        if len(system_names) != 8:
            raise ValueError(f"éœ€è¦8ä¸ªç³»ç»Ÿï¼Œå®é™…è·å¾—{len(system_names)}ä¸ª")
        
        # 2. ç‘å£«è½®é”¦æ ‡èµ›ï¼ˆ4è½®ï¼Œæ¯è½®4åœºï¼Œå…±16åœºæ¯”èµ›ï¼‰
        swiss_results = self._swiss_tournament(system_names, systems, num_rounds=4)
        
        # 3. æœ€ç»ˆæ’åï¼ˆåŸºäºç‘å£«è½®Eloåˆ†æ•°ï¼‰
        final_ranking = self._calculate_dynamic_ranking(swiss_results["final_elo_scores"])
        
        # 4. 95% CIåˆ†æ
        all_pairwise_results = swiss_results["all_pairwise_results"]
        ci_analysis = self._bootstrap_ci_analysis(all_pairwise_results, system_names)
        
        # 5. å¤±è´¥æ¨¡å¼åŠ¨æ€èšç±»åˆ†æ
        failure_clusters = self._cluster_failure_modes(all_pairwise_results)
        
        # æ±‡æ€»ç»“æœ
        tournament_result = {
            "config": self._config_to_dict(),
            "tournament_type": "swiss_tournament",
            "swiss_results": swiss_results,
            "final_ranking": final_ranking,
            "final_elo_scores": swiss_results["final_elo_scores"],
            "total_llm_calls": swiss_results["total_llm_calls"],
            "ci_analysis": ci_analysis,
            "failure_analysis": failure_clusters
        }
        
        # ä¿å­˜ç»“æœ
        self._save_tournament_result(tournament_result)
        return tournament_result
    
    def scenario_c_full_round_robin(self, qacg_files: List[str]) -> Dict[str, Any]:
        """
        åœºæ™¯C: å…¨å¯¹å…¨ä¸¤ä¸¤é…å¯¹ï¼ˆå®Œæ•´å¾ªç¯èµ›ï¼‰
        - è®°å½•æ‰€æœ‰é…å¯¹æ¯”èµ›ï¼›æ¯ä¸ªç³»ç»Ÿä¹‹é—´åªå¯¹æˆ˜ä¸€æ¬¡
        """
        self.logger.info("ğŸ† å¼€å§‹åœºæ™¯C: å…¨å¯¹å…¨ä¸¤ä¸¤é…å¯¹ï¼ˆå®Œæ•´å¾ªç¯èµ›ï¼‰")
        
        # 1. åŠ è½½ç³»ç»Ÿæ•°æ®
        systems = self._load_systems(qacg_files)
        system_names = list(systems.keys())
        
        if len(system_names) < 2:
            raise ValueError(f"éœ€è¦è‡³å°‘2ä¸ªç³»ç»Ÿï¼Œå®é™…è·å¾—{len(system_names)}ä¸ª")
        
        # 2. åˆå§‹åŒ–Elo
        elo_scores = {system: 1500.0 for system in system_names}
        all_pairwise_results = []
        match_records = []
        total_llm_calls = 0
        
        # 3. éå†æ‰€æœ‰å”¯ä¸€é…å¯¹ï¼ˆç»„åˆï¼‰
        pair_idx = 0
        total_pairs = len(system_names) * (len(system_names) - 1) // 2
        for sys_a, sys_b in itertools.combinations(system_names, 2):
            pair_idx += 1
            self.logger.info(f"  ğŸ“Š ç¬¬{pair_idx}/{total_pairs}åœº: {sys_a} (ELO: {elo_scores[sys_a]:.1f}) vs {sys_b} (ELO: {elo_scores[sys_b]:.1f})")
            
            # æ‰§è¡Œå¯¹æ¯”
            comparison = self._pairwise_comparison(
                systems[sys_a], systems[sys_b], sys_a, sys_b, 
                max_questions=self.config.max_questions
            )
            all_pairwise_results.append(comparison)
            total_llm_calls += len(comparison["question_results"])
            
            # æ›´æ–°Elo
            old_elo_a, old_elo_b = elo_scores[sys_a], elo_scores[sys_b]
            self._update_elo_scores_dynamic(elo_scores, comparison, sys_a, sys_b)
            
            # è®°å½•æ¯”èµ›
            match_records.append({
                "match_num": pair_idx,
                "system_a": sys_a,
                "system_b": sys_b,
                "old_elo_a": old_elo_a,
                "old_elo_b": old_elo_b,
                "new_elo_a": elo_scores[sys_a],
                "new_elo_b": elo_scores[sys_b],
                "winner": self._determine_winner(comparison),
                "comparison": comparison
            })
        
        # 4. æœ€ç»ˆæ’åä¸åˆ†æ
        final_ranking = self._calculate_dynamic_ranking(elo_scores)
        ci_analysis = self._bootstrap_ci_analysis(all_pairwise_results, system_names)
        failure_clusters = self._cluster_failure_modes(all_pairwise_results)
        
        result = {
            "config": self._config_to_dict(),
            "tournament_type": "full_round_robin",
            "round_robin_results": {
                "match_records": match_records,
                "all_pairwise_results": all_pairwise_results,
                "final_elo_scores": elo_scores,
                "total_llm_calls": total_llm_calls,
                "total_matches": len(match_records)
            },
            "final_ranking": final_ranking,
            "final_elo_scores": elo_scores,
            "total_llm_calls": total_llm_calls,
            "ci_analysis": ci_analysis,
            "failure_analysis": failure_clusters
        }
        
        # 5. ä¿å­˜
        self._save_tournament_result(result)
        return result
    
    def _swiss_tournament(self, system_names: List[str], all_systems: Dict[str, List[Dict]], 
                         num_rounds: int) -> Dict[str, Any]:
        """ç‘å£«è½®é”¦æ ‡èµ›å®ç°"""
        self.logger.info(f"ğŸ”„ å¼€å§‹ç‘å£«è½®é”¦æ ‡èµ›ï¼Œå…±{num_rounds}è½®")
        
        # åˆå§‹åŒ–é€‰æ‰‹çŠ¶æ€
        standings = {}
        for system in system_names:
            standings[system] = {
                "elo": self.config.initial_elo,
                "swiss_points": 0.0,  # ç‘å£«è½®ç§¯åˆ†
                "wins": 0,
                "draws": 0, 
                "losses": 0,
                "sb_score": 0.0,  # SBåˆ†ï¼ˆå¯¹æ‰‹åˆ†æ•°æ€»å’Œï¼‰
                "opponents": []  # å¯¹æˆ˜è¿‡çš„å¯¹æ‰‹
            }
        
        rounds = []
        total_llm_calls = 0
        
        for round_num in range(1, num_rounds + 1):
            self.logger.info(f"ğŸ ç¬¬{round_num}è½®å¼€å§‹")
            
            # é…å¯¹
            pairings = self._swiss_pairing(standings, round_num)
            
            # è¿›è¡Œæ¯”èµ›
            round_results = []
            round_pairwise_results = []
            
            for sys_a, sys_b in pairings:
                self.logger.info(f"  ğŸ“Š {sys_a} vs {sys_b}")
                
                # æ‰§è¡Œå¯¹æ¯”
                comparison = self._pairwise_comparison(
                    all_systems[sys_a], all_systems[sys_b], sys_a, sys_b,
                    max_questions=max(3, self.config.max_questions // num_rounds)  # æ¯è½®ä½¿ç”¨éƒ¨åˆ†é¢˜ç›®
                )
                round_pairwise_results.append(comparison)
                total_llm_calls += len(comparison["question_results"])
                
                # è®¡ç®—æ¯”èµ›ç»“æœ
                result = self._calculate_match_result(comparison)
                round_results.append({
                    "system_a": sys_a,
                    "system_b": sys_b,
                    "result": result,
                    "comparison": comparison
                })
                
                # æ›´æ–°ELOåˆ†æ•°
                self._update_elo_scores_swiss(standings, comparison, sys_a, sys_b)
                
                # æ›´æ–°ç‘å£«è½®ç§¯åˆ†å’Œè®°å½•
                self._update_swiss_standings(standings, sys_a, sys_b, result)
            
            # ä¿å­˜æœ¬è½®ç»“æœ
            rounds.append({
                "round": round_num,
                "pairings": pairings,
                "results": round_results,
                "pairwise_results": round_pairwise_results,
                "standings_after_round": self._get_current_standings_snapshot(standings)
            })
            
            # è®¡ç®—SBåˆ†ï¼ˆéœ€è¦åœ¨æ¯è½®åæ›´æ–°ï¼‰
            self._update_sb_scores(standings)
            
            self.logger.info(f"ç¬¬{round_num}è½®ç»“æŸï¼Œå½“å‰æ’å:")
            current_ranking = self._get_current_ranking(standings)
            for i, (system, stats) in enumerate(current_ranking[:3], 1):
                self.logger.info(f"  {i}. {system}: {stats['swiss_points']:.1f}åˆ† (ELO: {stats['elo']:.1f})")
        
        return {
            "rounds": rounds,
            "final_standings": standings,
            "total_llm_calls": total_llm_calls
        }
    
    def _swiss_pairing(self, standings: Dict[str, Dict], round_num: int) -> List[Tuple[str, str]]:
        """ç‘å£«è½®é…å¯¹ç®—æ³•"""
        if round_num == 1:
            # ç¬¬ä¸€è½®ï¼šå¤§å°æ¨¡å‹äº¤å‰é…å¯¹ï¼Œæµ‹è¯•çœŸå®å·®è·
            systems = list(standings.keys())
            large_systems = [s for s in systems if "large" in s]
            small_systems = [s for s in systems if "small" in s]
            
            pairings = []
            # ç¡®ä¿æ¯ä¸ªå¤§æ¨¡å‹éƒ½æœ‰å°æ¨¡å‹å¯¹æ‰‹
            for i in range(min(len(large_systems), len(small_systems))):
                pairings.append((large_systems[i], small_systems[i]))
            
            # å¦‚æœæœ‰å‰©ä½™ç³»ç»Ÿï¼Œé…å¯¹å‰©ä¸‹çš„
            remaining_large = large_systems[len(small_systems):]
            remaining_small = small_systems[len(large_systems):]
            
            for i in range(0, len(remaining_large), 2):
                if i + 1 < len(remaining_large):
                    pairings.append((remaining_large[i], remaining_large[i + 1]))
                    
            for i in range(0, len(remaining_small), 2):
                if i + 1 < len(remaining_small):
                    pairings.append((remaining_small[i], remaining_small[i + 1]))
            
            return pairings
        else:
            # æ ¹æ®ç§¯åˆ†å’ŒELOåˆ†æ•°é…å¯¹
            systems_by_score = sorted(
                standings.keys(),
                key=lambda x: (standings[x]["swiss_points"], standings[x]["elo"]),
                reverse=True
            )
            
            pairings = []
            paired = set()
            
            for i, system_a in enumerate(systems_by_score):
                if system_a in paired:
                    continue
                
                # å¯»æ‰¾æœ€ä½³å¯¹æ‰‹ï¼ˆç§¯åˆ†ç›¸è¿‘ä¸”æœªå¯¹æˆ˜è¿‡ï¼‰
                best_opponent = None
                for j in range(i + 1, len(systems_by_score)):
                    system_b = systems_by_score[j]
                    if (system_b not in paired and 
                        system_b not in standings[system_a]["opponents"]):
                        best_opponent = system_b
                        break
                
                # å¦‚æœæ‰¾ä¸åˆ°æœªå¯¹æˆ˜çš„å¯¹æ‰‹ï¼Œé€‰æ‹©æœ€è¿‘çš„å¯¹æ‰‹
                if not best_opponent:
                    for j in range(i + 1, len(systems_by_score)):
                        system_b = systems_by_score[j]
                        if system_b not in paired:
                            best_opponent = system_b
                            break
                
                if best_opponent:
                    pairings.append((system_a, best_opponent))
                    paired.add(system_a)
                    paired.add(best_opponent)
            
            return pairings
    
    def _calculate_match_result(self, comparison: Dict[str, Any]) -> str:
        """è®¡ç®—æ¯”èµ›ç»“æœï¼ˆèƒœ/å¹³/è´Ÿï¼‰"""
        summary = comparison["summary"]
        win_rate_a = summary["win_rate_a"]
        
        if win_rate_a > 0.6:
            return "A_wins"
        elif win_rate_a < 0.4:
            return "B_wins"
        else:
            return "draw"
    
    def _update_elo_scores_swiss(self, standings: Dict[str, Dict], 
                               comparison: Dict[str, Any], sys_a: str, sys_b: str):
        """æ›´æ–°ELOåˆ†æ•°ï¼ˆç‘å£«è½®ç‰ˆæœ¬ï¼Œä½¿ç”¨åŠ æƒç®—æ³•ï¼‰"""
        summary = comparison["summary"]
        win_rate_a = summary["win_rate_a"]
        win_rate_b = summary["win_rate_b"]
        
        elo_a = standings[sys_a]["elo"]
        elo_b = standings[sys_b]["elo"]
        
        # è®¡ç®—æœŸæœ›èƒœç‡
        expected_a = 1 / (1 + 10 ** ((elo_b - elo_a) / 400))
        expected_b = 1 - expected_a
        
        # è®¡ç®—åˆ†å·®å’ŒåŠ æƒç³»æ•°
        rating_diff = abs(elo_a - elo_b)
        base_k = self.config.k_factor
        
        # åŠ æƒç³»æ•°ï¼šåŸºäºåˆ†å·®çš„éçº¿æ€§å‡½æ•°
        weight_factor = 0.5 + 1.5 * (1 - math.exp(-rating_diff / 200))
        
        # çˆ†å†·å¥–åŠ±
        upset_bonus_a = 1.0
        upset_bonus_b = 1.0
        
        if elo_a < elo_b and win_rate_a > 0.5:  # Açˆ†å†·å‡»è´¥B
            upset_bonus_a = 1.0 + (rating_diff / 400)
            upset_bonus_b = 1.0 + (rating_diff / 600)
        elif elo_b < elo_a and win_rate_b > 0.5:  # Bçˆ†å†·å‡»è´¥A
            upset_bonus_b = 1.0 + (rating_diff / 400)
            upset_bonus_a = 1.0 + (rating_diff / 600)
        
        # è®¡ç®—æœ€ç»ˆKå› å­
        k_a = base_k * weight_factor * upset_bonus_a
        k_b = base_k * weight_factor * upset_bonus_b
        
        # æ›´æ–°ELO
        standings[sys_a]["elo"] += k_a * (win_rate_a - expected_a)
        standings[sys_b]["elo"] += k_b * (win_rate_b - expected_b)
    
    def _update_swiss_standings(self, standings: Dict[str, Dict], 
                              sys_a: str, sys_b: str, result: str):
        """æ›´æ–°ç‘å£«è½®ç§¯åˆ†å’Œæˆ˜ç»©"""
        # è®°å½•å¯¹æ‰‹
        standings[sys_a]["opponents"].append(sys_b)
        standings[sys_b]["opponents"].append(sys_a)
        
        # æ›´æ–°ç§¯åˆ†å’Œæˆ˜ç»©
        if result == "A_wins":
            standings[sys_a]["swiss_points"] += 1.0
            standings[sys_a]["wins"] += 1
            standings[sys_b]["losses"] += 1
        elif result == "B_wins":
            standings[sys_b]["swiss_points"] += 1.0
            standings[sys_b]["wins"] += 1
            standings[sys_a]["losses"] += 1
        else:  # draw
            standings[sys_a]["swiss_points"] += 0.5
            standings[sys_b]["swiss_points"] += 0.5
            standings[sys_a]["draws"] += 1
            standings[sys_b]["draws"] += 1
    
    def _update_sb_scores(self, standings: Dict[str, Dict]):
        """æ›´æ–°SBåˆ†ï¼ˆå¯¹æ‰‹åˆ†æ•°æ€»å’Œï¼‰"""
        for system in standings:
            sb_score = 0.0
            for opponent in standings[system]["opponents"]:
                sb_score += standings[opponent]["swiss_points"]
            standings[system]["sb_score"] = sb_score
    
    def _get_current_standings_snapshot(self, standings: Dict[str, Dict]) -> Dict[str, Dict]:
        """è·å–å½“å‰ç§¯åˆ†æ¦œå¿«ç…§"""
        return {system: stats.copy() for system, stats in standings.items()}
    
    def _get_current_ranking(self, standings: Dict[str, Dict]) -> List[Tuple[str, Dict]]:
        """è·å–å½“å‰æ’å"""
        return sorted(
            standings.items(),
            key=lambda x: (x[1]["swiss_points"], x[1]["elo"], x[1]["sb_score"]),
            reverse=True
        )
    
    def _calculate_swiss_ranking(self, final_standings: Dict[str, Dict]) -> List[str]:
        """è®¡ç®—ç‘å£«è½®æœ€ç»ˆæ’å"""
        # æ’åè§„åˆ™ï¼š
        # 1. ç‘å£«è½®ç§¯åˆ†ï¼ˆèƒœ1åˆ†ï¼Œå¹³0.5åˆ†ï¼Œè´Ÿ0åˆ†ï¼‰
        # 2. ELOåˆ†æ•°
        # 3. SBåˆ†ï¼ˆå¯¹æ‰‹åˆ†æ•°æ€»å’Œï¼‰
        # 4. èƒœåœºæ•°
        # 5. ç³»ç»Ÿåç§°ï¼ˆå­—å…¸åºï¼‰
        
        ranked_systems = sorted(
            final_standings.items(),
            key=lambda x: (
                x[1]["swiss_points"],    # ä¸»è¦ï¼šç‘å£«è½®ç§¯åˆ†
                x[1]["elo"],            # æ¬¡è¦ï¼šELOåˆ†æ•°
                x[1]["sb_score"],       # ç¬¬ä¸‰ï¼šSBåˆ†
                x[1]["wins"],           # ç¬¬å››ï¼šèƒœåœºæ•°
                x[0]                    # ç¬¬äº”ï¼šç³»ç»Ÿåç§°
            ),
            reverse=True
        )
        
        return [system for system, _ in ranked_systems]
    
    def _bootstrap_ci_analysis(self, pairwise_results: List[Dict], system_names: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œbootstrap CIåˆ†æ"""
        all_score_diffs = []
        for result in pairwise_results:
            for qr in result["question_results"]:
                # ä½¿ç”¨å¾—åˆ†å·®å€¼ä»£æ›¿elo_delta
                score_diff = qr["score_a"] - qr["score_b"]
                all_score_diffs.append(score_diff)

        if not all_score_diffs:
            return {
                "mean_score_diff": 0.0,
                "ci_95": "0.00 - 0.00",
                "significance": "æ— æ•°æ®"
            }

        # è®¡ç®—å¹³å‡å¾—åˆ†å·®
        mean_score_diff = np.mean(all_score_diffs)

        # æ‰§è¡Œbootstrap CI
        try:
            from scipy.stats import bootstrap
            boot_results = bootstrap((all_score_diffs,), np.mean, confidence_level=0.95, n_resamples=1000)
            ci_95 = boot_results.confidence_interval
            ci_95_str = f"{ci_95.low:.2f} - {ci_95.high:.2f}"
            
            # æ˜¾è‘—æ€§åˆ¤æ–­ (åŸºäºCI)
            significance = "æ˜¾è‘—" if not (ci_95.low <= 0 <= ci_95.high) else "ä¸æ˜¾è‘—"
        except Exception as e:
            self.logger.warning(f"Bootstrap CIè®¡ç®—å¤±è´¥: {e}")
            ci_95_str = "è®¡ç®—å¤±è´¥"
            significance = "æœªçŸ¥"

        return {
            "mean_score_diff": mean_score_diff,
            "ci_95": ci_95_str,
            "significance": significance
        }
    
    def _cluster_failure_modes(self, pairwise_results: List[Dict]) -> Dict[str, Any]:
        """åŠ¨æ€è¯­ä¹‰èšç±»åˆ†æå¤±è´¥æ¨¡å¼ - åŸºäºLLMå›ç­”çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        # æ”¶é›†æ‰€æœ‰å¤±è´¥åŸå› æ–‡æœ¬
        failure_reasons = []
        reason_to_systems = {}
        
        for result in pairwise_results:
            for qr in result["question_results"]:
                passage_judgment = qr.get("passage_judgment", {})
                reason = passage_judgment.get("reason", "")
                if reason and len(reason.strip()) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„åŸå› 
                    failure_reasons.append(reason.strip())
                    if reason not in reason_to_systems:
                        reason_to_systems[reason] = set()
                    reason_to_systems[reason].add(result["system_a"])
                    reason_to_systems[reason].add(result["system_b"])

        if len(failure_reasons) < 5:
            # æ•°æ®ä¸è¶³ï¼Œè¿”å›ç®€å•ç»Ÿè®¡
            return {
                "cluster_0": {
                    "label": "å¤±è´¥åŸå› åˆ†æ",
                    "systems": list(set().union(*reason_to_systems.values())) if reason_to_systems else [],
                    "reasons": failure_reasons,
                    "top_keywords": self._extract_top_keywords(failure_reasons),
                    "size": len(failure_reasons)
                }
            }

        try:
            if not SKLEARN_AVAILABLE:
                self.logger.warning("sklearnä¸å¯ç”¨ï¼Œè·³è¿‡åŠ¨æ€è¯­ä¹‰èšç±»ï¼Œè¿”å›ç®€å•ç»Ÿè®¡")
                return {
                    "cluster_0": {
                        "label": "å¤±è´¥åŸå› åˆ†æ(ç®€åŒ–æ¨¡å¼)",
                        "systems": list(set().union(*reason_to_systems.values())) if reason_to_systems else [],
                        "reasons": failure_reasons,
                        "top_keywords": self._extract_top_keywords(failure_reasons),
                        "size": len(failure_reasons)
                    }
                }
            
            # åŠ¨æ€TF-IDFå‘é‡åŒ–ï¼ˆä¸­æ–‡åˆ†è¯å‹å¥½ï¼‰
            vectorizer = TfidfVectorizer(
                max_features=200, 
                stop_words=None, 
                ngram_range=(1, 3),
                min_df=1,
                max_df=0.8,
                token_pattern=r'[\u4e00-\u9fff]+|[a-zA-Z]+\d*'  # ä¸­æ–‡å­—ç¬¦æˆ–è‹±æ–‡å•è¯
            )
            tfidf_matrix = vectorizer.fit_transform(failure_reasons)
            
            # åŠ¨æ€ç¡®å®šèšç±»æ•°é‡ï¼ˆåŸºäºæ•°æ®è§„æ¨¡å’Œè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
            n_clusters = self._determine_optimal_clusters(tfidf_matrix, failure_reasons)
            
            if n_clusters <= 1:
                # èšç±»æ•ˆæœä¸ä½³ï¼Œè¿”å›ç»Ÿä¸€åˆ†æ
                return {
                    "cluster_0": {
                        "label": "é€šç”¨å¤±è´¥æ¨¡å¼",
                        "systems": list(set().union(*reason_to_systems.values())) if reason_to_systems else [],
                        "reasons": failure_reasons,
                        "top_keywords": self._extract_top_keywords(failure_reasons),
                        "size": len(failure_reasons)
                    }
                }
            
            # K-meansèšç±»
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # åŠ¨æ€ç”Ÿæˆèšç±»æ ‡ç­¾
            feature_names = vectorizer.get_feature_names_out()
            clusters = {}
            
            for cluster_id in range(n_clusters):
                cluster_reasons = [failure_reasons[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
                cluster_systems = set()
                
                # æ”¶é›†è¯¥èšç±»å¯¹åº”çš„ç³»ç»Ÿ
                for reason in cluster_reasons:
                    if reason in reason_to_systems:
                        cluster_systems.update(reason_to_systems[reason])
                
                # åŠ¨æ€ç”Ÿæˆèšç±»æ ‡ç­¾ï¼ˆåŸºäºTF-IDFæƒé‡æœ€é«˜çš„è¯ï¼‰
                cluster_label = self._generate_cluster_label(cluster_reasons, feature_names, kmeans.cluster_centers_[cluster_id])
                
                # æå–è¯¥èšç±»çš„å…³é”®è¯
                top_keywords = self._extract_cluster_keywords(cluster_reasons, feature_names)
                
                clusters[f"cluster_{cluster_id}"] = {
                    "label": cluster_label,
                    "systems": list(cluster_systems),
                    "reasons": cluster_reasons,
                    "top_keywords": top_keywords,
                    "size": len(cluster_reasons)
                }
            
            # æŒ‰èšç±»å¤§å°æ’åº
            sorted_clusters = dict(sorted(clusters.items(), key=lambda x: x[1]["size"], reverse=True))
            
            return sorted_clusters
            
        except Exception as e:
            self.logger.warning(f"åŠ¨æ€è¯­ä¹‰èšç±»å¤±è´¥: {e}")
            # è¿”å›ç®€å•çš„å…³é”®è¯ç»Ÿè®¡
            return {
                "cluster_0": {
                    "label": "å¤±è´¥æ¨¡å¼åˆ†æ",
                    "systems": list(set().union(*reason_to_systems.values())) if reason_to_systems else [],
                    "reasons": failure_reasons,
                    "top_keywords": self._extract_top_keywords(failure_reasons),
                    "size": len(failure_reasons)
                }
            }
    
    def _determine_optimal_clusters(self, tfidf_matrix, failure_reasons: List[str]) -> int:
        """åŠ¨æ€ç¡®å®šæœ€ä¼˜èšç±»æ•°é‡"""
        n_samples = len(failure_reasons)
        
        # åŸºäºæ•°æ®è§„æ¨¡ç¡®å®šèšç±»æ•°é‡èŒƒå›´
        if n_samples < 5:
            return 1
        elif n_samples < 15:
            max_clusters = 2
        elif n_samples < 30:
            max_clusters = 3
        else:
            max_clusters = min(5, n_samples // 8)
        
        # ä½¿ç”¨è½®å»“ç³»æ•°é€‰æ‹©æœ€ä¼˜èšç±»æ•°
        try:
            from sklearn.metrics import silhouette_score
            best_n_clusters = 1
            best_score = -1
            
            for n in range(2, max_clusters + 1):
                kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)
                labels = kmeans.fit_predict(tfidf_matrix)
                score = silhouette_score(tfidf_matrix, labels)
                
                if score > best_score and score > 0.3:  # è¦æ±‚ä¸€å®šçš„èšç±»è´¨é‡
                    best_score = score
                    best_n_clusters = n
            
            return best_n_clusters
            
        except Exception:
            # è½®å»“åˆ†æå¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™
            return min(3, max(1, n_samples // 10))
    
    def _generate_cluster_label(self, cluster_reasons: List[str], 
                              feature_names: list, cluster_center: list) -> str:
        """åŸºäºTF-IDFæƒé‡åŠ¨æ€ç”Ÿæˆèšç±»æ ‡ç­¾"""
        try:
            # è·å–æƒé‡æœ€é«˜çš„å‰3ä¸ªç‰¹å¾
            top_indices = sorted(range(len(cluster_center)), 
                               key=lambda i: cluster_center[i], reverse=True)[:3]
            top_features = [feature_names[i] for i in top_indices if cluster_center[i] > 0]
            
            if not top_features:
                return "æœªåˆ†ç±»å¤±è´¥æ¨¡å¼"
            
            # åŸºäºå…³é”®ç‰¹å¾ç”Ÿæˆæœ‰æ„ä¹‰çš„æ ‡ç­¾
            label_mapping = {
                ('æ£€ç´¢', 'ç¼ºå¤±', 'æ®µè½'): "æ£€ç´¢ç¼ºå…³é”®æ®µ",
                ('æ•°å­—', 'é”™è¯¯', 'è®¡ç®—'): "æ•°å€¼è®¡ç®—é”™è¯¯", 
                ('é€»è¾‘', 'è·³è·ƒ', 'æ¨ç†'): "é€»è¾‘æ¨ç†é—®é¢˜",
                ('è¯æ®', 'ä¸è¶³', 'æ”¯æ’‘'): "è¯æ®æ”¯æ’‘ä¸è¶³",
                ('å›ç­”', 'ä¸å®Œæ•´', 'ç¼ºå¤±'): "å›ç­”ä¸å®Œæ•´",
                ('ç†è§£', 'é”™è¯¯', 'ç†è§£'): "ç†è§£åå·®",
                ('æ ¼å¼', 'é”™è¯¯', 'ç»“æ„'): "æ ¼å¼ç»“æ„é—®é¢˜"
            }
            
            # å°è¯•åŒ¹é…é¢„å®šä¹‰æ¨¡å¼
            top_features_str = ' '.join(top_features)
            for pattern, label in label_mapping.items():
                if any(keyword in top_features_str for keyword in pattern):
                    return label
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼ŒåŸºäºæœ€é‡è¦çš„ç‰¹å¾ç”Ÿæˆæ ‡ç­¾
            main_feature = top_features[0]
            if 'æ£€ç´¢' in main_feature or 'æŸ¥æ‰¾' in main_feature:
                return "æ£€ç´¢ç›¸å…³é—®é¢˜"
            elif 'å›ç­”' in main_feature or 'ç­”æ¡ˆ' in main_feature:
                return "å›ç­”è´¨é‡é—®é¢˜"
            elif 'é€»è¾‘' in main_feature or 'æ¨ç†' in main_feature:
                return "é€»è¾‘æ¨ç†é—®é¢˜"
            elif 'æ•°å­—' in main_feature or 'è®¡ç®—' in main_feature:
                return "æ•°å€¼å¤„ç†é—®é¢˜"
            else:
                return f"{main_feature}ç›¸å…³é—®é¢˜"
                
        except Exception:
            return "å¤±è´¥æ¨¡å¼"
    
    def _extract_cluster_keywords(self, cluster_reasons: List[str], feature_names: list) -> List[Tuple[str, int]]:
        """æå–èšç±»çš„å…³é”®è¯åŠé¢‘æ¬¡"""
        try:
            if not SKLEARN_AVAILABLE:
                # ç®€åŒ–çš„å…³é”®è¯æå–
                return self._extract_top_keywords(cluster_reasons)
            
            # é‡æ–°å¯¹è¯¥èšç±»çš„æ–‡æœ¬è¿›è¡ŒTF-IDFåˆ†æ
            vectorizer = TfidfVectorizer(
                max_features=50,
                ngram_range=(1, 2),
                token_pattern=r'[\u4e00-\u9fff]+|[a-zA-Z]+\d*'
            )
            tfidf_matrix = vectorizer.fit_transform(cluster_reasons)
            feature_names = vectorizer.get_feature_names_out()
            
            # è®¡ç®—TF-IDFæ€»åˆ†
            tfidf_scores = tfidf_matrix.sum(axis=0).A1
            
            # è·å–å‰5ä¸ªå…³é”®è¯
            top_indices = sorted(range(len(tfidf_scores)), 
                               key=lambda i: tfidf_scores[i], reverse=True)[:5]
            
            top_keywords = []
            for idx in top_indices:
                if tfidf_scores[idx] > 0:
                    keyword = feature_names[idx]
                    # è®¡ç®—è¯¥è¯åœ¨æ–‡æœ¬ä¸­çš„å‡ºç°æ¬¡æ•°
                    count = sum(1 for reason in cluster_reasons if keyword in reason)
                    top_keywords.append((keyword, count))
            
            return top_keywords
            
        except Exception:
            return self._extract_top_keywords(cluster_reasons)
    
    def _extract_top_keywords(self, reasons: List[str]) -> List[Tuple[str, int]]:
        """ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        common_keywords = [
            "æ£€ç´¢", "ç¼ºå¤±", "ä¸è¶³", "é”™è¯¯", "ä¸å‡†ç¡®", "ä¸å®Œæ•´", "ä¸ç›¸å…³", 
            "é€»è¾‘", "æ¨ç†", "è¯æ®", "æ”¯æ’‘", "å›ç­”", "æ•°å­—", "è®¡ç®—", "ç†è§£",
            "æ®µè½", "æ–‡æ¡£", "ä¿¡æ¯", "å…³é”®", "é‡è¦", "é—æ¼", "åå·®"
        ]
        
        keyword_counts = defaultdict(int)
        all_text = ' '.join(reasons)
        
        for keyword in common_keywords:
            count = all_text.count(keyword)
            if count > 0:
                keyword_counts[keyword] = count
        
        # è¿”å›å‰5ä¸ªå…³é”®è¯
        return sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    
    def _swiss_tournament(self, system_names: List[str], all_systems: Dict[str, List[Dict]], 
                         num_rounds: int = 4) -> Dict[str, Any]:
        """ç‘å£«è½®é”¦æ ‡èµ› - 4è½®æ¯”èµ›ï¼Œæ¯è½®4åœºï¼Œæ¯é˜Ÿæ¯è½®åªæ¯”ä¸€åœº"""
        self.logger.info(f"ğŸ”„ å¼€å§‹ç‘å£«è½®é”¦æ ‡èµ›ï¼Œ{num_rounds}è½®æ¯”èµ›")
        
        # åˆå§‹åŒ–æ‰€æœ‰é˜Ÿä¼Elo=1500ï¼ˆæ— å…ˆéªŒä¿¡æ¯ï¼‰
        elo_scores = {system: 1500.0 for system in system_names}
        match_history = set()  # è®°å½•å·²å¯¹æˆ˜çš„é˜Ÿä¼å¯¹
        all_pairwise_results = []
        match_records = []
        total_llm_calls = 0
        
        # ç‘å£«è½®è¿›åº¦æ¡
        tournament_progress = tqdm(range(1, num_rounds + 1), 
                                 desc="ğŸ† ç‘å£«è½®è¿›åº¦", 
                                 unit="è½®",
                                 ncols=100,
                                 colour='green')
        
        for round_num in tournament_progress:
            self.logger.info(f"ğŸ ç¬¬{round_num}è½®æ¯”èµ›")
            
            # ä¸ºå½“å‰è½®æ¬¡é€‰æ‹©é…å¯¹
            round_pairs = self._select_swiss_round_pairs(elo_scores, match_history, system_names)
            
            if not round_pairs:
                self.logger.info("æ— æ³•æ‰¾åˆ°æ›´å¤šæœ‰æ•ˆé…å¯¹ï¼Œæå‰ç»“æŸ")
                tournament_progress.close()
                break
            
            # æ‰§è¡Œå½“å‰è½®æ¬¡çš„æ‰€æœ‰æ¯”èµ›
            for match_idx, (sys_a, sys_b) in enumerate(round_pairs, 1):
                match_num = (round_num - 1) * 4 + match_idx
                self.logger.info(f"  ğŸ“Š ç¬¬{match_num}åœº: {sys_a} (ELO: {elo_scores[sys_a]:.1f}) vs {sys_b} (ELO: {elo_scores[sys_b]:.1f})")
                
                # è®°å½•è¿™åœºå¯¹æˆ˜
                match_history.add((sys_a, sys_b))
                match_history.add((sys_b, sys_a))  # åŒå‘è®°å½•
                
                # æ‰§è¡Œå¯¹æ¯”
                comparison = self._pairwise_comparison(
                    all_systems[sys_a], all_systems[sys_b], sys_a, sys_b,
                    max_questions=self.config.max_questions
                )
                all_pairwise_results.append(comparison)
                total_llm_calls += len(comparison["question_results"])
                
                # æ›´æ–°Eloåˆ†æ•°
                old_elo_a, old_elo_b = elo_scores[sys_a], elo_scores[sys_b]
                self._update_elo_scores_dynamic(elo_scores, comparison, sys_a, sys_b)
                
                # è®°å½•è¯¦ç»†æ¯”èµ›ä¿¡æ¯
                match_records.append({
                    "round": round_num,
                    "match_num": match_num,
                    "system_a": sys_a,
                    "system_b": sys_b,
                    "old_elo_a": old_elo_a,
                    "old_elo_b": old_elo_b,
                    "new_elo_a": elo_scores[sys_a],
                    "new_elo_b": elo_scores[sys_b],
                    "winner": self._determine_winner(comparison),
                    "comparison": comparison
                })
            
            # è¾“å‡ºå½“å‰è½®æ¬¡åçš„æ’å
            current_ranking = sorted(system_names, key=lambda x: elo_scores[x], reverse=True)
            self.logger.info(f"  ç¬¬{round_num}è½®åæ’å: {current_ranking[0]}({elo_scores[current_ranking[0]]:.1f}) > {current_ranking[1]}({elo_scores[current_ranking[1]]:.1f}) > {current_ranking[2]}({elo_scores[current_ranking[2]]:.1f})")
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            tournament_progress.set_description(f"ğŸ† ç¬¬{round_num}è½®å®Œæˆ - é¢†å…ˆ: {current_ranking[0]}")
        
        # å…³é—­è¿›åº¦æ¡
        tournament_progress.close()
        
        return {
            "match_records": match_records,
            "all_pairwise_results": all_pairwise_results,
            "final_elo_scores": elo_scores,
            "total_llm_calls": total_llm_calls,
            "total_matches": len(match_records),
            "total_rounds": num_rounds
        }
    
    def _select_swiss_round_pairs(self, elo_scores: Dict[str, float], match_history: set, 
                                 system_names: List[str]) -> List[Tuple[str, str]]:
        """ä¸ºç‘å£«è½®é€‰æ‹©å½“å‰è½®æ¬¡çš„é…å¯¹ - æ”¹è¿›ç‰ˆæœ¬"""
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å¯¹æˆ˜ç»„åˆ
        all_possible_pairs = []
        for i, sys_a in enumerate(system_names):
            for sys_b in system_names[i+1:]:
                if (sys_a, sys_b) not in match_history:
                    elo_diff = abs(elo_scores[sys_a] - elo_scores[sys_b])
                    all_possible_pairs.append((sys_a, sys_b, elo_diff))
        
        # æŒ‰Eloå·®è·æ’åºï¼ˆä¼˜å…ˆé€‰æ‹©Eloæ¥è¿‘çš„å¯¹æˆ˜ï¼‰
        all_possible_pairs.sort(key=lambda x: x[2])
        
        # ä½¿ç”¨å›æº¯ç®—æ³•æ‰¾åˆ°æœ€ä¼˜çš„4åœºå¯¹æˆ˜ç»„åˆ
        best_combination = self._find_best_round_combination(all_possible_pairs, len(system_names) // 2)
        
        if best_combination:
            return [(pair[0], pair[1]) for pair in best_combination]
        else:
            self.logger.warning("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç‘å£«è½®é…å¯¹ç»„åˆ")
            return []
    
    def _find_best_round_combination(self, all_pairs: List[Tuple[str, str, float]], 
                                   target_pairs: int) -> List[Tuple[str, str, float]]:
        """ä½¿ç”¨å›æº¯ç®—æ³•æ‰¾åˆ°æœ€ä¼˜çš„è½®æ¬¡å¯¹æˆ˜ç»„åˆ"""
        def backtrack(used_systems: set, current_pairs: List[Tuple[str, str, float]], 
                     pair_index: int) -> List[Tuple[str, str, float]]:
            # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„é…å¯¹ï¼Œè¿”å›ç»“æœ
            if len(current_pairs) == target_pairs:
                return current_pairs.copy()
            
            # å¦‚æœå·²ç»æ£€æŸ¥å®Œæ‰€æœ‰å¯èƒ½çš„é…å¯¹ï¼Œè¿”å›None
            if pair_index >= len(all_pairs):
                return None
            
            # å°è¯•åŒ…å«å½“å‰é…å¯¹
            sys_a, sys_b, elo_diff = all_pairs[pair_index]
            if sys_a not in used_systems and sys_b not in used_systems:
                used_systems.add(sys_a)
                used_systems.add(sys_b)
                current_pairs.append(all_pairs[pair_index])
                
                result = backtrack(used_systems, current_pairs, pair_index + 1)
                if result:
                    return result
                
                # å›æº¯
                current_pairs.pop()
                used_systems.remove(sys_a)
                used_systems.remove(sys_b)
            
            # å°è¯•è·³è¿‡å½“å‰é…å¯¹
            return backtrack(used_systems, current_pairs, pair_index + 1)
        
        # å¼€å§‹å›æº¯æœç´¢
        result = backtrack(set(), [], 0)
        return result if result else []
    
    def _dynamic_elo_tournament(self, system_names: List[str], all_systems: Dict[str, List[Dict]], 
                               max_matches: int) -> Dict[str, Any]:
        """åŠ¨æ€Eloé…å¯¹é”¦æ ‡èµ› - æ ¹æ®updated_recommandation.md"""
        self.logger.info(f"ğŸ”„ å¼€å§‹åŠ¨æ€Eloé…å¯¹é”¦æ ‡èµ›ï¼Œæœ€å¤§{max_matches}åœºæ¯”èµ›")
        
        # åˆå§‹åŒ–æ‰€æœ‰é˜Ÿä¼Elo=1500ï¼ˆæ— å…ˆéªŒä¿¡æ¯ï¼‰
        elo_scores = {system: 1500.0 for system in system_names}
        match_history = set()  # è®°å½•å·²å¯¹æˆ˜çš„é˜Ÿä¼å¯¹
        all_pairwise_results = []
        match_records = []
        total_llm_calls = 0
        
        # åŠ¨æ€é…å¯¹ç›´åˆ°è¾¾åˆ°æœ€å¤§åœºæ¬¡ - æ·»åŠ æ€»ä½“è¿›åº¦æ¡
        tournament_progress = tqdm(range(1, max_matches + 1), 
                                 desc="ğŸ† é”¦æ ‡èµ›è¿›åº¦", 
                                 unit="åœºæ¯”èµ›",
                                 ncols=100,
                                 colour='green')
        
        for match_num in tournament_progress:
            self.logger.info(f"ğŸ ç¬¬{match_num}åœºæ¯”èµ›")
            
            # é€‰æ‹©å½“å‰Eloæœ€æ¥è¿‘çš„æœªå¯¹æˆ˜è¿‡çš„ä¸¤é˜Ÿ
            best_pair = self._find_best_elo_pair(elo_scores, match_history)
            
            if not best_pair:
                self.logger.info("æ‰€æœ‰å¯èƒ½çš„å¯¹æˆ˜å·²å®Œæˆï¼Œæå‰ç»“æŸ")
                tournament_progress.close()
                break
                
            sys_a, sys_b = best_pair
            self.logger.info(f"  ğŸ“Š {sys_a} (ELO: {elo_scores[sys_a]:.1f}) vs {sys_b} (ELO: {elo_scores[sys_b]:.1f})")
            
            # è®°å½•è¿™åœºå¯¹æˆ˜
            match_history.add((sys_a, sys_b))
            match_history.add((sys_b, sys_a))  # åŒå‘è®°å½•
            
            # æ‰§è¡Œå¯¹æ¯”
            comparison = self._pairwise_comparison(
                all_systems[sys_a], all_systems[sys_b], sys_a, sys_b,
                max_questions=self.config.max_questions  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å®Œæ•´é¢˜ç›®æ•°
            )
            all_pairwise_results.append(comparison)
            total_llm_calls += len(comparison["question_results"])
            
            # æ›´æ–°Eloåˆ†æ•°ï¼ˆä½¿ç”¨åŠ æƒç®—æ³•ï¼‰
            old_elo_a, old_elo_b = elo_scores[sys_a], elo_scores[sys_b]
            self._update_elo_scores_dynamic(elo_scores, comparison, sys_a, sys_b)
            
            # è®°å½•è¯¦ç»†æ¯”èµ›ä¿¡æ¯
            match_records.append({
                "match_num": match_num,
                "system_a": sys_a,
                "system_b": sys_b,
                "old_elo_a": old_elo_a,
                "old_elo_b": old_elo_b,
                "new_elo_a": elo_scores[sys_a],
                "new_elo_b": elo_scores[sys_b],
                "winner": self._determine_winner(comparison),
                "comparison": comparison
            })
            
            # è¾“å‡ºå½“å‰æ’åï¼ˆå‰3åï¼‰
            current_ranking = sorted(system_names, key=lambda x: elo_scores[x], reverse=True)
            self.logger.info(f"  å½“å‰æ’å: {current_ranking[0]}({elo_scores[current_ranking[0]]:.1f}) > {current_ranking[1]}({elo_scores[current_ranking[1]]:.1f}) > {current_ranking[2]}({elo_scores[current_ranking[2]]:.1f})")
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            tournament_progress.set_description(f"ğŸ† ç¬¬{match_num}åœºå®Œæˆ - é¢†å…ˆ: {current_ranking[0]}")
            
            # æ”¶æ•›æœºåˆ¶å·²ç§»é™¤ - è¿è¡Œå®Œæ•´åœºæ¬¡ä»¥è·å¾—æ›´å‡†ç¡®æ’å
        
        # å…³é—­è¿›åº¦æ¡
        tournament_progress.close()
        
        return {
            "match_records": match_records,
            "all_pairwise_results": all_pairwise_results,
            "final_elo_scores": elo_scores,
            "total_llm_calls": total_llm_calls,
            "total_matches": len(match_records)
        }
    
    def _find_best_elo_pair(self, elo_scores: Dict[str, float], match_history: set) -> Tuple[str, str]:
        """å¯»æ‰¾Eloæœ€æ¥è¿‘çš„æœªå¯¹æˆ˜è¿‡çš„ä¸¤é˜Ÿ"""
        systems = list(elo_scores.keys())
        best_pair = None
        min_elo_diff = float('inf')
        
        for i, sys_a in enumerate(systems):
            for j, sys_b in enumerate(systems[i+1:], i+1):
                # æ£€æŸ¥æ˜¯å¦å·²å¯¹æˆ˜è¿‡
                if (sys_a, sys_b) in match_history:
                    continue
                
                # è®¡ç®—Eloå·®è·
                elo_diff = abs(elo_scores[sys_a] - elo_scores[sys_b])
                
                if elo_diff < min_elo_diff:
                    min_elo_diff = elo_diff
                    best_pair = (sys_a, sys_b)
        
        return best_pair
    
    def _update_elo_scores_dynamic(self, elo_scores: Dict[str, float], 
                                 comparison: Dict[str, Any], sys_a: str, sys_b: str):
        """åŠ¨æ€Eloæ›´æ–°ï¼ˆæ–°çš„soft winè¯„åˆ†æœºåˆ¶ï¼‰- ç®€åŒ–ç‰ˆæœ¬"""
        summary = comparison["summary"]
        
        # æ–°çš„è¯„åˆ†æœºåˆ¶å·²ç»è®¡ç®—å¥½äº†elo_delta
        elo_delta = summary["elo_delta"]
        
        old_elo_a = elo_scores[sys_a]
        old_elo_b = elo_scores[sys_b]
        
        # ç›´æ¥åº”ç”¨elo_deltaï¼ˆAè·å¾—çš„åˆ†æ•°å˜åŒ–ï¼‰
        elo_scores[sys_a] += elo_delta
        elo_scores[sys_b] -= elo_delta  # Bçš„å˜åŒ–ä¸Aç›¸å
        
        # è®°å½•è¯¦ç»†å˜åŒ–ä¿¡æ¯ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        self.logger.debug(f"Eloæ›´æ–°: {sys_a}({old_elo_a:.1f}â†’{elo_scores[sys_a]:.1f}, +{elo_delta:.1f}) vs {sys_b}({old_elo_b:.1f}â†’{elo_scores[sys_b]:.1f}, {-elo_delta:.1f})")
    
    def _determine_winner(self, comparison: Dict[str, Any]) -> str:
        """ç¡®å®šæ¯”èµ›èƒœè€…"""
        summary = comparison["summary"]
        win_rate_a = summary["win_rate_a"]
        
        if win_rate_a > 0.6:
            return "A"
        elif win_rate_a < 0.4:
            return "B"
        else:
            return "Tie"
    
    # æ”¶æ•›æ£€æŸ¥æ–¹æ³•å·²ç§»é™¤ - ä½¿ç”¨å®Œæ•´åœºæ¬¡è¯„ä¼°ä»¥è·å¾—æ›´å‡†ç¡®çš„æ’å
    
    def _calculate_dynamic_ranking(self, final_elo_scores: Dict[str, float]) -> List[str]:
        """åŸºäºæœ€ç»ˆEloåˆ†æ•°è®¡ç®—æ’å"""
        return sorted(final_elo_scores.keys(), key=lambda x: final_elo_scores[x], reverse=True)
    
    def _parse_tournament_rankings(self, tournament_report_path: str = None) -> Dict[str, Dict]:
        """è§£æé”¦æ ‡èµ›æ’åï¼Œæå–1ã€5ã€8åçš„ç³»ç»Ÿä¿¡æ¯"""
        if not tournament_report_path:
            # ä½¿ç”¨é»˜è®¤è·¯å¾„
            tournament_report_path = "dice_simplified_output/tournament_report.md"
        
        try:
            with open(tournament_report_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£ææ’å
            rankings = {}
            lines = content.split('\n')
            
            for i, line in enumerate(lines):
                if '**bge-' in line and '**:' in line:
                    # æå–æ’åï¼ˆä»å½“å‰è¡Œç›´æ¥æå–ï¼‰
                    rank = None
                    
                    # ä»è¡Œé¦–æå–æ’åæ•°å­—
                    line_stripped = line.strip()
                    if line_stripped.startswith('1.'):
                        rank = 1
                    elif line_stripped.startswith('2.'):
                        rank = 2
                    elif line_stripped.startswith('3.'):
                        rank = 3
                    elif line_stripped.startswith('4.'):
                        rank = 4
                    elif line_stripped.startswith('5.'):
                        rank = 5
                    elif line_stripped.startswith('6.'):
                        rank = 6
                    elif line_stripped.startswith('7.'):
                        rank = 7
                    elif line_stripped.startswith('8.'):
                        rank = 8
                    
                    if rank is None:
                        continue
                    
                    parts = line.split('**:')
                    if len(parts) == 2:
                        system_name = parts[0].replace('**', '').strip()
                        # ç§»é™¤æ’åå‰ç¼€ï¼ˆå¦‚ "1. ", "2. " ç­‰ï¼‰
                        if '. ' in system_name:
                            system_name = system_name.split('. ', 1)[1]
                        elo_score = float(parts[1].strip().split()[0])
                        
                        rankings[rank] = {
                            'system_name': system_name,
                            'elo_score': elo_score,
                            'rank': rank
                        }
            
            # ç¡®ä¿æœ‰1ã€5ã€8å
            required_ranks = [1, 5, 8]
            result = {}
            
            for rank in required_ranks:
                if rank in rankings:
                    rank_name = {1: "1st_Place", 5: "5th_Place", 8: "8th_Place"}[rank]
                    result[rank_name] = rankings[rank]
                else:
                    self.logger.warning(f"æœªæ‰¾åˆ°ç¬¬{rank}åçš„ç³»ç»Ÿä¿¡æ¯")
            
            self.logger.info(f"è§£æåˆ°é”¦æ ‡èµ›æ’å: {list(result.keys())}")
            return result
            
        except Exception as e:
            self.logger.error(f"è§£æé”¦æ ‡èµ›æ’åå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„è™šæ‹ŸåŸºçº¿
            return {
                "1st_Place": {"system_name": "tournament_1st", "elo_score": 1520.0, "rank": 1},
                "5th_Place": {"system_name": "tournament_5th", "elo_score": 1495.0, "rank": 5},
                "8th_Place": {"system_name": "tournament_8th", "elo_score": 1480.0, "rank": 8}
            }
    
    def _create_tournament_baseline_data(self, target_data: List[Dict], baseline_info: Dict) -> Tuple[List[Dict], int]:
        """åŸºäºé”¦æ ‡èµ›æ’ååˆ›å»ºåŸºçº¿æ•°æ®"""
        baseline_name = baseline_info['system_name']
        elo_score = baseline_info['elo_score']
        rank = baseline_info['rank']
        
        # æ ¹æ®æ’åè°ƒæ•´ç”Ÿæˆè´¨é‡
        if rank == 1:
            quality_level = "high"
            instruction = f"ä½œä¸ºé”¦æ ‡èµ›ç¬¬1åçš„ç³»ç»Ÿ({baseline_name}, Elo: {elo_score:.1f})ï¼Œè¯·ç”Ÿæˆé«˜è´¨é‡å›ç­”ã€‚è¦æ±‚ï¼š1)æä¾›å®Œæ•´å‡†ç¡®çš„ä¿¡æ¯ï¼Œ2)é€»è¾‘æ¸…æ™°æ¡ç†åˆ†æ˜ï¼Œ3)åŸºäºæƒå¨èµ„æ–™ï¼Œ4)è¡¨è¿°ä¸“ä¸šå‡†ç¡®ã€‚"
        elif rank == 5:
            quality_level = "medium"
            instruction = f"ä½œä¸ºé”¦æ ‡èµ›ç¬¬5åçš„ç³»ç»Ÿ({baseline_name}, Elo: {elo_score:.1f})ï¼Œè¯·ç”Ÿæˆä¸­ç­‰è´¨é‡å›ç­”ã€‚è¦æ±‚ï¼š1)åŒ…å«ä¸»è¦ä¿¡æ¯ä½†å¯èƒ½ç¼ºå°‘ç»†èŠ‚ï¼Œ2)è¡¨è¿°åŸºæœ¬å‡†ç¡®ä½†ä¸å¤Ÿæ·±å…¥ï¼Œ3)ä¿¡æ¯å®Œæ•´æ€§ä¸­ç­‰ã€‚"
        else:  # rank == 8
            quality_level = "low"
            instruction = f"ä½œä¸ºé”¦æ ‡èµ›ç¬¬8åçš„ç³»ç»Ÿ({baseline_name}, Elo: {elo_score:.1f})ï¼Œè¯·ç”Ÿæˆè¾ƒä½è´¨é‡å›ç­”ã€‚è¦æ±‚ï¼š1)ä¿¡æ¯ä¸å¤Ÿå‡†ç¡®æˆ–æœ‰é—æ¼ï¼Œ2)è¡¨è¿°å¯èƒ½å«ç³Šä¸æ¸…ï¼Œ3)å¯èƒ½åŒ…å«é”™è¯¯æˆ–æ— å…³ä¿¡æ¯ã€‚"
        
        # ç”ŸæˆåŸºçº¿æ•°æ®
        baseline_data = []
        generation_calls = 0
        
        for item in target_data:
            question = item['question']
            groundtruth = item['groundtruth']
            
            # ç”ŸæˆåŸºçº¿å›ç­”
            baseline_answer = self._generate_baseline_answer(question, groundtruth, instruction, quality_level)
            generation_calls += 1
            
            # ç”ŸæˆåŸºçº¿ä¸Šä¸‹æ–‡
            baseline_contexts = self._generate_baseline_contexts(question, groundtruth, quality_level)
            generation_calls += 3  # 3ä¸ªä¸Šä¸‹æ–‡
            
            baseline_data.append({
                'question': question,
                'groundtruth': groundtruth,
                'answer': baseline_answer,
                'context': baseline_contexts
            })
        
        return baseline_data, generation_calls
    
    def _summarize_tournament_baseline_comparison(self, baseline_results: Dict, target_system: str) -> Dict:
        """æ€»ç»“é”¦æ ‡èµ›åŸºçº¿å¯¹æ¯”ç»“æœ"""
        summary = {
            "target_system": target_system,
            "comparisons": {}
        }
        
        for rank_name, result in baseline_results.items():
            baseline_info = result["baseline_info"]
            comparison = result["comparison"]
            
            # è®¡ç®—èƒœç‡
            total_questions = len(comparison["question_results"])
            wins = sum(1 for qr in comparison["question_results"] 
                      if qr["passage_judgment"]["win_type"] == "A wins")
            ties = sum(1 for qr in comparison["question_results"] 
                      if qr["passage_judgment"]["win_type"] == "Tie")
            
            win_rate = wins / total_questions if total_questions > 0 else 0
            tie_rate = ties / total_questions if total_questions > 0 else 0
            
            # åˆ¤æ–­ç»“è®º
            if win_rate > 0.6:
                conclusion = f"æ˜¾è‘—ä¼˜äº{rank_name}"
            elif win_rate > 0.4:
                conclusion = f"ç•¥ä¼˜äº{rank_name}"
            elif win_rate > 0.2:
                conclusion = f"ä¸{rank_name}ç›¸å½“"
            else:
                conclusion = f"ä¸å¦‚{rank_name}"
            
            summary["comparisons"][rank_name] = {
                "baseline_system": baseline_info["system_name"],
                "baseline_elo": baseline_info["elo_score"],
                "baseline_rank": baseline_info["rank"],
                "win_rate": win_rate,
                "tie_rate": tie_rate,
                "total_questions": total_questions,
                "conclusion": conclusion
            }
        
        return summary
    
    def scenario_b_baseline_comparison(self, qacg_file: str, target_system: str = None, 
                                     tournament_report_path: str = None) -> Dict[str, Any]:
        """
        åœºæ™¯B: å•ç³»ç»Ÿvsé”¦æ ‡èµ›æ’ååŸºçº¿
        
        Args:
            qacg_file: ç›®æ ‡ç³»ç»Ÿçš„QACGæ–‡ä»¶
            target_system: ç³»ç»Ÿåç§°ï¼ˆå¯é€‰ï¼‰
            tournament_report_path: é”¦æ ‡èµ›æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŸºçº¿å¯¹æ¯”ç»“æœ
        """
        self.logger.info("ğŸ¯ å¼€å§‹åœºæ™¯B: å•ç³»ç»Ÿvsé”¦æ ‡èµ›æ’ååŸºçº¿")
        
        # 1. åŠ è½½ç›®æ ‡ç³»ç»Ÿ
        target_data = self._load_qacg_file(qacg_file)
        if not target_system:
            target_system = Path(qacg_file).stem.replace("qacg_", "")
        
        # 2. è§£æé”¦æ ‡èµ›æ’å
        tournament_rankings = self._parse_tournament_rankings(tournament_report_path)
        
        # 3. ä¸é”¦æ ‡èµ›æ’ååŸºçº¿å¯¹æ¯”
        baseline_results = {}
        total_calls = 0
        
        for rank_name, baseline_info in tournament_rankings.items():
            self.logger.info(f"ğŸ”„ {target_system} vs {rank_name} ({baseline_info['system_name']}) å¯¹æ¯”")
            
            # æ„é€ åŸºçº¿æ•°æ®
            baseline_data, baseline_generation_calls = self._create_tournament_baseline_data(
                target_data, baseline_info
            )
            
            # æ‰§è¡Œå¯¹æ¯”
            comparison_result = self._pairwise_comparison(
                target_data, baseline_data, 
                f"{target_system}", f"{rank_name}_{baseline_info['system_name']}",
                max_questions=self.config.max_questions
            )
            
            # ä¿å­˜åŸºçº¿æ•°æ®ä»¥ä¾›è¯¦ç»†å¯¹æ¯”ä½¿ç”¨
            comparison_result["baseline_data"] = baseline_data
            comparison_result["baseline_generation_calls"] = baseline_generation_calls
            comparison_result["baseline_info"] = baseline_info
            baseline_results[rank_name] = comparison_result
            total_calls += len(comparison_result["question_results"]) + baseline_generation_calls
        
        # 4. ç»Ÿè®¡åˆ†æ
        comparison_summary = self._summarize_tournament_baseline_comparison(baseline_results, target_system)
        
        # 5. ç”Ÿæˆè¯¦ç»†QACGå¯¹æ¯”æ•°æ®
        detailed_qacg_comparisons = self._generate_detailed_qacg_comparisons(target_data, target_system, baseline_results)
        
        result = {
            "config": self._config_to_dict(),
            "target_system": target_system,
            "tournament_rankings": tournament_rankings,
            "baseline_comparisons": baseline_results,
            "summary": comparison_summary,
            "detailed_qacg_comparisons": detailed_qacg_comparisons,
            "total_llm_calls": total_calls
        }
        
        # ä¿å­˜ç»“æœ
        self._save_baseline_result(result)
        return result
    
    def _load_systems(self, qacg_files: List[str]) -> Dict[str, List[Dict]]:
        """åŠ è½½æ‰€æœ‰ç³»ç»Ÿæ•°æ®"""
        systems = {}
        for file_path in qacg_files:
            system_name = Path(file_path).stem.replace("qacg_", "")
            systems[system_name] = self._load_qacg_file(file_path)
        return systems
    
    def _load_qacg_file(self, file_path: str) -> List[Dict]:
        """åŠ è½½QACGæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data[:self.config.max_questions]  # é™åˆ¶é¢˜ç›®æ•°é‡
    
    def _create_groups(self, system_names: List[str]) -> List[List[str]]:
        """åˆ›å»ºåˆ†ç»„ (æ ¹æ®ç³»ç»Ÿæ•°é‡è‡ªåŠ¨åˆ†ç»„)"""
        # ç®€å•æŒ‰é¡ºåºåˆ†ç»„ï¼Œå®é™…å¯ä»¥æ ¹æ®é¢„æœŸå®åŠ›åˆ†ç»„
        mid = len(system_names) // 2
        return [system_names[:mid], system_names[mid:]]
    
    def _group_stage(self, group_systems: List[str], all_systems: Dict[str, List[Dict]], 
                    stage_name: str = "å°ç»„èµ›") -> Dict[str, Any]:
        """ç»„å†…å¯¹æˆ˜"""
        self.logger.info(f"ğŸ”„ {stage_name}: {group_systems}")
        
        # åˆå§‹åŒ–Eloåˆ†æ•°
        elo_scores = {system: self.config.initial_elo for system in group_systems}
        pairwise_results = []
        total_calls = 0
        
        # æ‰€æœ‰ä¸¤ä¸¤å¯¹æˆ˜
        for sys_a, sys_b in itertools.combinations(group_systems, 2):
            self.logger.info(f"  ğŸ“Š {sys_a} vs {sys_b}")
            
            # æ‰§è¡Œå¯¹æ¯”
            comparison = self._pairwise_comparison(
                all_systems[sys_a], all_systems[sys_b], sys_a, sys_b
            )
            pairwise_results.append(comparison)
            total_calls += len(comparison["question_results"])
            
            # æ›´æ–°Eloåˆ†æ•°
            self._update_elo_scores(elo_scores, comparison, sys_a, sys_b)
        
        # æ’å
        ranking = sorted(group_systems, key=lambda x: elo_scores[x], reverse=True)
        
        return {
            "stage": stage_name,
            "systems": group_systems,
            "pairwise_results": pairwise_results,
            "elo_scores": elo_scores,
            "ranking": ranking,
            "total_llm_calls": total_calls
        }
    
    def _judge_single_question(self, question_data: Tuple[int, Dict, Dict, str]) -> Tuple[int, Dict[str, Any]]:
        """
        åˆ¤å†³å•ä¸ªé—®é¢˜ï¼ˆç”¨äºå¹¶å‘å¤„ç†ï¼‰- ä½¿ç”¨æ–°çš„soft winæœºåˆ¶
        
        Args:
            question_data: (index, qa_a, qa_b, groundtruth)
            
        Returns:
            (index, question_result): ç´¢å¼•å’Œåˆ¤å†³ç»“æœ
        """
        i, qa_a, qa_b, groundtruth = question_data
        
        try:
            # åªè¿›è¡Œpassageç²’åº¦åˆ¤å†³
            question = qa_a["question"]
            expected_answer = qa_a.get("expected_answer", "")
            
            # # æ‰“å°å½“å‰é—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆå’Œæ­£ç¡®è¯æ®
            # print(f"\nğŸ“‹ é—®é¢˜ {i+1}: {question}")
            # print(f"ğŸ“ æ ‡å‡†ç­”æ¡ˆ: {expected_answer}")
            # print(f"ğŸ“„ æ­£ç¡®è¯æ®: {groundtruth}")
            # print("-" * 80)
            
            # æ„å»ºpassageç²’åº¦å¯¹æ¯”
            passage_judgment = self._judge_passage_only(question, qa_a, qa_b, groundtruth)
            
            # è®¡ç®—soft winå¾—åˆ†
            score_a, score_b = self._calculate_soft_win_score(passage_judgment)
            
            question_result = {
                "question": question,
                "passage_judgment": passage_judgment,
                "score_a": score_a,
                "score_b": score_b,
                "winner": passage_judgment.get("win_type", "Tie"),
                "index": i  # ä¿æŒåŸå§‹é¡ºåº
            }
            
            return i, question_result
            
        except Exception as e:
            # å¤„ç†å¼‚å¸¸æƒ…å†µ
            self.logger.error(f"é—®é¢˜ {i+1} åˆ¤å†³å¤±è´¥: {e}")
            error_result = {
                "question": qa_a.get("question", ""),
                "passage_judgment": {
                    "label": "Tie",
                    "reason": f"åˆ¤å†³å¤±è´¥: {str(e)}",
                    "score": 0.5,
                    "margin_score": 0.0,
                    "granularity": "passage",
                    "logit_a": 0.0,
                    "logit_b": 0.0,
                    "logit_t": 0.0,
                    "prob_a": 0.33,
                    "prob_b": 0.33,
                    "prob_t": 0.33,
                    "win_type": "Error tie",
                    "score_a": 0.5,
                    "score_b": 0.5,
                    "prob_diff": 0.0
                },
                "score_a": 0.5,
                "score_b": 0.5,
                "winner": "Error tie",
                "index": i
            }
            return i, error_result

    def _pairwise_comparison(self, data_a: List[Dict], data_b: List[Dict], 
                           name_a: str, name_b: str, max_questions: int = None) -> Dict[str, Any]:
        """æ‰§è¡Œæˆå¯¹æ¯”è¾ƒï¼ˆæ”¯æŒå¹¶å‘å¤„ç†ï¼‰"""
        if max_questions is None:
            max_questions = self.config.max_questions
        
        total_questions = min(len(data_a), len(data_b), max_questions)
        
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {total_questions} ä¸ªé—®é¢˜...")
        self.logger.info(f"âš™ï¸ å¹¶å‘é…ç½®: {self.config.max_workers} workers, æ‰¹å¤§å°: {self.config.batch_size}")
        
        # å‡†å¤‡æ‰€æœ‰é—®é¢˜æ•°æ®
        questions_data = []
        for i in range(total_questions):
            qa_a = data_a[i]
            qa_b = data_b[i]
            groundtruth = qa_a.get("groundtruth", qa_a.get("expected_answer", ""))
            questions_data.append((i, qa_a, qa_b, groundtruth))
        
        # å¹¶å‘å¤„ç† - æ·»åŠ é—®é¢˜å¤„ç†è¿›åº¦æ¡
        question_results = []
        completed_count = 0
        
        # åˆ›å»ºé—®é¢˜çº§è¿›åº¦æ¡
        question_progress = tqdm(total=total_questions, 
                               desc=f"ğŸ“ {name_a} vs {name_b}", 
                               unit="é¢˜",
                               ncols=100,
                               colour='blue',
                               leave=False)
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, len(questions_data), self.config.batch_size):
            batch_end = min(batch_start + self.config.batch_size, len(questions_data))
            batch_data = questions_data[batch_start:batch_end]
            
            self.logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_start//self.config.batch_size + 1}: é—®é¢˜ {batch_start+1}-{batch_end}")
            
            # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
            with concurrent.futures.ThreadPoolExecutor(max_workers=min(self.config.max_workers, len(batch_data))) as executor:
                # æäº¤ä»»åŠ¡
                future_to_index = {executor.submit(self._judge_single_question, question_data): question_data[0] 
                                 for question_data in batch_data}
                
                # æ”¶é›†ç»“æœ
                batch_results = []
                for future in concurrent.futures.as_completed(future_to_index):
                    try:
                        i, result = future.result()
                        batch_results.append((i, result))
                        completed_count += 1
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        question_progress.update(1)
                        winner = result.get("winner", "Unknown")
                        question_progress.set_description(f"ğŸ“ {name_a} vs {name_b} - æœ€æ–°: {winner}")
                        
                        # è¾“å‡ºè¯¦ç»†çš„åˆ¤å†³ç»“æœï¼ˆé‡è¦ï¼åŒ…å«ç†ç”±ç­‰ä¿¡æ¯ï¼‰
                        with self._lock:
                            self._log_question_result(result, completed_count, total_questions)
                            
                    except Exception as e:
                        i = future_to_index[future]
                        self.logger.error(f"é—®é¢˜ {i+1} å¤„ç†å¼‚å¸¸: {e}")
                
                # æŒ‰åŸå§‹é¡ºåºæ’åº
                batch_results.sort(key=lambda x: x[0])
                question_results.extend([result for _, result in batch_results])
            
            # æ—©åœæœºåˆ¶å·²ç§»é™¤ - æŒ‰ç”¨æˆ·è¦æ±‚å»é™¤æ‰€æœ‰æ”¶æ•›/æ—©åœæœºåˆ¶
        
        # å…³é—­é—®é¢˜è¿›åº¦æ¡
        question_progress.close()
        
        self.logger.info(f"âœ… å¹¶å‘å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(question_results)} ä¸ªé—®é¢˜")
        
        # æ±‡æ€»ç»“æœ - ä½¿ç”¨æ–°çš„ç´¯è®¡è¯„åˆ†æœºåˆ¶
        summary = self._summarize_pairwise_result_with_soft_win(question_results, name_a, name_b)
        
        return {
            "system_a": name_a,
            "system_b": name_b,
            "question_results": question_results,
            "summary": summary
        }
    
    def _judge_passage_only(self, question: str, qa_a: Dict, qa_b: Dict, groundtruth: str) -> Dict[str, Any]:
        """ä»…è¿›è¡Œpassageç²’åº¦åˆ¤å†³ï¼ˆæ£€ç´¢-è¯æ®åŒé€šé“ï¼‰"""
        # æ„å»ºæ£€ç´¢-è¯æ®åŒé€šé“prompt
        context_a = qa_a.get("context", [])
        context_b = qa_b.get("context", [])
        answer_a = qa_a.get("rag_answer", "")
        answer_b = qa_b.get("rag_answer", "")
        expected_answer = qa_a.get("expected_answer", "")
        
        # ç®€åŒ–çš„passageçº§åˆ¤å†³prompt
        prompt = f"""ä½œä¸ºRAGç³»ç»Ÿè¯„ä¼°ä¸“å®¶ï¼Œè¯·å¯¹æ¯”ä¸¤ä¸ªç³»ç»Ÿçš„æ£€ç´¢-å›ç­”è´¨é‡ã€‚

é—®é¢˜: {question}
æ ‡å‡†ç­”æ¡ˆ: {groundtruth}

ç³»ç»ŸA:
æ£€ç´¢è¯æ®: {' '.join(context_a[:3])}  
å›ç­”: {answer_a}

ç³»ç»ŸB:
æ£€ç´¢è¯æ®: {' '.join(context_b[:3])}
å›ç­”: {answer_b}

è¯·ä»ä»¥ä¸‹è§’åº¦å¯¹æ¯”:
1. æ£€ç´¢è¯æ®çš„ç›¸å…³æ€§å’Œå®Œæ•´æ€§
2. å›ç­”çš„å‡†ç¡®æ€§å’Œé€»è¾‘æ€§
3. è¯æ®ä¸å›ç­”çš„ä¸€è‡´æ€§
4. åœ¨ä¸€æ–¹ç»™å‡ºç­”æ¡ˆï¼Œå¦ä¸€æ–¹å›ç­”"ä¿¡æ¯ä¸è¶³"çš„æƒ…å†µä¸‹ï¼Œè¦æ˜¯ç»™å‡ºç­”æ¡ˆçš„é‚£ä¸€æ–¹ç­”æ¡ˆå®Œå…¨é”™è¯¯ï¼ˆä¸æ ‡å‡†ç­”æ¡ˆå®Œå…¨ä¸ä¸€è‡´ï¼‰ï¼Œç®—ä¿¡æ¯ä¸è¶³çš„ä¸€æ–¹èµ¢
5. å¯¹äºç­”æ¡ˆè´¨é‡è¯·éµå®ˆå¦‚ä¸‹æ³•åˆ™ï¼šå®Œå…¨ç­”å¯¹>éƒ¨åˆ†ç­”å¯¹>éƒ¨åˆ†ç­”é”™>ä¿¡æ¯ä¸è¶³>å®Œå…¨é”™è¯¯


åˆ¤å†³æ ¼å¼ï¼š
åˆ¤å†³: [A wins/B wins/Tie]
ç†ç”±: [åŸºäºä¸Šè¿°åŸåˆ™çš„å…·ä½“åˆ†æ]"""

        try:
            # ä½¿ç”¨judge_pairè·å¾—æ·±åº¦æ€è€ƒç»“æœ
            judge_result = self.pairwise_judge.judge_pair(
                question=question,
                qa_a={
                    "rag_answer": answer_a, 
                    "retrieved_docs": context_a,
                    "expected_answer": expected_answer,
                    "groundtruth": groundtruth
                },
                qa_b={
                    "rag_answer": answer_b, 
                    "retrieved_docs": context_b,
                    "expected_answer": expected_answer,  # ä¸¤ä¸ªç³»ç»Ÿçš„æ ‡å‡†ç­”æ¡ˆç›¸åŒ
                    "groundtruth": groundtruth  # ä¸¤ä¸ªç³»ç»Ÿçš„æ ‡å‡†è¯æ®ç›¸åŒ
                },
                granularity="passage",
                atoms={}
            )
            
            # ä»æ·±åº¦åˆ¤å†³ç»“æœä¸­æå–ä¿¡æ¯
            label = judge_result.get("label", "Tie")
            response = judge_result.get("reason", "")
            logit_a = judge_result.get("logit_a", 0.0)
            logit_b = judge_result.get("logit_b", 0.0)
            logit_t = judge_result.get("logit_t", 0.0)
            prob_a = judge_result.get("prob_a", 0.33)
            prob_b = judge_result.get("prob_b", 0.33)
            prob_t = judge_result.get("prob_t", 0.33)
            
            # ç®€åŒ–æ—¥å¿—ï¼šåªè¾“å‡ºå…³é”®ä¿¡æ¯
            # self.logger.info(f"ğŸ” ä»judge_resultè·å–çš„logits: A={logit_a}, B={logit_b}, T={logit_t}")
            # self.logger.info(f"ğŸ” ä»judge_resultè·å–çš„æ¦‚ç‡: A={prob_a:.3f}, B={prob_b:.3f}, T={prob_t:.3f}")
            # self.logger.info(f"ğŸ” judge_resultæ‰€æœ‰é”®: {list(judge_result.keys())}")
            
            # ğŸ”§ æ”¹è¿›ç†ç”±è§£æé€»è¾‘
            reason = "åŸºäºLLMåˆ¤å†³ç»“æœ"  # é»˜è®¤æè¿°
            lines = response.strip().split('\n')
            
            # å¤šç§æ–¹å¼å°è¯•æå–ç†ç”±
            found_reason = False
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # åŒ¹é…å„ç§ç†ç”±æ ¼å¼
                if (line.lower().startswith("ç†ç”±:") or line.lower().startswith("ç†ç”±ï¼š") or 
                    line.lower().startswith("reason:") or line.lower().startswith("åŸå› :")):
                    extracted_reason = line.split(":", 1)[-1].split("ï¼š", 1)[-1].strip()
                    if extracted_reason:  # ç¡®ä¿æå–åˆ°çš„ç†ç”±ä¸ä¸ºç©º
                        reason = extracted_reason
                        found_reason = True
                        break
                elif "å› ä¸º" in line or "ç”±äº" in line or "æ‰€ä»¥" in line:
                    reason = line.strip()
                    found_reason = True
                    break
                elif line.startswith("-") or line.startswith("*"):
                    # å¯èƒ½æ˜¯åˆ—è¡¨æ ¼å¼çš„ç†ç”±
                    reason = line[1:].strip()
                    found_reason = True
                    break
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ ¼å¼çš„ç†ç”±ï¼Œä½¿ç”¨æ•´ä¸ªå“åº”çš„æ‘˜è¦
            if not found_reason and len(response.strip()) > 0:
                # ä»å®Œæ•´å“åº”ä¸­æå–æœ‰æ„ä¹‰çš„å†…å®¹ä½œä¸ºç†ç”±
                response_clean = response.strip()
                
                # æ¸…ç†åºŸè¯ï¼šå»é™¤æ— ç”¨çš„é€‰æ‹©æç¤º
                unwanted_patterns = [
                    "A\n", "B\n", "T\n", "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºåˆ¤å†³",
                    "åˆ¤å†³: [A wins/B wins/Tie]", "ç†ç”±: [ç®€è¦è¯´æ˜åŸå› ]",
                    "ä½ çš„é€‰æ‹©æ˜¯ï¼ˆåªè¾“å‡ºä¸€ä¸ªå­—æ¯ï¼‰ï¼š", "è¯·é€‰æ‹©ï¼š",
                    "A - ç³»ç»ŸAæ›´å¥½", "B - ç³»ç»ŸBæ›´å¥½", "T - ä¸¤ç³»ç»Ÿç›¸å½“"
                ]
                
                for pattern in unwanted_patterns:
                    response_clean = response_clean.replace(pattern, "")
                
                # æ¸…ç†å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
                response_clean = " ".join(response_clean.split())
                
                # å¦‚æœå“åº”å¾ˆé•¿ï¼Œæå–å…³é”®éƒ¨åˆ†ï¼Œä½†ä¸æˆªæ–­
                if len(response_clean) > 300:
                    # å¯»æ‰¾åˆ¤å†³ç›¸å…³çš„å…³é”®å¥å­
                    key_sentences = []
                    for line in lines:
                        line = line.strip()
                        # è·³è¿‡åºŸè¯è¡Œ
                        if line in ["A", "B", "T", ""] or any(unwanted in line for unwanted in unwanted_patterns):
                            continue
                        if any(keyword in line for keyword in ["ç³»ç»ŸA", "ç³»ç»ŸB", "æ›´ä¼˜", "æ›´å¥½", "èƒœå‡º", "å‡†ç¡®", "å®Œæ•´", "ç›¸å…³", "ä¸€è‡´"]):
                            key_sentences.append(line)
                    
                    if key_sentences:
                        reason = " ".join(key_sentences)  # å–æ‰€æœ‰å…³é”®å¥å­ï¼Œä¸æˆªæ–­
                    else:
                        reason = response_clean  # ä¿ç•™å®Œæ•´å“åº”ï¼Œä¸æˆªæ–­
                else:
                    reason = response_clean
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç†ç”±ï¼Œå°è¯•ç”¨åˆ¤å†³çš„é€»è¾‘
            if reason == "åŸºäºLLMåˆ¤å†³ç»“æœ" and label != "Tie":
                if label == "A wins":
                    reason = "ç³»ç»ŸAåœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜"
                elif label == "B wins":
                    reason = "ç³»ç»ŸBåœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜"
            
            # è®¡ç®—margin_scoreï¼ˆMargin-Aware Tieï¼‰
            # ä¿®å¤é€»è¾‘ï¼šåªæœ‰å½“Tçš„æ¦‚ç‡ä¸æ˜¯æœ€é«˜æ—¶ï¼Œæ‰è€ƒè™‘A/Bçš„margin
            if label == "Tie":
                # å½“åˆ¤å†³ä¸ºTieæ—¶ï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯æ˜æ˜¾çš„å¹³å±€
                # å¦‚æœTçš„æ¦‚ç‡ç¡®å®æœ€é«˜ï¼Œå°±ä¿æŒTieï¼›å¦åˆ™è€ƒè™‘A/Bçš„ç»†å¾®å·®åˆ«
                max_prob = max(prob_a, prob_b, prob_t)
                if max_prob == prob_t:
                    # Tæ¦‚ç‡æœ€é«˜ï¼Œç¡®å®åº”è¯¥æ˜¯Tie
                    margin_score = 0.0
                    score = 0.5
                else:
                    # Aæˆ–Bæ¦‚ç‡æœ€é«˜ä½†è¢«è¯¯åˆ¤ä¸ºTieï¼Œä½¿ç”¨margin_scoreå¾®è°ƒ
                    margin_score = self._calculate_margin_score(logit_a, logit_b)
                    if abs(margin_score) > 0.05:  # margin_threshold
                        score = 0.5 + margin_score
                        if margin_score > 0:
                            label = "A soft wins"
                        else:
                            label = "B soft wins"
                    else:
                        score = 0.5
            else:
                # éTieåˆ¤å†³ï¼Œè®¡ç®—margin_scoreç”¨äºè®°å½•
                margin_score = self._calculate_margin_score(logit_a, logit_b)
                score = 1.0 if label == "A wins" else (0.0 if label == "B wins" else 0.5)
            
            return {
                "label": label,
                "reason": reason,
                "score": score,
                "margin_score": margin_score,
                "raw_response": response,
                "granularity": "passage",
                "logit_a": logit_a,
                "logit_b": logit_b,
                "logit_t": logit_t,
                "prob_a": prob_a,
                "prob_b": prob_b,
                "prob_t": prob_t
            }
            
        except Exception as e:
            self.logger.error(f"Passageåˆ¤å†³å¤±è´¥: {e}")
            return {
                "label": "Tie",
                "reason": f"åˆ¤å†³å¤±è´¥: {str(e)}",
                "score": 0.5,
                "margin_score": 0.0,
                "raw_response": "",
                "granularity": "passage",
                "logit_a": 0.0,
                "logit_b": 0.0,
                "logit_t": 0.0,
                "prob_a": 0.33,
                "prob_b": 0.33,
                "prob_t": 0.33
            }
    
    def _calculate_margin_score(self, logit_a: float, logit_b: float) -> float:
        """è®¡ç®—Margin-Aware Tieçš„margin_score - ç›´æ¥ä½¿ç”¨logits"""
        # è®¡ç®— logit_A - logit_B çš„å·®å€¼
        logit_diff = logit_a - logit_b
        
        # ç»æ¸©åº¦ 0.1 çš„ softmax æ˜ å°„åˆ° (0,1)
        temperature = 0.1
        margin_raw = 1.0 / (1.0 + math.exp(-logit_diff / temperature))
        
        # æ˜ å°„åˆ°(-0.5, 0.5)èŒƒå›´ï¼Œç”¨äºè°ƒæ•´score
        margin_score = (margin_raw - 0.5)
        
        return margin_score
    
    def _calculate_soft_win_score(self, passage_judgment: Dict[str, Any]) -> Tuple[float, float]:
        """
        è®¡ç®—soft winå¾—åˆ†æœºåˆ¶
        
        Args:
            passage_judgment: åŒ…å«prob_a, prob_b, prob_tçš„åˆ¤å†³ç»“æœ
            
        Returns:
            (score_a, score_b): Aå’ŒBç³»ç»Ÿçš„å¾—åˆ†
        """
        prob_a = passage_judgment.get("prob_a", 0.33)
        prob_b = passage_judgment.get("prob_b", 0.33)
        prob_t = passage_judgment.get("prob_t", 0.33)
        
        # æ‰¾å‡ºæœ€é«˜æ¦‚ç‡å’Œæ¬¡é«˜æ¦‚ç‡
        probs_sorted = sorted([prob_a, prob_b, prob_t], reverse=True)
        max_prob = probs_sorted[0]
        second_prob = probs_sorted[1]
        
        # è®¡ç®—æ¦‚ç‡å·®è·
        prob_diff = max_prob - second_prob
        
        # é˜ˆå€¼0.1åˆ¤æ–­æ˜¯hard winè¿˜æ˜¯soft win
        if prob_diff >= 0.1:
            # Hard win: èƒœè€…å¾—1åˆ†ï¼Œè´¥è€…å¾—0åˆ†
            if max_prob == prob_a:
                score_a, score_b = 1.0, 0.0
                win_type = "A hard wins"
            elif max_prob == prob_b:
                score_a, score_b = 0.0, 1.0
                win_type = "B hard wins"
            else:  # prob_tæ˜¯æœ€é«˜
                score_a, score_b = 0.5, 0.5
                win_type = "Hard tie"
        else:
            # Soft win: ä½¿ç”¨æ¦‚ç‡ä½œä¸ºå¾—åˆ†ï¼Œä½†åªåœ¨Aå’ŒBä¹‹é—´åˆ†é…
            # å°†Tçš„æ¦‚ç‡æŒ‰æ¯”ä¾‹åˆ†é…ç»™Aå’ŒB
            if prob_a + prob_b > 0:
                total_ab = prob_a + prob_b
                # å°†Tæ¦‚ç‡æŒ‰Aå’ŒBçš„ç›¸å¯¹æ¯”ä¾‹åˆ†é…
                score_a = prob_a + prob_t * (prob_a / total_ab)
                score_b = prob_b + prob_t * (prob_b / total_ab)
            else:
                score_a, score_b = 0.5, 0.5
            
            # ç¡®ä¿åˆ†æ•°åœ¨[0,1]èŒƒå›´å†…
            score_a = max(0.0, min(1.0, score_a))
            score_b = max(0.0, min(1.0, score_b))
            
            if score_a > score_b:
                win_type = "A soft wins"
            elif score_b > score_a:
                win_type = "B soft wins"
            else:
                win_type = "Soft tie"
        
        # è®°å½•åˆ°judgmentä¸­ç”¨äºæ—¥å¿—æ˜¾ç¤º
        passage_judgment["win_type"] = win_type
        passage_judgment["score_a"] = score_a
        passage_judgment["score_b"] = score_b
        passage_judgment["prob_diff"] = prob_diff
        
        return score_a, score_b
    
    def _summarize_pairwise_result_with_soft_win(self, question_results: List[Dict], name_a: str, name_b: str) -> Dict[str, Any]:
        """
        æ±‡æ€»æˆå¯¹æ¯”è¾ƒç»“æœ - ä½¿ç”¨æ–°çš„soft winç´¯è®¡è¯„åˆ†æœºåˆ¶
        
        Args:
            question_results: é—®é¢˜åˆ¤å†³ç»“æœåˆ—è¡¨
            name_a: ç³»ç»ŸAåç§°
            name_b: ç³»ç»ŸBåç§°
            
        Returns:
            æ±‡æ€»ç»“æœï¼ŒåŒ…å«ç´¯è®¡å¾—åˆ†å’ŒEloæ›´æ–°
        """
        if not question_results:
            return {
                "total_score_a": 0.0,
                "total_score_b": 0.0,
                "elo_delta": 0.0,
                "winner": "Tie",
                "confidence": 0.0,
                "question_details": []
            }
        
        # ç´¯è®¡æ‰€æœ‰é—®é¢˜çš„å¾—åˆ†
        total_score_a = sum(result["score_a"] for result in question_results)
        total_score_b = sum(result["score_b"] for result in question_results)
        total_questions = len(question_results)
        
        # è®¡ç®—å¹³å‡å¾—åˆ†ç‡
        avg_score_a = total_score_a / total_questions
        avg_score_b = total_score_b / total_questions
        
        # åŸºäºç´¯è®¡å¾—åˆ†å·®è·è®¡ç®—Eloæ›´æ–°
        score_diff = total_score_a - total_score_b
        
        # å°†å¾—åˆ†å·®è·è½¬æ¢ä¸ºèƒœç‡ç”¨äºEloè®¡ç®—
        # å¾—åˆ†èŒƒå›´ï¼š[-total_questions, +total_questions]
        # è½¬æ¢ä¸ºèƒœç‡èŒƒå›´ï¼š[0, 1]
        max_diff = total_questions
        normalized_diff = score_diff / max_diff  # [-1, 1]
        
        # ä½¿ç”¨sigmoidå‡½æ•°å°†å·®è·è½¬æ¢ä¸ºèƒœç‡
        # è¿™æ ·å¯ä»¥å¹³æ»‘å¤„ç†å„ç§å¾—åˆ†å·®è·
        import math
        win_rate_a = 1 / (1 + math.exp(-5 * normalized_diff))  # 5æ˜¯è°ƒèŠ‚å‚æ•°ï¼Œæ§åˆ¶è½¬æ¢çš„é™¡å³­ç¨‹åº¦
        
        # è®¡ç®—Eloæ›´æ–° - ä½¿ç”¨æ ‡å‡†Eloå…¬å¼
        k_factor = self.config.k_factor
        elo_delta = k_factor * (win_rate_a - 0.5)
        
        # ç¡®å®šèƒœè€…
        if abs(score_diff) < 0.1:  # éå¸¸æ¥è¿‘
            winner = "Tie"
            confidence = 0.5 + abs(score_diff) / (2 * max_diff)
        elif score_diff > 0:
            winner = f"{name_a} wins"
            confidence = win_rate_a
        else:
            winner = f"{name_b} wins"
            confidence = 1 - win_rate_a
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„åˆ¤å†³
        hard_wins_a = sum(1 for r in question_results if r["passage_judgment"].get("win_type", "").startswith("A hard"))
        hard_wins_b = sum(1 for r in question_results if r["passage_judgment"].get("win_type", "").startswith("B hard"))
        soft_wins_a = sum(1 for r in question_results if r["passage_judgment"].get("win_type", "").startswith("A soft"))
        soft_wins_b = sum(1 for r in question_results if r["passage_judgment"].get("win_type", "").startswith("B soft"))
        ties = sum(1 for r in question_results if "tie" in r["passage_judgment"].get("win_type", "").lower())
        
        self.logger.info(f"ğŸ† ç´¯è®¡è¯„åˆ†ç»“æœ:")
        self.logger.info(f"  ğŸ“Š æ€»åˆ†: {name_a}={total_score_a:.2f}, {name_b}={total_score_b:.2f} (å…±{total_questions}é¢˜)")
        self.logger.info(f"  ğŸ“ˆ å¹³å‡å¾—åˆ†ç‡: {name_a}={avg_score_a:.3f}, {name_b}={avg_score_b:.3f}")
        self.logger.info(f"  ğŸ¯ åˆ¤å†³ç»Ÿè®¡: Aç¡¬èƒœ{hard_wins_a}, Aè½¯èƒœ{soft_wins_a}, Bç¡¬èƒœ{hard_wins_b}, Bè½¯èƒœ{soft_wins_b}, å¹³å±€{ties}")
        self.logger.info(f"  âš–ï¸ Eloæ›´æ–°: {elo_delta:.1f} ({winner}, ç½®ä¿¡åº¦{confidence:.3f})")
        
        return {
            "total_score_a": total_score_a,
            "total_score_b": total_score_b,
            "avg_score_a": avg_score_a,
            "avg_score_b": avg_score_b,
            "score_diff": score_diff,
            "win_rate_a": win_rate_a,
            "elo_delta": elo_delta,
            "winner": winner,
            "confidence": confidence,
            "total_questions": total_questions,
            "hard_wins_a": hard_wins_a,
            "hard_wins_b": hard_wins_b,
            "soft_wins_a": soft_wins_a,
            "soft_wins_b": soft_wins_b,
            "ties": ties,
            "question_details": question_results
        }
    
    # æ—©åœæ–¹æ³•å·²ç§»é™¤ - æŒ‰ç”¨æˆ·è¦æ±‚å»é™¤æ‰€æœ‰æ”¶æ•›/æ—©åœæœºåˆ¶
    
    def _update_elo_scores(self, elo_scores: Dict[str, float], 
                         comparison: Dict[str, Any], sys_a: str, sys_b: str):
        """æ›´æ–°Eloåˆ†æ•°"""
        summary = comparison["summary"]
        win_rate_a = summary["win_rate_a"]
        win_rate_b = summary["win_rate_b"]
        
        # è®¡ç®—æœŸæœ›èƒœç‡
        expected_a = 1 / (1 + 10 ** ((elo_scores[sys_b] - elo_scores[sys_a]) / 400))
        expected_b = 1 - expected_a
        
        # æ›´æ–°Elo
        k = self.config.k_factor
        elo_scores[sys_a] += k * (win_rate_a - expected_a)
        elo_scores[sys_b] += k * (win_rate_b - expected_b)
    
    def _summarize_pairwise_result(self, question_results: List[Dict], 
                                 name_a: str, name_b: str) -> Dict[str, Any]:
        """æ±‡æ€»æˆå¯¹æ¯”è¾ƒç»“æœ"""
        total_questions = len(question_results)
        a_wins = sum(1 for r in question_results if r["winner"] == "A wins")
        b_wins = sum(1 for r in question_results if r["winner"] == "B wins")
        ties = total_questions - a_wins - b_wins
        
        return {
            "total_questions": total_questions,
            "a_wins": a_wins,
            "b_wins": b_wins,
            "ties": ties,
            "win_rate_a": a_wins / total_questions if total_questions > 0 else 0,
            "win_rate_b": b_wins / total_questions if total_questions > 0 else 0,
            "tie_rate": ties / total_questions if total_questions > 0 else 0,
            "avg_elo_delta": np.mean([r["elo_delta"] for r in question_results]) if question_results else 0
        }
    
    def _create_baseline_data(self, target_data: List[Dict], baseline_name: str) -> Tuple[List[Dict], int]:
        """åˆ›å»ºåŸºçº¿å¯¹æ¯”æ•°æ® - ä½¿ç”¨LLMç”ŸæˆçœŸå®çš„QACGå¯¹"""
        self.logger.info(f"ç”Ÿæˆ {baseline_name} åŸºçº¿çš„çœŸå®QACGæ•°æ®...")
        baseline_data = []
        baseline_prompt = self.baseline_prompts[baseline_name]
        llm_calls = 0
        
        for i, qa in enumerate(target_data):
            question = qa["question"]
            groundtruth = qa.get("groundtruth", qa.get("expected_answer", ""))
            
            self.logger.info(f"  ç”Ÿæˆç¬¬ {i+1}/{len(target_data)} ä¸ª{baseline_name}åŸºçº¿å›ç­”")
            
            # ç”ŸæˆåŸºçº¿å›ç­”
            generated_answer = self._generate_baseline_answer(question, groundtruth, baseline_prompt)
            llm_calls += 1
            
            # ç”ŸæˆåŸºçº¿æ£€ç´¢è¯æ®
            generated_context = self._generate_baseline_context(question, groundtruth, baseline_prompt)
            llm_calls += 1
            
            baseline_qa = {
                "question": question,
                "rag_answer": generated_answer,
                "context": generated_context,
                "groundtruth": groundtruth,
                "metadata": {
                    "system_type": "baseline",
                    "baseline_quality": baseline_name.lower(),
                    "generated_by": "llm_baseline_generator"
                }
            }
            baseline_data.append(baseline_qa)
        
        return baseline_data, llm_calls
    
    def _generate_baseline_answer(self, question: str, groundtruth: str, baseline_prompt: Dict) -> str:
        """ä½¿ç”¨LLMç”ŸæˆåŸºçº¿å›ç­”"""
        prompt = f"""
{baseline_prompt["instruction"]}

é—®é¢˜: {question}
å‚è€ƒæ ‡å‡†ç­”æ¡ˆ: {groundtruth}

è¯·åŸºäºä¸Šè¿°è¦æ±‚ç”Ÿæˆä¸€ä¸ª{baseline_prompt["quality_level"]}è´¨é‡çš„å›ç­”:
"""
        
        try:
            response = self.pairwise_judge._call_llm(prompt)
            return response.strip()
        except Exception as e:
            self.logger.error(f"ç”ŸæˆåŸºçº¿å›ç­”å¤±è´¥: {e}")
            # é™çº§åˆ°é»˜è®¤å›ç­”
            fallback_answers = {
                "high": f"åŸºäºç›¸å…³èµ„æ–™ï¼Œ{groundtruth}",
                "medium": f"æ ¹æ®ä¿¡æ¯æ˜¾ç¤ºï¼Œ{groundtruth[:len(groundtruth)//2]}...",
                "low": "ä¿¡æ¯ä¸å¤Ÿæ˜ç¡®ï¼Œå¯èƒ½éœ€è¦æ›´å¤šèµ„æ–™ã€‚"
            }
            return fallback_answers.get(baseline_prompt["quality_level"], "æ— æ³•ç”Ÿæˆå›ç­”")
    
    def _generate_baseline_context(self, question: str, groundtruth: str, baseline_prompt: Dict) -> List[str]:
        """ä½¿ç”¨LLMç”ŸæˆåŸºçº¿æ£€ç´¢è¯æ®"""
        prompt = f"""
{baseline_prompt["context_instruction"]}

é—®é¢˜: {question}
å‚è€ƒä¿¡æ¯: {groundtruth}

è¯·ç”Ÿæˆ3æ¡ç¬¦åˆ{baseline_prompt["quality_level"]}è´¨é‡è¦æ±‚çš„æ£€ç´¢è¯æ®ï¼Œæ¯æ¡è¯æ®åº”è¯¥ç‹¬ç«‹æˆæ®µï¼š

è¯æ®1ï¼š
è¯æ®2ï¼š
è¯æ®3ï¼š
"""
        
        try:
            response = self.pairwise_judge._call_llm(prompt)
            # è§£æå“åº”ï¼Œæå–3æ¡è¯æ®
            lines = response.strip().split('\n')
            contexts = []
            current_context = ""
            
            for line in lines:
                line = line.strip()
                if line.startswith("è¯æ®") and "ï¼š" in line:
                    if current_context:
                        contexts.append(current_context.strip())
                    current_context = line.split("ï¼š", 1)[1]
                elif line and not line.startswith("è¯æ®"):
                    current_context += " " + line
            
            if current_context:
                contexts.append(current_context.strip())
            
            # ç¡®ä¿æœ‰3æ¡è¯æ®
            while len(contexts) < 3:
                fallback_contexts = {
                    "high": f"è¿™æ˜¯åŸºäºæƒå¨èµ„æ–™çš„é«˜è´¨é‡è¯æ®ï¼Œè¯¦ç»†è¯´æ˜äº†{question}çš„ç›¸å…³ä¿¡æ¯ã€‚",
                    "medium": f"è¿™æ˜¯å…³äº{question}çš„åŸºæœ¬ä¿¡æ¯ï¼Œæä¾›äº†éƒ¨åˆ†ç›¸å…³å†…å®¹ã€‚",
                    "low": f"è¿™æ˜¯ä¸{question}ç›¸å…³çš„ä¸€èˆ¬æ€§ä¿¡æ¯ï¼Œå¯èƒ½ä¸å¤Ÿå‡†ç¡®ã€‚"
                }
                contexts.append(fallback_contexts.get(baseline_prompt["quality_level"], "ç›¸å…³ä¿¡æ¯ä¸è¶³"))
            
            return contexts[:3]
            
        except Exception as e:
            self.logger.error(f"ç”ŸæˆåŸºçº¿è¯æ®å¤±è´¥: {e}")
            # é™çº§åˆ°é»˜è®¤è¯æ®
            fallback_contexts = {
                "high": [
                    f"æƒå¨èµ„æ–™æ˜¾ç¤ºï¼Œ{groundtruth[:50]}...",
                    f"è¯¦ç»†åˆ†æè¡¨æ˜ï¼Œ{question}æ¶‰åŠå¤šä¸ªæ–¹é¢çš„è€ƒé‡ã€‚",
                    "åŸºäºå¯é æ¥æºçš„ä¿¡æ¯ï¼Œä»¥ä¸Šå†…å®¹å…·æœ‰è¾ƒé«˜å‡†ç¡®æ€§ã€‚"
                ],
                "medium": [
                    f"ç›¸å…³ä¿¡æ¯è¡¨æ˜ï¼Œ{groundtruth[:30]}...",
                    f"å…³äº{question}çš„åŸºæœ¬ä¿¡æ¯å¦‚ä¸Šæ‰€è¿°ã€‚",
                    "è¿™äº›ä¿¡æ¯åŸºæœ¬å‡†ç¡®ä½†å¯èƒ½ä¸å¤Ÿå®Œæ•´ã€‚"
                ],
                "low": [
                    f"æ®äº†è§£ï¼Œ{groundtruth[:20]}...",
                    f"å…³äº{question}çš„ä¿¡æ¯å¯èƒ½ä¸å¤Ÿå‡†ç¡®ã€‚",
                    "éœ€è¦è¿›ä¸€æ­¥éªŒè¯ç›¸å…³å†…å®¹çš„å‡†ç¡®æ€§ã€‚"
                ]
            }
            return fallback_contexts.get(baseline_prompt["quality_level"], ["ä¿¡æ¯ä¸è¶³"])
    
    def _summarize_baseline_comparison(self, baseline_results: Dict[str, Any], 
                                     target_system: str) -> Dict[str, Any]:
        """æ±‡æ€»åŸºçº¿å¯¹æ¯”ç»“æœ"""
        summary = {
            "target_system": target_system,
            "comparisons": {}
        }
        
        for baseline_name, result in baseline_results.items():
            win_rate = result["summary"]["win_rate_a"]  # targetç³»ç»Ÿçš„èƒœç‡
            total_questions = result["summary"]["total_questions"]
            
            # ç»Ÿè®¡æ˜¾è‘—æ€§ç®€åŒ–åˆ¤æ–­
            if win_rate > 0.6:
                conclusion = f"æ˜¾è‘—ä¼˜äº{baseline_name}åŸºçº¿"
            elif win_rate < 0.4:
                conclusion = f"æ˜¾è‘—åŠ£äº{baseline_name}åŸºçº¿"
            else:
                conclusion = f"ä¸{baseline_name}åŸºçº¿ç›¸å½“"
            
            summary["comparisons"][baseline_name] = {
                "win_rate": win_rate,
                "total_questions": total_questions,
                "conclusion": conclusion
            }
        
        return summary
    
    def _generate_detailed_qacg_comparisons(self, target_data: List[Dict], target_system: str, baseline_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†çš„QACGå¯¹æ¯”æ•°æ® - é‡ç”¨å·²ç”Ÿæˆçš„åŸºçº¿æ•°æ®"""
        self.logger.info("æ•´ç†è¯¦ç»†QACGå¯¹æ¯”æ•°æ®...")
        
        detailed_comparisons = {
            "target_system": target_system,
            "total_questions": len(target_data),
            "qacg_pairs": []
        }
        
        # é™åˆ¶è¾“å‡ºæ•°é‡ä»¥é¿å…æ–‡ä»¶è¿‡å¤§
        sample_size = min(len(target_data), self.config.max_questions)
        
        # ä»baseline_resultsä¸­æå–å·²ç”Ÿæˆçš„åŸºçº¿æ•°æ®
        baseline_data_by_name = {}
        for baseline_name, result in baseline_results.items():
            baseline_data_by_name[baseline_name] = result.get("baseline_data", [])
        
        for i, target_qa in enumerate(target_data[:sample_size]):
            question = target_qa["question"]
            
            # æ„å»ºå¯¹æ¯”å¯¹
            qacg_pair = {
                "question_id": i + 1,
                "question": question,
                "groundtruth": target_qa.get("groundtruth", target_qa.get("expected_answer", "")),
                
                # ç›®æ ‡ç³»ç»Ÿçš„QACG
                "target_system": {
                    "name": target_system,
                    "answer": target_qa.get("rag_answer", ""),
                    "context": target_qa.get("context", []),
                    "metadata": target_qa.get("metadata", {})
                },
                
                # å„ä¸ªåŸºçº¿çš„QACG
                "baselines": {}
            }
            
            # ä½¿ç”¨å·²ç”Ÿæˆçš„åŸºçº¿æ•°æ®
            for baseline_name in self.baseline_prompts.keys():
                if baseline_name in baseline_data_by_name and i < len(baseline_data_by_name[baseline_name]):
                    baseline_qa = baseline_data_by_name[baseline_name][i]
                    baseline_qacg = {
                        "name": f"Baseline_{baseline_name}",
                        "answer": baseline_qa.get("rag_answer", ""),
                        "context": baseline_qa.get("context", []),
                        "quality_level": baseline_name.lower(),
                        "description": self._get_baseline_description(baseline_name),
                        "generation_instruction": self.baseline_prompts[baseline_name]["instruction"],
                        "metadata": baseline_qa.get("metadata", {})
                    }
                else:
                    # å¤‡ç”¨åŸºçº¿æ•°æ®ï¼ˆå¦‚æœå‡ºç°æ•°æ®ä¸åŒ¹é…ï¼‰
                    baseline_qacg = {
                        "name": f"Baseline_{baseline_name}",
                        "answer": f"æœªèƒ½ç”Ÿæˆ{baseline_name}è´¨é‡çš„åŸºçº¿å›ç­”",
                        "context": [f"æœªèƒ½ç”Ÿæˆ{baseline_name}è´¨é‡çš„åŸºçº¿è¯æ®"],
                        "quality_level": baseline_name.lower(),
                        "description": self._get_baseline_description(baseline_name),
                        "generation_instruction": self.baseline_prompts[baseline_name]["instruction"],
                        "metadata": {"error": "baseline_generation_failed"}
                    }
                
                qacg_pair["baselines"][baseline_name] = baseline_qacg
            
            detailed_comparisons["qacg_pairs"].append(qacg_pair)
        
        return detailed_comparisons
    
    def _get_baseline_description(self, baseline_name: str) -> str:
        """è·å–åŸºçº¿æè¿°"""
        descriptions = {
            "Good": "é«˜è´¨é‡åŸºçº¿ï¼šæä¾›è¯¦ç»†å‡†ç¡®çš„å›ç­”ï¼ŒåŒ…å«å®Œæ•´å…³é”®ä¿¡æ¯ï¼Œé€»è¾‘æ¸…æ™°",
            "Medium": "ä¸­ç­‰è´¨é‡åŸºçº¿ï¼šæä¾›åŸºæœ¬æ­£ç¡®ä½†ä¸å¤Ÿè¯¦ç»†çš„å›ç­”ï¼Œå­˜åœ¨ä¿¡æ¯ç¼ºå¤±", 
            "Bad": "ä½è´¨é‡åŸºçº¿ï¼šå›ç­”ä¸å¤Ÿå‡†ç¡®ï¼Œå­˜åœ¨æ˜æ˜¾é”™è¯¯æˆ–é—æ¼"
        }
        return descriptions.get(baseline_name, "æœªçŸ¥åŸºçº¿")
    
    def _analyze_failures(self, pairwise_results: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå¤±è´¥åŸå› ï¼ˆè¯äº‘æ•°æ®ï¼‰"""
        failure_reasons = []
        
        for result in pairwise_results:
            for qr in result["question_results"]:
                passage_judgment = qr.get("passage_judgment", {})
                reason = passage_judgment.get("reason", "")
                if reason:
                    failure_reasons.append(reason)
        
        # ç®€åŒ–çš„è¯é¢‘ç»Ÿè®¡
        reason_counts = defaultdict(int)
        for reason in failure_reasons:
            # ç®€å•çš„å…³é”®è¯æå–
            keywords = ["å‡†ç¡®", "å®Œæ•´", "ç›¸å…³", "è¯æ®", "é€»è¾‘", "é”™è¯¯", "ç¼ºå¤±", "æ¨¡ç³Š"]
            for keyword in keywords:
                if keyword in reason:
                    reason_counts[keyword] += 1
        
        return {
            "total_reasons": len(failure_reasons),
            "keyword_counts": dict(reason_counts),
            "top_reasons": sorted(reason_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        }
    
    def _config_to_dict(self) -> Dict[str, Any]:
        """é…ç½®è½¬å­—å…¸"""
        return {
            "llm_model": self.config.llm_model,
            "max_questions": self.config.max_questions,
            "early_stop_elo_diff": self.config.early_stop_elo_diff,
            "early_stop_ci_threshold": self.config.early_stop_ci_threshold,
            "initial_elo": self.config.initial_elo,
            "k_factor": self.config.k_factor
        }
    
    def _save_tournament_result(self, result: Dict[str, Any]):
        """ä¿å­˜é”¦æ ‡èµ›ç»“æœ"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(output_dir / "tournament_result.json", 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜ç®€è¦æŠ¥å‘Š
        self._save_tournament_report(result, output_dir)
        
        self.logger.info(f"ğŸ† é”¦æ ‡èµ›ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _save_baseline_result(self, result: Dict[str, Any]):
        """ä¿å­˜åŸºçº¿å¯¹æ¯”ç»“æœ"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(output_dir / "baseline_comparison.json", 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜è¯¦ç»†çš„QACGå¯¹æ¯”æ•°æ®åˆ°å•ç‹¬æ–‡ä»¶
        if "detailed_qacg_comparisons" in result:
            with open(output_dir / "qacg_detailed_comparisons.json", 'w', encoding='utf-8') as f:
                json.dump(result["detailed_qacg_comparisons"], f, ensure_ascii=False, indent=2, default=str)
            self.logger.info(f"ğŸ“‹ è¯¦ç»†QACGå¯¹æ¯”æ•°æ®å·²ä¿å­˜åˆ°: {output_dir / 'qacg_detailed_comparisons.json'}")
        
        # ä¿å­˜ç®€è¦æŠ¥å‘Š
        self._save_baseline_report(result, output_dir)
        
        self.logger.info(f"ğŸ¯ åŸºçº¿å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _save_tournament_report(self, result: Dict[str, Any], output_dir: Path):
        """ä¿å­˜é”¦æ ‡èµ›æŠ¥å‘Šï¼ˆæ”¯æŒç‘å£«è½®å’ŒåŠ¨æ€Eloé…å¯¹ï¼‰"""
        tournament_type = result.get("tournament_type", "swiss_tournament")
        
        with open(output_dir / "tournament_report.md", 'w', encoding='utf-8') as f:
            if tournament_type == "swiss_tournament":
                f.write("# DICEç²¾ç®€ç‰ˆé”¦æ ‡èµ›æŠ¥å‘Š (ç‘å£«è½®ç³»ç»Ÿ)\n\n")
            elif tournament_type == "full_round_robin":
                f.write("# DICEç²¾ç®€ç‰ˆé”¦æ ‡èµ›æŠ¥å‘Š (å®Œæ•´å¾ªç¯èµ›)\n\n")
            else:
                f.write("# DICEç²¾ç®€ç‰ˆé”¦æ ‡èµ›æŠ¥å‘Š (åŠ¨æ€Eloé…å¯¹ç³»ç»Ÿ)\n\n")
            
            # æœ€ç»ˆæ’å
            f.write("## ğŸ† æœ€ç»ˆæ’å (åŠ¨æ€Elo)\n\n")
            final_ranking = result["final_ranking"]
            final_elo_scores = result["final_elo_scores"]
            
            for i, system in enumerate(final_ranking, 1):
                elo_score = final_elo_scores[system]
                # å‰3åæ ‡è®°
                medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else ""
                f.write(f"{i}. **{system}**: {elo_score:.1f} {medal}\n")
            
            # æ¯”èµ›è¿‡ç¨‹
            if tournament_type == "swiss_tournament":
                f.write("\n## ğŸ“Š ç‘å£«è½®æ¯”èµ›è¿‡ç¨‹\n\n")
                swiss_results = result["swiss_results"]
                match_records = swiss_results["match_records"]
                total_rounds = swiss_results.get("total_rounds", 4)
                
                f.write(f"æ€»æ¯”èµ›åœºæ¬¡: {len(match_records)}åœº ({total_rounds}è½®ï¼Œæ¯è½®4åœº)\n\n")
                
                # æŒ‰è½®æ¬¡æ˜¾ç¤ºæ¯”èµ›
                f.write("### è½®æ¬¡æ¯”èµ›å›é¡¾\n")
                current_round = 1
                for i, match in enumerate(match_records):
                    if match.get('round', 1) != current_round:
                        current_round = match.get('round', 1)
                        f.write(f"\n#### ç¬¬{current_round}è½®\n")
                    
                    f.write(f"**ç¬¬{match['match_num']}åœº**: {match['system_a']} (ELO: {match['old_elo_a']:.1f}) vs {match['system_b']} (ELO: {match['old_elo_b']:.1f})\n")
                    f.write(f"- èƒœè€…: {match['winner']}\n")
                    f.write(f"- Eloå˜åŒ–: {match['system_a']} ({match['old_elo_a']:.1f}â†’{match['new_elo_a']:.1f}), {match['system_b']} ({match['old_elo_b']:.1f}â†’{match['new_elo_b']:.1f})\n\n")
                
                # ç‘å£«è½®ç³»ç»Ÿè¯´æ˜
                f.write("## ğŸ¯ ç‘å£«è½®ç³»ç»Ÿè¯´æ˜\n\n")
                f.write("- **è½®æ¬¡é…å¯¹**: 4è½®æ¯”èµ›ï¼Œæ¯è½®4åœºï¼Œæ¯é˜Ÿæ¯è½®åªæ¯”ä¸€åœº\n")
                f.write("- **æ™ºèƒ½é…å¯¹**: æ¯è½®é€‰æ‹©Eloæœ€æ¥è¿‘çš„æœªå¯¹æˆ˜è¿‡çš„ä¸¤é˜Ÿ\n")
                f.write("- **åŠ¨æ€è°ƒæ•´**: å®æ—¶æ›´æ–°Eloåˆ†æ•°ï¼Œåæ˜ çœŸå®å®åŠ›å˜åŒ–\n")
                f.write("- **æ— ç§å­é˜Ÿ**: åˆå§‹Elo=1500ï¼Œå®Œå…¨åŸºäºæ¯”èµ›ç»“æœå­¦ä¹ \n")
                f.write("- **å…¬å¹³æ€§**: ç¡®ä¿æ¯å¯¹ç³»ç»Ÿåªå¯¹æˆ˜ä¸€æ¬¡\n\n")
            elif tournament_type == "full_round_robin":
                f.write("\n## ğŸ“Š å®Œæ•´å¾ªç¯èµ›æ¯”èµ›è¿‡ç¨‹\n\n")
                rr = result["round_robin_results"]
                match_records = rr.get("match_records", [])
                f.write(f"æ€»æ¯”èµ›åœºæ¬¡: {len(match_records)}åœºï¼ˆå…¨å¯¹å…¨ï¼Œæ¯å¯¹ç³»ç»Ÿä»…ä¸€æ¬¡å¯¹æˆ˜ï¼‰\n\n")
                
                # æŒ‰é¡ºåºæ˜¾ç¤ºæ¯”èµ›
                f.write("### æ¯”èµ›å›é¡¾\n")
                for match in match_records:
                    f.write(f"**ç¬¬{match['match_num']}åœº**: {match['system_a']} (ELO: {match['old_elo_a']:.1f}) vs {match['system_b']} (ELO: {match['old_elo_b']:.1f})\n")
                    f.write(f"- èƒœè€…: {match['winner']}\n")
                    f.write(f"- Eloå˜åŒ–: {match['system_a']} ({match['old_elo_a']:.1f}â†’{match['new_elo_a']:.1f}), {match['system_b']} ({match['old_elo_b']:.1f}â†’{match['new_elo_b']:.1f})\n\n")
                
                # å¾ªç¯èµ›è¯´æ˜
                f.write("## ğŸ¯ å®Œæ•´å¾ªç¯èµ›è¯´æ˜\n\n")
                f.write("- **é…å¯¹æ–¹å¼**: æ‰€æœ‰ç³»ç»Ÿä¸¤ä¸¤å¯¹æˆ˜ä¸€æ¬¡ï¼ˆå…±N(N-1)/2åœºï¼‰\n")
                f.write("- **è¯„åˆ†æ–¹å¼**: ä½¿ç”¨soft winç´¯è®¡è¯„åˆ†ä¸åŠ¨æ€Eloæ›´æ–°\n")
                f.write("- **å¯æ¯”æ€§**: è¦†ç›–å…¨éƒ¨é…å¯¹ï¼Œé¿å…æŠ½æ ·ä¸å®Œæ•´çš„åå·®\n\n")
            else:
                f.write("\n## ğŸ“Š åŠ¨æ€é…å¯¹è¿‡ç¨‹\n\n")
                # å®‰å…¨è·å–dynamic_results
                dynamic_results = result.get("dynamic_results")
                if dynamic_results:
                    match_records = dynamic_results.get("match_records", [])
            f.write(f"æ€»æ¯”èµ›åœºæ¬¡: {len(match_records)}åœº\n\n")
            
            # æ˜¾ç¤ºå…³é”®æ¯”èµ›
            f.write("### å…³é”®æ¯”èµ›å›é¡¾\n")
            for i, match in enumerate(match_records):  # æ˜¾ç¤ºå‰10åœºå…³é”®æ¯”èµ›
                f.write(f"**ç¬¬{match['match_num']}åœº**: {match['system_a']} (ELO: {match['old_elo_a']:.1f}) vs {match['system_b']} (ELO: {match['old_elo_b']:.1f})\n")
                f.write(f"- èƒœè€…: {match['winner']}\n")
                f.write(f"- Eloå˜åŒ–: {match['system_a']} ({match['old_elo_a']:.1f}â†’{match['new_elo_a']:.1f}), {match['system_b']} ({match['old_elo_b']:.1f}â†’{match['new_elo_b']:.1f})\n\n")
            else:
                f.write("æ€»æ¯”èµ›åœºæ¬¡: æœªçŸ¥\n\n")
                f.write("### å…³é”®æ¯”èµ›å›é¡¾\n")
                f.write("æ¯”èµ›è®°å½•ä¸å¯ç”¨\n\n")
            
            # åŠ¨æ€Eloç³»ç»Ÿè¯´æ˜
            f.write("## ğŸ¯ åŠ¨æ€Eloé…å¯¹ç³»ç»Ÿè¯´æ˜\n\n")
            f.write("- **æ™ºèƒ½é…å¯¹**: æ¯è½®é€‰æ‹©Eloæœ€æ¥è¿‘çš„æœªå¯¹æˆ˜è¿‡çš„ä¸¤é˜Ÿ\n")
            f.write("- **åŠ¨æ€è°ƒæ•´**: å®æ—¶æ›´æ–°Eloåˆ†æ•°ï¼Œåæ˜ çœŸå®å®åŠ›å˜åŒ–\n")
            f.write("- **é«˜æ•ˆæ€§**: æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šï¼Œå‡å°‘å†—ä½™æ¯”èµ›\n")
            f.write("- **æ— ç§å­é˜Ÿ**: åˆå§‹Elo=1500ï¼Œå®Œå…¨åŸºäºæ¯”èµ›ç»“æœå­¦ä¹ \n")
            f.write("- **æ”¶æ•›åˆ¤æ–­**: å½“æ’åç¨³å®šæˆ–è¾¾åˆ°æœ€å¤§åœºæ¬¡æ—¶ç»“æŸ\n\n")
            
            # å¤±è´¥åˆ†æ - ä½¿ç”¨åŠ¨æ€èšç±»ç»“æœ
            f.write("## ğŸ“Š åŠ¨æ€å¤±è´¥æ¨¡å¼èšç±»åˆ†æ\n\n")
            failure_clusters = result["failure_analysis"]
            for cluster_id, cluster_data in failure_clusters.items():
                f.write(f"### {cluster_data['label']}\n")
                f.write(f"- ç›¸å…³ç³»ç»Ÿ: {', '.join(cluster_data['systems'][:5])}{'...' if len(cluster_data['systems']) > 5 else ''}\n")
                f.write(f"- å¤±è´¥æ¡ˆä¾‹æ•°: {cluster_data['size']}\n")
                
                # æ˜¾ç¤ºåŠ¨æ€æå–çš„å…³é”®è¯
                top_keywords = cluster_data.get('top_keywords', [])
                if top_keywords:
                    keyword_str = ', '.join([f'{k}({v}æ¬¡)' for k, v in top_keywords[:3]])
                    f.write(f"- å…³é”®è¯: {keyword_str}\n")
                f.write("\n")
            
            # è°ƒç”¨é‡ç»Ÿè®¡
            total_calls = result["total_llm_calls"]
            total_matches = len(match_records)
            f.write(f"## ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡\n\n")
            f.write(f"- æ€»æ¯”èµ›åœºæ¬¡: {total_matches}åœº (vs ä¼ ç»Ÿè”èµ›28åœºï¼Œå‡å°‘{(28-total_matches)/28*100:.1f}%)\n")
            f.write(f"- æ€»LLMè°ƒç”¨æ¬¡æ•°: {total_calls}\n")
            f.write(f"- ä¼°è®¡ç”¨æ—¶: ~{total_calls/40:.1f}åˆ†é’Ÿ (8Ã—A100)\n")
            f.write(f"- æ¯é˜Ÿå¹³å‡å¯¹æˆ˜: {total_matches*2/8:.1f}åœº\n")

            # CIåˆ†æ
            ci_analysis = result.get("ci_analysis", {})
            if ci_analysis:
                f.write(f"\n## ğŸ“Š 95% ç½®ä¿¡åŒºé—´åˆ†æ\n\n")
                f.write(f"- å¹³å‡å¾—åˆ†å·®å€¼: {ci_analysis.get('mean_score_diff', 0):.2f}\n")
                f.write(f"- 95% CI: {ci_analysis.get('ci_95', 'N/A')}\n")
                f.write(f"- ç»Ÿè®¡æ˜¾è‘—æ€§: {ci_analysis.get('significance', 'N/A')}\n")
    
    def _save_baseline_report(self, result: Dict[str, Any], output_dir: Path):
        """ä¿å­˜åŸºçº¿å¯¹æ¯”æŠ¥å‘Š"""
        with open(output_dir / "baseline_report.md", 'w', encoding='utf-8') as f:
            f.write("# DICEç²¾ç®€ç‰ˆåŸºçº¿å¯¹æ¯”æŠ¥å‘Š\n\n")
            
            target_system = result["target_system"]
            f.write(f"## ğŸ¯ ç›®æ ‡ç³»ç»Ÿ: {target_system}\n\n")
            
            # å¯¹æ¯”ç»“æœ
            f.write("## ğŸ“Š åŸºçº¿å¯¹æ¯”ç»“æœ\n\n")
            summary = result["summary"]
            
            for baseline_name, comparison in summary["comparisons"].items():
                win_rate = comparison["win_rate"]
                conclusion = comparison["conclusion"]
                f.write(f"### vs {baseline_name} åŸºçº¿\n")
                f.write(f"- èƒœç‡: {win_rate:.1%}\n")
                f.write(f"- ç»“è®º: {conclusion}\n\n")
            
            # æ€§èƒ½ç»Ÿè®¡
            total_calls = result["total_llm_calls"]
            f.write(f"## ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡\n\n")
            f.write(f"- æ€»LLMè°ƒç”¨æ¬¡æ•°: {total_calls}\n")
            f.write(f"- ä¼°è®¡ç”¨æ—¶: ~{total_calls/40:.1f}åˆ†é’Ÿ\n")


def create_simplified_evaluator(config: SimplifiedDICEConfig = None) -> SimplifiedDICEEvaluator:
    """åˆ›å»ºç²¾ç®€ç‰ˆDICEè¯„ä¼°å™¨"""
    return SimplifiedDICEEvaluator(config) 