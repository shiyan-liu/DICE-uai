"""
åŸºäºLlamaIndexçš„RAGç³»ç»Ÿå®ç°
æ”¯æŒå¤šç§embeddingæ¨¡å‹ã€chunkingç­–ç•¥å’ŒLLMæ¨¡å‹
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

from llama_index.core import (
    VectorStoreIndex, 
    Document, 
    ServiceContext,
    SimpleDirectoryReader,
    Settings
)
from llama_index.core.node_parser import (
    SimpleNodeParser,
    SentenceSplitter,
    SemanticSplitterNodeParser
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.llms.ollama import Ollama
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.storage.storage_context import StorageContext
import chromadb

from .base_rag import BaseRAGSystem, RAGConfig, RetrievalResult, RAGResponse


class LlamaIndexRAGSystem(BaseRAGSystem):
    """åŸºäºLlamaIndexçš„RAGç³»ç»Ÿå®ç°"""
    
    def __init__(self, config: RAGConfig):
        super().__init__(config)
        self.index = None
        self.query_engine = None
        self.documents = []
        self.nodes = []
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self._setup_embedding_model()
        
        # åˆå§‹åŒ–LLMæ¨¡å‹
        self._setup_llm_model()
        
        # åˆå§‹åŒ–node parser
        self._setup_node_parser()
        
        # è®¾ç½®å…¨å±€é…ç½®
        Settings.embed_model = self.embed_model
        Settings.llm = self.llm_model
        Settings.node_parser = self.node_parser
        
    def _setup_embedding_model(self):
        """è®¾ç½®embeddingæ¨¡å‹ï¼ˆæ”¯æŒGPUåŠ é€Ÿï¼‰"""
        import torch
        import os
        
        # è®¾ç½®é•œåƒç¯å¢ƒå˜é‡
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        if self.config.embedding_model == "bge-large-zh":
            self.embed_model = HuggingFaceEmbedding(
                model_name="BAAI/bge-large-zh-v1.5",
                cache_folder="./models",
                device=device,
                max_length=512,
                trust_remote_code=True,  # å…è®¸åŠ è½½è‡ªå®šä¹‰ä»£ç 
            )
        elif self.config.embedding_model == "bge-small-zh":
            self.embed_model = HuggingFaceEmbedding(
                model_name="BAAI/bge-small-zh-v1.5",
                cache_folder="./models",
                device=device,
                max_length=512,
                trust_remote_code=True,  # å…è®¸åŠ è½½è‡ªå®šä¹‰ä»£ç 
            )
        else:
            # é»˜è®¤ä½¿ç”¨å°æ¨¡å‹
            self.embed_model = HuggingFaceEmbedding(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                cache_folder="./models",
                device=device,
                max_length=384,
                trust_remote_code=True,  # å…è®¸åŠ è½½è‡ªå®šä¹‰ä»£ç 
            )
    
    def _setup_llm_model(self):
        """è®¾ç½®LLMæ¨¡å‹"""
        if self.config.llm_model == "qwen2.5":
            # ä½¿ç”¨Ollamaæœ¬åœ°éƒ¨ç½²çš„Qwen2.5
            self.llm_model = Ollama(
                model="qwen2.5:7b",
                request_timeout=120.0,
                temperature=self.config.temperature
            )
        elif self.config.llm_model == "qwen2.5-mini":
            # ä½¿ç”¨Ollamaæœ¬åœ°éƒ¨ç½²çš„Qwen2.5-0.5B (è¶…å°æ¨¡å‹)
            self.llm_model = Ollama(
                model="qwen2.5:0.5b",
                request_timeout=120.0,
                temperature=self.config.temperature
            )
        elif self.config.llm_model.startswith("openai"):
            # å¦‚æœéœ€è¦ä½¿ç”¨OpenAIæ¨¡å‹
            from llama_index.llms.openai import OpenAI
            self.llm_model = OpenAI(
                model=self.config.llm_model.replace("openai-", ""),
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
        else:
            # é»˜è®¤ä½¿ç”¨Qwen2.5
            self.llm_model = Ollama(
                model="qwen2.5:7b",
                request_timeout=120.0,
                temperature=self.config.temperature
            )
    
    def _setup_node_parser(self):
        """è®¾ç½®æ–‡æ¡£åˆ†å—å™¨"""
        if self.config.chunking_strategy == "chunk_256":
            # 256å­—ç¬¦é•¿åº¦åˆ†å—
            self.node_parser = SentenceSplitter(
                chunk_size=256,
                chunk_overlap=self.config.chunk_overlap
            )
        elif self.config.chunking_strategy == "chunk_512":
            # 512å­—ç¬¦é•¿åº¦åˆ†å—  
            self.node_parser = SentenceSplitter(
                chunk_size=512,
                chunk_overlap=self.config.chunk_overlap
            )
        elif self.config.chunking_strategy == "sentence":
            # ä¿ç•™åŸæœ‰sentenceåˆ†å—é€‰é¡¹
            self.node_parser = SentenceSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap
            )
        elif self.config.chunking_strategy == "semantic":
            # ä¸ºä¸­æ–‡æ–‡æœ¬è®¾ç½®åˆé€‚çš„åˆ†è¯å™¨
            def chinese_sentence_splitter(text: str) -> List[str]:
                """ä¸­æ–‡åˆ†å¥å™¨ï¼Œç»“åˆjiebaåˆ†è¯å’Œæ ‡ç‚¹ç¬¦å·"""
                import jieba
                import re
                
                # ğŸ”§ ä¿®æ”¹ï¼šå‡å°‘å¥å·åˆ†å‰²ï¼Œåªä½¿ç”¨å¼ºåˆ†å¥ç¬¦å·
                # ç§»é™¤å¥å·ï¼Œåªä¿ç•™æ„Ÿå¹å·ã€é—®å·ã€åˆ†å·ç­‰å¼ºåˆ†å¥ç¬¦å·
                sentences = re.split(r'[ï¼ï¼Ÿï¼›\n]+', text)
                sentences = [s.strip() for s in sentences if s.strip()]
                
                # ğŸ”§ ä¿®æ”¹ï¼šå¯¹äºæé•¿çš„å¥å­ï¼ˆ>400å­—ç¬¦ï¼‰ï¼Œæ‰è¿›è¡Œè¿›ä¸€æ­¥åˆ†å‰²
                final_sentences = []
                for sentence in sentences:
                    if len(sentence) > 400:  # è¿›ä¸€æ­¥æé«˜é˜ˆå€¼åˆ°400
                        # ğŸ”§ ä¿®æ”¹ï¼šåªæŒ‰ç…§å¥å·åˆ†å‰²ï¼ˆä½œä¸ºäºŒçº§åˆ†å‰²ï¼‰
                        sub_parts = re.split(r'[ã€‚]', sentence)
                        sub_parts = [part.strip() for part in sub_parts if part.strip() and len(part) > 30]
                        if len(sub_parts) > 1:  # åªæœ‰çœŸæ­£åˆ†å‰²å‡ºå¤šä¸ªéƒ¨åˆ†æ‰ä½¿ç”¨
                            final_sentences.extend(sub_parts)
                        else:
                            final_sentences.append(sentence)
                    else:
                        final_sentences.append(sentence)
                
                return final_sentences
            
            self.node_parser = SemanticSplitterNodeParser(
                buffer_size=4,  # ğŸ”§ ä¿®æ”¹ï¼šä»3å¢åŠ åˆ°4ï¼Œæ›´å¤šä¸Šä¸‹æ–‡
                breakpoint_percentile_threshold=85,  # ğŸ”§ ä¿®æ”¹ï¼šä»80æå‡åˆ°85ï¼Œæ›´å°‘åˆ†å‰²ç‚¹
                embed_model=self.embed_model,
                sentence_splitter=chinese_sentence_splitter
            )
        else:
            # é»˜è®¤ä½¿ç”¨ç®€å•åˆ†å—
            self.node_parser = SimpleNodeParser.from_defaults(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap
            )
    
    def chunk_documents(self, documents: Dict[str, str]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LlamaIndexè¿›è¡Œæ–‡æ¡£åˆ†å—"""
        self.logger.info(f"ä½¿ç”¨LlamaIndexè¿›è¡Œæ–‡æ¡£åˆ†å—ï¼Œç­–ç•¥: {self.config.chunking_strategy}")
        
        # è½¬æ¢ä¸ºLlamaIndex Documentæ ¼å¼
        llama_documents = []
        for doc_id, content in documents.items():
            doc = Document(
                text=content,
                metadata={"doc_id": doc_id, "source": doc_id}
            )
            llama_documents.append(doc)
        
        self.documents = llama_documents
        
        # ä½¿ç”¨node parserè¿›è¡Œåˆ†å—
        self.nodes = self.node_parser.get_nodes_from_documents(llama_documents)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        chunks = []
        for i, node in enumerate(self.nodes):
            chunk = {
                "chunk_id": f"{node.metadata.get('doc_id', 'unknown')}_{i}",
                "content": node.text,
                "source_doc_id": node.metadata.get("doc_id", "unknown"),
                "metadata": dict(node.metadata)
            }
            chunks.append(chunk)
        
        self.logger.info(f"åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªchunks")
        return chunks
    
    def create_embeddings(self, chunks: List[Dict[str, Any]]) -> List[List[float]]:
        """
        LlamaIndexä¼šåœ¨build_vector_storeä¸­è‡ªåŠ¨å¤„ç†embedding
        
        æ³¨é‡Šè¯´æ˜ï¼š
        - LlamaIndexçš„VectorStoreIndex.from_documents()ä¼šè‡ªåŠ¨è°ƒç”¨embed_model
        - æ¯ä¸ªæ–‡æ¡£å—éƒ½ä¼šè¢«è½¬æ¢ä¸ºembeddingå‘é‡å¹¶å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­
        - GPUåŠ é€Ÿåœ¨_setup_embedding_modelä¸­é…ç½®ï¼Œè‡ªåŠ¨åº”ç”¨åˆ°è¿™ä¸ªè¿‡ç¨‹
        - è¿”å›ç©ºåˆ—è¡¨æ˜¯å› ä¸ºembeddingæ˜¯å†…éƒ¨å¤„ç†çš„ï¼Œå¤–éƒ¨ä¸éœ€è¦ç›´æ¥è®¿é—®
        """
        self.logger.info("ğŸ“Š LlamaIndexå°†åœ¨æ„å»ºç´¢å¼•æ—¶è‡ªåŠ¨å¤„ç†embeddingï¼ˆæ”¯æŒGPUåŠ é€Ÿï¼‰")
        self.logger.info(f"ğŸ¯ å°†ä½¿ç”¨ {self.config.embedding_model} æ¨¡å‹è¿›è¡Œå‘é‡åŒ–")
        return []
    
    def _get_cache_path(self) -> str:
        """è·å–å‘é‡å­˜å‚¨ç¼“å­˜è·¯å¾„"""
        config_hash = hash(f"{self.config.chunking_strategy}_{self.config.chunk_size}_{self.config.chunk_overlap}_{self.config.embedding_model}")
        return f"./chroma_db/{self.config.system_name}_{abs(config_hash) % 10000}"
    
    def _check_vector_store_cache(self) -> bool:
        """æ£€æŸ¥å‘é‡å­˜å‚¨ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ"""
        cache_path = self._get_cache_path()
        if not os.path.exists(cache_path):
            return False
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•°æ®
        try:
            chroma_client = chromadb.PersistentClient(path=cache_path)
            collections = chroma_client.list_collections()
            if not collections:
                return False
            
            # æ£€æŸ¥ç¬¬ä¸€ä¸ªé›†åˆæ˜¯å¦æœ‰æ•°æ®
            collection = collections[0]
            count = collection.count()
            self.logger.info(f"å‘ç°ç¼“å­˜çš„å‘é‡å­˜å‚¨ï¼ŒåŒ…å« {count} ä¸ªå‘é‡")
            return count > 0
            
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥å‘é‡å­˜å‚¨ç¼“å­˜æ—¶å‡ºé”™: {e}")
            return False
    
    def _load_vector_store_cache(self):
        """åŠ è½½ç¼“å­˜çš„å‘é‡å­˜å‚¨"""
        cache_path = self._get_cache_path()
        self.logger.info(f"åŠ è½½ç¼“å­˜çš„å‘é‡å­˜å‚¨: {cache_path}")
        
        try:
            chroma_client = chromadb.PersistentClient(path=cache_path)
            collections = chroma_client.list_collections()
            
            if collections:
                chroma_collection = collections[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé›†åˆ
                vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
                storage_context = StorageContext.from_defaults(vector_store=vector_store)
                
                # é‡æ–°åˆ›å»ºç´¢å¼•
                self.index = VectorStoreIndex.from_vector_store(
                    vector_store=vector_store,
                    storage_context=storage_context,
                    embed_model=self.embed_model
                )
                
                # åˆ›å»ºæŸ¥è¯¢å¼•æ“
                self.query_engine = self.index.as_query_engine(
                    similarity_top_k=self.config.retrieval_top_k,
                    response_mode="compact",
                    llm=self.llm_model
                )
                
                self.logger.info(f"æˆåŠŸåŠ è½½ç¼“å­˜çš„å‘é‡å­˜å‚¨")
                return True
                
        except Exception as e:
            self.logger.error(f"åŠ è½½å‘é‡å­˜å‚¨ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def _convert_cached_chunks_to_nodes(self, cached_chunks: List[Dict[str, Any]]):
        """
        å°†ç¼“å­˜çš„chunksæ•°æ®è½¬æ¢ä¸ºLlamaIndexçš„Documentå’ŒNodeæ ¼å¼
        
        Args:
            cached_chunks: ç¼“å­˜çš„chunksæ•°æ®
        """
        from llama_index.core import Document
        from llama_index.core.schema import TextNode
        
        self.logger.info(f"ğŸ“¦ è½¬æ¢ {len(cached_chunks)} ä¸ªç¼“å­˜chunksä¸ºLlamaIndexæ ¼å¼")
        
        # è½¬æ¢ä¸ºDocumentsï¼ˆæŒ‰source_doc_idåˆ†ç»„ï¼‰
        doc_contents = {}
        for chunk in cached_chunks:
            source_doc_id = chunk.get('source_doc_id', 'unknown')
            if source_doc_id not in doc_contents:
                doc_contents[source_doc_id] = []
            doc_contents[source_doc_id].append(chunk['content'])
        
        # åˆ›å»ºDocuments
        self.documents = []
        for doc_id, contents in doc_contents.items():
            doc_text = '\n'.join(contents)
            doc = Document(
                text=doc_text,
                metadata={"doc_id": doc_id, "source": doc_id}
            )
            self.documents.append(doc)
        
        # åˆ›å»ºNodes
        self.nodes = []
        for chunk in cached_chunks:
            node = TextNode(
                text=chunk['content'],
                metadata=chunk.get('metadata', {})
            )
            # ç¡®ä¿metadataä¸­æœ‰å¿…è¦çš„å­—æ®µ
            if 'doc_id' not in node.metadata:
                node.metadata['doc_id'] = chunk.get('source_doc_id', 'unknown')
            self.nodes.append(node)
        
        self.logger.info(f"âœ… è½¬æ¢å®Œæˆ: {len(self.documents)} ä¸ªæ–‡æ¡£, {len(self.nodes)} ä¸ªèŠ‚ç‚¹")
    
    def build_vector_store(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]]):
        """æ„å»ºå‘é‡å­˜å‚¨å’ŒæŸ¥è¯¢å¼•æ“ï¼ˆæ”¯æŒç¼“å­˜ï¼‰"""
        self.logger.info(f"ä¸ºç³»ç»Ÿ {self.config.system_name} æ„å»ºç‹¬ç«‹çš„å‘é‡å­˜å‚¨")
        self.logger.info(f"ä½¿ç”¨ç­–ç•¥ - Chunking: {self.config.chunking_strategy}, "
                        f"Embedding: {self.config.embedding_model}, "
                        f"LLM: {self.config.llm_model}")
        
        # æ£€æŸ¥ç¼“å­˜
        if self._check_vector_store_cache():
            if self._load_vector_store_cache():
                self.logger.info("âœ… æˆåŠŸä½¿ç”¨ç¼“å­˜çš„å‘é‡å­˜å‚¨")
                return
            else:
                self.logger.warning("âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥ï¼Œé‡æ–°æ„å»ºå‘é‡å­˜å‚¨")
        
        # å¦‚æœä¼ å…¥çš„æ˜¯ç¼“å­˜çš„chunksæ•°æ®ï¼Œéœ€è¦å…ˆè½¬æ¢ä¸ºLlamaIndexæ ¼å¼
        if chunks and len(chunks) > 0 and isinstance(chunks[0], dict) and 'content' in chunks[0]:
            self.logger.info(f"ğŸ”„ æ£€æµ‹åˆ°ç¼“å­˜çš„chunksæ•°æ®ï¼Œè½¬æ¢ä¸ºLlamaIndexæ ¼å¼")
            self._convert_cached_chunks_to_nodes(chunks)
        
        # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
        cache_path = self._get_cache_path()
        os.makedirs(cache_path, exist_ok=True)
        
        # æ¸…ç†æ—§çš„å‘é‡å­˜å‚¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        import shutil
        if os.path.exists(cache_path) and os.listdir(cache_path):
            self.logger.info(f"æ¸…ç†æ—§çš„å‘é‡å­˜å‚¨: {cache_path}")
            shutil.rmtree(cache_path)
            os.makedirs(cache_path, exist_ok=True)
        
        chroma_client = chromadb.PersistentClient(path=cache_path)
        collection_name = f"kb_{self.config.system_name}_{hash(str(self.config.__dict__)) % 10000}"
        
        # åˆ é™¤å·²å­˜åœ¨çš„åŒåé›†åˆ
        try:
            chroma_client.delete_collection(name=collection_name)
        except:
            pass
        
        chroma_collection = chroma_client.get_or_create_collection(
            name=collection_name,
            metadata={
                "system_name": self.config.system_name,
                "chunking_strategy": self.config.chunking_strategy,
                "embedding_model": self.config.embedding_model,
                "llm_model": self.config.llm_model,
                "chunk_size": self.config.chunk_size,
                "chunk_overlap": self.config.chunk_overlap
            }
        )
        
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        # è®°å½•å¤„ç†çš„æ–‡æ¡£æ•°é‡å’Œchunkingè¯¦æƒ…
        self.logger.info(f"å¤„ç† {len(self.documents)} ä¸ªåŸå§‹æ–‡æ¡£")
        self.logger.info(f"ç”Ÿæˆ {len(self.nodes)} ä¸ªæ–‡æ¡£å—")
        
        # æ‰“å°å‰å‡ ä¸ªå—çš„ä¿¡æ¯ç”¨äºéªŒè¯
        for i, node in enumerate(self.nodes[:3]):
            self.logger.info(f"å— {i+1} (é•¿åº¦: {len(node.text)}): {node.text[:100]}...")
        
        # åˆ›å»ºç´¢å¼•ï¼ˆGPUåŠ é€Ÿçš„embeddingä¼šåœ¨è¿™é‡Œè‡ªåŠ¨ä½¿ç”¨ï¼‰
        self.logger.info("ğŸš€ å¼€å§‹åˆ›å»ºå‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨GPUåŠ é€Ÿçš„embeddingï¼‰")
        self.index = VectorStoreIndex.from_documents(
            self.documents,
            storage_context=storage_context,
            embed_model=self.embed_model,
            node_parser=self.node_parser,
            show_progress=True
        )
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=self.config.retrieval_top_k,
            response_mode="compact",
            llm=self.llm_model
        )
        
        self.logger.info(f"âœ… ç³»ç»Ÿ {self.config.system_name} å‘é‡å­˜å‚¨æ„å»ºå®Œæˆå¹¶ç¼“å­˜")
        self.logger.info(f"ğŸ“ å­˜å‚¨è·¯å¾„: {cache_path}")
        self.logger.info(f"ğŸ·ï¸ é›†åˆåç§°: {collection_name}")
    
    def retrieve(self, query: str) -> List[RetrievalResult]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_vector_store")
        
        # ä½¿ç”¨ç´¢å¼•è¿›è¡Œæ£€ç´¢
        retriever = self.index.as_retriever(
            similarity_top_k=self.config.retrieval_top_k
        )
        
        retrieved_nodes = retriever.retrieve(query)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        results = []
        for i, node in enumerate(retrieved_nodes):
            result = RetrievalResult(
                content=node.text,
                score=node.score if hasattr(node, 'score') else 1.0,
                chunk_id=node.metadata.get("doc_id", f"chunk_{i}"),
                source_doc_id=node.metadata.get("doc_id", f"doc_{i}")
            )
            results.append(result)
        
        return results
    
    def generate_answer(self, query: str, retrieved_chunks: List[RetrievalResult]) -> str:
        """ä½¿ç”¨æ£€ç´¢åˆ°çš„chunksç”Ÿæˆç­”æ¡ˆ"""
        if not retrieved_chunks:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
        
        try:
            # å°†æ£€ç´¢åˆ°çš„chunksè½¬æ¢ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
            context_pieces = []
            for i, chunk in enumerate(retrieved_chunks):
                context_pieces.append(f"[æ–‡æ¡£{i+1}] {chunk.content}")
            
            context_str = "\n\n".join(context_pieces)
            
            # ç»Ÿä¸€çš„æ™ºèƒ½æç¤ºè¯ï¼Œæ—¢èƒ½å‘æŒ¥largeæ¨¡å‹ä¼˜åŠ¿ï¼Œåˆèƒ½çº¦æŸminiæ¨¡å‹
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

å¯ç”¨ä¿¡æ¯ï¼š
{context_str}

ç”¨æˆ·é—®é¢˜ï¼š{query}

å›ç­”è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºä¸Šè¿°ä¿¡æ¯å›ç­”ï¼Œä¸å¾—æ·»åŠ ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–ç¼–é€ å†…å®¹
2. ä»”ç»†åˆ†æé—®é¢˜ä¸æä¾›ä¿¡æ¯çš„å…³è”åº¦å’Œå®Œæ•´æ€§
3. å¦‚æœä¿¡æ¯å®Œå…¨ç›¸å…³ä¸”å……è¶³ï¼Œè¯·ç»™å‡ºå®Œæ•´ã€å‡†ç¡®çš„å›ç­”
4. å¦‚æœä¿¡æ¯éƒ¨åˆ†ç›¸å…³ä½†ä¸å®Œæ•´ï¼Œè¯·åŸºäºå·²çŸ¥ä¿¡æ¯å›ç­”ï¼Œå¹¶è¯´æ˜ä¿¡æ¯å±€é™æ€§
5. å¦‚æœä¿¡æ¯ä¸ç›¸å…³æˆ–ä¸¥é‡ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜"ä¿¡æ¯ä¸è¶³"
6. åˆ©ç”¨æ¨ç†èƒ½åŠ›ä»å¤šä¸ªæ–‡æ¡£ç‰‡æ®µä¸­ç»¼åˆä¿¡æ¯ï¼Œé¿å…é‡å¤
7. ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œé¿å…è¿‡åº¦æ¨æµ‹

æ¨ç†æ­¥éª¤ï¼š
- è¯†åˆ«é—®é¢˜ä¸­çš„å…³é”®æ¦‚å¿µå’Œå®ä½“
- åœ¨æä¾›çš„ä¿¡æ¯ä¸­å¯»æ‰¾ç›¸å…³å†…å®¹ç‰‡æ®µ
- ç»¼åˆå¤šä¸ªç‰‡æ®µä¿¡æ¯ï¼Œé¿å…çŸ›ç›¾
- ä¼˜å…ˆä½¿ç”¨æœ€ç›´æ¥ç›¸å…³çš„ä¿¡æ¯

è¯·å›ç­”ï¼š"""

            # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            if hasattr(self.llm_model, 'complete'):
                response = self.llm_model.complete(prompt)
                answer = str(response).strip()
            else:
                # å¯¹äºOllamaç­‰å…¶ä»–æ¨¡å‹
                from llama_index.core.base.llms.types import ChatMessage
                messages = [ChatMessage(role="user", content=prompt)]
                response = self.llm_model.chat(messages)
                answer = str(response.message.content).strip()
            
            # æ¸…ç†ç­”æ¡ˆï¼Œç§»é™¤å¯èƒ½çš„å‰è¨€
            if answer.startswith(("æ ¹æ®", "åŸºäº", "æ®", "ä»")):
                # å°è¯•æ‰¾åˆ°å®é™…ç­”æ¡ˆå¼€å§‹çš„ä½ç½®
                for prefix in ["ï¼Œ", "ï¼š", ":", ",", "ã€‚"]:
                    if prefix in answer:
                        answer = answer.split(prefix, 1)[1].strip()
                        break
            
            self.logger.info(f"æˆåŠŸç”Ÿæˆç­”æ¡ˆï¼Œä½¿ç”¨äº† {len(retrieved_chunks)} ä¸ªæ–‡æ¡£å—")
            return answer
            
        except Exception as e:
            self.logger.error(f"ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}")
            # é™çº§ä¸ºç®€å•æ‹¼æ¥
            context = "\n".join([f"- {chunk.content}" for chunk in retrieved_chunks[:3]])
            return f"åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š\n{context}\n\nå¯¹äºé—®é¢˜ '{query}'ï¼Œç”±äºæŠ€æœ¯é—®é¢˜æ— æ³•ç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼Œä½†ä¸Šè¿°ä¿¡æ¯å¯èƒ½å¯¹æ‚¨æœ‰å¸®åŠ©ã€‚"
    
    def query(self, question: str) -> RAGResponse:
        """æŸ¥è¯¢RAGç³»ç»Ÿ"""
        try:
            # æ£€ç´¢
            retrieved_chunks = self.retrieve(question)
            
            # ç”Ÿæˆç­”æ¡ˆ
            answer = self.generate_answer(question, retrieved_chunks)
            
            # æ„å»ºè¯æ®åˆ—è¡¨
            evidence = [chunk.content for chunk in retrieved_chunks]
            
            return RAGResponse(
                question=question,
                answer=answer,
                retrieved_chunks=retrieved_chunks,  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                system_config=self.config,          # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                evidence=[chunk.content for chunk in retrieved_chunks],  # æ·»åŠ evidenceå­—æ®µ
                metadata={
                    "retrieval_count": len(retrieved_chunks),
                    "confidence": 0.8
                }
            )
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return RAGResponse(
                question=question,
                answer=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
                retrieved_chunks=[],
                system_config=self.config,
                evidence=[],  # æ·»åŠ ç©ºçš„evidenceåˆ—è¡¨
                metadata={"error": str(e)}
            ) 