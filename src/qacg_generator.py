"""
QACGå››å…ƒç»„ç”Ÿæˆå™¨
ç”¨äºä¸ºæ¯ä¸ªRAGç³»ç»Ÿç”ŸæˆQuestion-Answer-Context-Groundtruthå››å…ƒç»„æ•°æ®
"""

import json
import logging
import random
from typing import List, Dict, Any, Tuple
from pathlib import Path
import os
import pandas as pd

from llama_index.llms.ollama import Ollama
from llama_index.llms.openai import OpenAI

from .rag_systems.llamaindex_rag import LlamaIndexRAGSystem
from .rag_systems.base_rag import RAGConfig


class QACGGenerator:
    """QACGå››å…ƒç»„ç”Ÿæˆå™¨"""
    
    def __init__(self, llm_model: str = "qwen2.5:7b"):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            llm_model: ç”¨äºç”Ÿæˆé—®é¢˜çš„LLMæ¨¡å‹
        """
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç”¨äºç”Ÿæˆé—®é¢˜çš„LLM
        if llm_model.startswith("openai"):
            self.question_llm = OpenAI(model=llm_model.replace("openai-", ""))
        else:
            self.question_llm = Ollama(model=llm_model, request_timeout=120.0)
        
        # é—®é¢˜ç”Ÿæˆæ¨¡æ¿
        self.question_templates = [
            "æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªå…·ä½“çš„é—®é¢˜ï¼š\n{context}\n\nè¯·ç”Ÿæˆä¸€ä¸ªå¯ä»¥ä»ä¸Šè¿°å†…å®¹ä¸­æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆçš„é—®é¢˜ï¼š",
            "åŸºäºè¿™æ®µæ–‡å­—ï¼Œæå‡ºä¸€ä¸ªå…³é”®é—®é¢˜ï¼š\n{context}\n\né—®é¢˜åº”è¯¥é’ˆå¯¹æ–‡æœ¬ä¸­çš„æ ¸å¿ƒä¿¡æ¯ï¼š",
            "é˜…è¯»ä¸‹é¢çš„å†…å®¹ï¼Œè®¾è®¡ä¸€ä¸ªé—®é¢˜ï¼š\n{context}\n\né—®é¢˜è¦æ±‚èƒ½å¤Ÿé€šè¿‡æ–‡æœ¬å†…å®¹å›ç­”ï¼š",
            "è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æå‡ºä¸€ä¸ªé—®é¢˜ï¼š\n{context}\n\nç¡®ä¿é—®é¢˜çš„ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­å¯ä»¥æ‰¾åˆ°ï¼š",
            "åˆ†æè¿™æ®µæ–‡å­—ï¼Œç”Ÿæˆç›¸å…³é—®é¢˜ï¼š\n{context}\n\né—®é¢˜åº”è¯¥æµ‹è¯•å¯¹æ–‡æœ¬å†…å®¹çš„ç†è§£ï¼š"
        ]
    
    def load_knowledge_base(self, jsonl_path: str) -> Dict[str, str]:
        """
        åŠ è½½çŸ¥è¯†åº“
        
        Args:
            jsonl_path: JSONLæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[str, str]: æ–‡æ¡£IDåˆ°å†…å®¹çš„æ˜ å°„
        """
        knowledge_base = {}
        
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                if line.strip():
                    try:
                        doc = json.loads(line)
                        content_id = doc.get('content_id', f'doc_{line_num}')
                        content = doc.get('content', '')
                        if content:
                            knowledge_base[content_id] = content
                    except json.JSONDecodeError:
                        self.logger.warning(f"è·³è¿‡æ— æ³•è§£æçš„è¡Œ {line_num + 1}")
        
        self.logger.info(f"åŠ è½½äº† {len(knowledge_base)} ä¸ªæ–‡æ¡£")
        return knowledge_base
    
    def sample_documents(self, knowledge_base: Dict[str, str], sample_size: int = 50) -> Dict[str, str]:
        """
        ä»çŸ¥è¯†åº“ä¸­é‡‡æ ·æ–‡æ¡£
        
        Args:
            knowledge_base: å®Œæ•´çŸ¥è¯†åº“
            sample_size: é‡‡æ ·å¤§å°
            
        Returns:
            Dict[str, str]: é‡‡æ ·åçš„æ–‡æ¡£
        """
        if len(knowledge_base) <= sample_size:
            return knowledge_base
        
        # éšæœºé‡‡æ ·
        doc_ids = list(knowledge_base.keys())
        sampled_ids = random.sample(doc_ids, sample_size)
        
        sampled_docs = {doc_id: knowledge_base[doc_id] for doc_id in sampled_ids}
        self.logger.info(f"é‡‡æ ·äº† {len(sampled_docs)} ä¸ªæ–‡æ¡£ç”¨äºç”ŸæˆQACG")
        
        return sampled_docs
    
    def generate_question_from_context(self, context: str) -> str:
        """
        ä»ä¸Šä¸‹æ–‡ç”Ÿæˆé—®é¢˜
        
        Args:
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬
            
        Returns:
            str: ç”Ÿæˆçš„é—®é¢˜
        """
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡æ¿
        template = random.choice(self.question_templates)
        prompt = template.format(context=context[:1000])  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        
        try:
            if hasattr(self.question_llm, 'complete'):
                response = self.question_llm.complete(prompt)
                question = str(response).strip()
            else:
                # å¯¹äºOllamaç­‰å…¶ä»–æ¨¡å‹
                response = self.question_llm.generate([prompt])
                question = str(response).strip()
            
            # æ¸…ç†é—®é¢˜æ ¼å¼
            question = question.replace("é—®é¢˜ï¼š", "").replace("Question:", "").strip()
            if not question.endswith('?') and not question.endswith('ï¼Ÿ'):
                question += 'ï¼Ÿ'
            
            return question
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
            # é™çº§ä¸ºè§„åˆ™ç”Ÿæˆ
            return self._generate_rule_based_question(context)
    
    def _generate_rule_based_question(self, context: str) -> str:
        """
        åŸºäºè§„åˆ™ç”Ÿæˆé—®é¢˜ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        
        Args:
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬
            
        Returns:
            str: ç”Ÿæˆçš„é—®é¢˜
        """
        # ç®€å•çš„è§„åˆ™ï¼šæå–å…³é”®ä¿¡æ¯ç”Ÿæˆé—®é¢˜
        if "æ—¶é—´" in context or "æ—¥æœŸ" in context:
            return "è¿™ä»¶äº‹å‘ç”Ÿåœ¨ä»€ä¹ˆæ—¶é—´ï¼Ÿ"
        elif "åŸå› " in context or "å› ä¸º" in context:
            return "é€ æˆè¿™ç§æƒ…å†µçš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
        elif "ç»“æœ" in context or "å½±å“" in context:
            return "è¿™ä»¶äº‹äº§ç”Ÿäº†ä»€ä¹ˆå½±å“æˆ–ç»“æœï¼Ÿ"
        elif "åœ°ç‚¹" in context or "åœ°åŒº" in context:
            return "è¿™ä»¶äº‹å‘ç”Ÿåœ¨å“ªé‡Œï¼Ÿ"
        else:
            return "æ ¹æ®æ–‡æœ¬å†…å®¹ï¼Œä¸»è¦è®²è¿°äº†ä»€ä¹ˆï¼Ÿ"
    
    def load_test_questions(self, qa_file_path: str = "dice/70æ¡æµ‹è¯•æ•°æ®QA.txt") -> List[Dict[str, Any]]:
        """
        åŠ è½½ç»™å®šçš„70æ¡æµ‹è¯•æ•°æ®
        
        Args:
            qa_file_path: QAæµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Dict]: æµ‹è¯•é—®é¢˜åˆ—è¡¨
        """
        try:
            with open(qa_file_path, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
            return test_data
        except Exception as e:
            self.logger.error(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            return []

    def generate_qacg_for_system(self, 
                                 rag_system: LlamaIndexRAGSystem, 
                                 knowledge_base: Dict[str, str],
                                 num_questions: int = 70) -> List[Dict[str, Any]]:
        """
        ä¸ºç‰¹å®šRAGç³»ç»Ÿä½¿ç”¨ç»™å®šçš„70æ¡æµ‹è¯•æ•°æ®ç”ŸæˆQACGå››å…ƒç»„
        
        Args:
            rag_system: RAGç³»ç»Ÿå®ä¾‹
            knowledge_base: çŸ¥è¯†åº“
            num_questions: ä½¿ç”¨çš„é—®é¢˜æ•°é‡ï¼ˆé»˜è®¤70ï¼‰
            
        Returns:
            List[Dict]: QACGå››å…ƒç»„åˆ—è¡¨
        """
        self.logger.info(f"ä¸ºç³»ç»Ÿ {rag_system.config.system_name} ä½¿ç”¨ç»™å®šæµ‹è¯•æ•°æ®ç”ŸæˆQACGå››å…ƒç»„")
        
        # åŠ è½½ç»™å®šçš„70æ¡æµ‹è¯•æ•°æ®
        test_questions = self.load_test_questions()
        if not test_questions:
            self.logger.error("æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®ï¼Œå›é€€åˆ°ç”Ÿæˆæ¨¡å¼")
            return self._generate_qacg_fallback(rag_system, knowledge_base, num_questions)
        
        # ä½¿ç”¨æŒ‡å®šæ•°é‡çš„é—®é¢˜
        questions_to_use = test_questions[:num_questions]
        self.logger.info(f"ä½¿ç”¨å‰ {len(questions_to_use)} æ¡æµ‹è¯•é—®é¢˜")
        
        qacg_list = []
        
        for i, test_item in enumerate(questions_to_use):
            try:
                question = test_item["question"]
                expected_answer = test_item["answer"]
                
                # ä½¿ç”¨RAGç³»ç»Ÿç”Ÿæˆç­”æ¡ˆ
                rag_response = rag_system.query(question)
                rag_answer = rag_response.answer
                evidence = rag_response.evidence
                
                # æ„å»ºQACGå››å…ƒç»„
                qacg = {
                    "question": question,
                    "rag_answer": rag_answer,  # RAGç”Ÿæˆçš„ç­”æ¡ˆ
                    "expected_answer": expected_answer,  # é¢„æœŸç­”æ¡ˆ
                    "context": evidence,  # RAGæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
                    "groundtruth": test_item.get("relevant_content", expected_answer),
                    "metadata": {
                        "system_name": rag_system.config.system_name,
                        "embedding_model": rag_system.config.embedding_model,
                        "llm_model": rag_system.config.llm_model,
                        "chunking_strategy": rag_system.config.chunking_strategy,
                        "retrieval_top_k": rag_system.config.retrieval_top_k,
                        "question_id": f"test_q_{i+1}",
                        "task_name": test_item.get("task_name", "unknown"),
                        "relevant_passage": test_item.get("relevant_passage", ""),
                        "generated_at": str(pd.Timestamp.now())
                    }
                }
                
                qacg_list.append(qacg)
                self.logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(questions_to_use)} ä¸ªæµ‹è¯•é—®é¢˜: {question[:50]}...")
                
            except Exception as e:
                self.logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªæµ‹è¯•é—®é¢˜æ—¶å‡ºé”™: {e}")
                continue
        
        self.logger.info(f"æˆåŠŸå¤„ç† {len(qacg_list)} ä¸ªæµ‹è¯•é—®é¢˜")
        return qacg_list
    
    def _generate_qacg_fallback(self, 
                               rag_system: LlamaIndexRAGSystem, 
                               knowledge_base: Dict[str, str],
                               num_questions: int) -> List[Dict[str, Any]]:
        """
        å›é€€åˆ°åŸå§‹çš„é—®é¢˜ç”Ÿæˆæ¨¡å¼ï¼ˆå½“æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®æ—¶ä½¿ç”¨ï¼‰
        """
        self.logger.info("ä½¿ç”¨å›é€€æ¨¡å¼ç”Ÿæˆé—®é¢˜")
        
        # é‡‡æ ·æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
        sampled_docs = self.sample_documents(knowledge_base, min(50, len(knowledge_base)))
        doc_contents = list(sampled_docs.values())
        
        qacg_list = []
        
        for i in range(num_questions):
            try:
                # éšæœºé€‰æ‹©ä¸€ä¸ªæ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
                context = random.choice(doc_contents)
                
                # æˆªå–é€‚å½“é•¿åº¦çš„ä¸Šä¸‹æ–‡
                context = context[:800] if len(context) > 800 else context
                
                # ç”Ÿæˆé—®é¢˜
                question = self.generate_question_from_context(context)
                
                # ä½¿ç”¨RAGç³»ç»Ÿç”Ÿæˆç­”æ¡ˆ
                rag_response = rag_system.query(question)
                answer = rag_response.answer
                evidence = rag_response.evidence
                
                # æ„å»ºQACGå››å…ƒç»„
                qacg = {
                    "question": question,
                    "answer": answer,
                    "context": evidence,
                    "groundtruth": context,
                    "metadata": {
                        "system_name": rag_system.config.system_name,
                        "embedding_model": rag_system.config.embedding_model,
                        "llm_model": rag_system.config.llm_model,
                        "chunking_strategy": rag_system.config.chunking_strategy,
                        "retrieval_top_k": rag_system.config.retrieval_top_k,
                        "question_id": f"fallback_q_{i+1}",
                        "generated_at": str(pd.Timestamp.now())
                    }
                }
                
                qacg_list.append(qacg)
                self.logger.info(f"ç”Ÿæˆç¬¬ {i+1}/{num_questions} ä¸ªQACG")
                
            except Exception as e:
                self.logger.error(f"ç”Ÿæˆç¬¬ {i+1} ä¸ªQACGæ—¶å‡ºé”™: {e}")
                continue
        
        self.logger.info(f"æˆåŠŸç”Ÿæˆ {len(qacg_list)} ä¸ªQACGå››å…ƒç»„")
        return qacg_list
    
    def create_rag_systems(self) -> List[LlamaIndexRAGSystem]:
        """
        åˆ›å»º8ç§RAGç³»ç»Ÿé…ç½® (2x2x2)
        
        Returns:
            List[LlamaIndexRAGSystem]: RAGç³»ç»Ÿåˆ—è¡¨
        """
        embedding_models = ["bge-large-zh", "bge-small-zh"]
        chunking_strategies = ["chunk_256", "chunk_512"]  # æ”¹ä¸ºåŸºäºé•¿åº¦çš„åˆ†å—ç­–ç•¥
        llm_models = ["qwen2.5", "qwen2.5-mini"]
        
        systems = []
        
        for embedding in embedding_models:
            for chunking in chunking_strategies:
                for llm in llm_models:
                    config = RAGConfig(
                        system_name=f"{embedding}_{chunking}_{llm}",
                        chunking_strategy=chunking,
                        chunk_size=512,
                        chunk_overlap=50,
                        embedding_model=embedding,
                        llm_model=llm,
                        retrieval_top_k=3,
                        temperature=0.1
                    )
                    
                    system = LlamaIndexRAGSystem(config)
                    systems.append(system)
        
        self.logger.info(f"åˆ›å»ºäº† {len(systems)} ä¸ªRAGç³»ç»Ÿ")
        return systems
    
    def save_qacg_results(self, qacg_data: List[Dict[str, Any]], output_path: str):
        """
        ä¿å­˜QACGç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            qacg_data: QACGæ•°æ®
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(qacg_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"QACGç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    def generate_all_qacg(self, 
                          jsonl_path: str, 
                          output_dir: str = "qacg_output",
                          num_questions: int = 70):
        """
        ä¸ºæ‰€æœ‰RAGç³»ç»Ÿç”ŸæˆQACGå››å…ƒç»„
        ç¡®ä¿æ¯ä¸ªç³»ç»Ÿç‹¬ç«‹å¤„ç†JSONLä¸­çš„æ¯è¡Œæ–‡æœ¬
        
        Args:
            jsonl_path: çŸ¥è¯†åº“JSONLæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            num_questions: æ¯ä¸ªç³»ç»Ÿç”Ÿæˆçš„é—®é¢˜æ•°é‡
        """
        self.logger.info("ğŸš€ å¼€å§‹ä¸ºæ‰€æœ‰RAGç³»ç»Ÿç”ŸæˆQACGå››å…ƒç»„")
        self.logger.info("=" * 80)
        
        # é¢„å…ˆåŠ è½½åŸå§‹çŸ¥è¯†åº“ï¼Œè®°å½•åŸºæœ¬ä¿¡æ¯
        raw_knowledge_base = self.load_knowledge_base(jsonl_path)
        self.logger.info(f"ğŸ“‚ åŠ è½½åŸå§‹çŸ¥è¯†åº“: {jsonl_path}")
        self.logger.info(f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {len(raw_knowledge_base)}")
        self.logger.info(f"ğŸ“ æ–‡æ¡£é•¿åº¦èŒƒå›´: {min(len(v) for v in raw_knowledge_base.values())} - {max(len(v) for v in raw_knowledge_base.values())} å­—ç¬¦")
        
        # åˆ›å»ºRAGç³»ç»Ÿ
        rag_systems = self.create_rag_systems()
        self.logger.info(f"ğŸ—ï¸  åˆ›å»ºäº† {len(rag_systems)} ä¸ªRAGç³»ç»Ÿé…ç½®")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¸ºæ¯ä¸ªç³»ç»Ÿç‹¬ç«‹å¤„ç†çŸ¥è¯†åº“å¹¶ç”ŸæˆQACG
        system_stats = []
        
        for i, system in enumerate(rag_systems, 1):
            try:
                self.logger.info("=" * 80)
                self.logger.info(f"ğŸ”„ å¤„ç†ç³»ç»Ÿ {i}/{len(rag_systems)}: {system.config.system_name}")
                self.logger.info("=" * 80)
                
                # æ¯ä¸ªç³»ç»Ÿç‹¬ç«‹åŠ è½½å’Œå¤„ç†çŸ¥è¯†åº“
                self.logger.info(f"ğŸ“š ä¸ºç³»ç»Ÿ {system.config.system_name} ç‹¬ç«‹å¤„ç†çŸ¥è¯†åº“")
                self.logger.info("ğŸ”‘ å…³é”®ç‰¹æ€§: æ¯ä¸ªç³»ç»Ÿæ ¹æ®å…¶é…ç½®ç‹¬ç«‹åˆ†å—å’ŒåµŒå…¥ç›¸åŒçš„åŸå§‹æ•°æ®")
                
                # ä½¿ç”¨ç³»ç»Ÿç‰¹å®šçš„å¤„ç†ç­–ç•¥å¤„ç†çŸ¥è¯†åº“
                processed_knowledge_base = self._process_knowledge_base_for_system(
                    system, raw_knowledge_base.copy()  # ä¼ é€’å‰¯æœ¬ç¡®ä¿ç‹¬ç«‹æ€§
                )
                
                # è®°å½•ç³»ç»Ÿå¤„ç†ç»Ÿè®¡
                stats = {
                    'system_name': system.config.system_name,
                    'chunking_strategy': system.config.chunking_strategy,
                    'embedding_model': system.config.embedding_model,
                    'llm_model': system.config.llm_model,
                    'input_docs': len(processed_knowledge_base),
                    'chunks_generated': len(system.nodes) if hasattr(system, 'nodes') and system.nodes else 0
                }
                system_stats.append(stats)
                
                # ç”ŸæˆQACG
                self.logger.info(f"â“ å¼€å§‹ä¸ºç³»ç»Ÿ {system.config.system_name} ç”Ÿæˆ {num_questions} ä¸ªQACGå››å…ƒç»„")
                qacg_data = self.generate_qacg_for_system(
                    system, processed_knowledge_base, num_questions
                )
                
                # ä¿å­˜ç»“æœ
                output_path = os.path.join(
                    output_dir, 
                    f"qacg_{system.config.system_name}.json"
                )
                self.save_qacg_results(qacg_data, output_path)
                
                self.logger.info(f"âœ… ç³»ç»Ÿ {system.config.system_name} å¤„ç†å®Œæˆ")
                self.logger.info(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_path}")
                
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†ç³»ç»Ÿ {system.config.system_name} æ—¶å‡ºé”™: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
                continue
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š æ‰€æœ‰RAGç³»ç»Ÿå¤„ç†å®Œæˆ - ç‹¬ç«‹æ€§éªŒè¯æŠ¥å‘Š")
        self.logger.info("=" * 80)
        
        if system_stats:
            # æŒ‰chunkingç­–ç•¥åˆ†ç»„ç»Ÿè®¡
            chunking_stats = {}
            for stat in system_stats:
                strategy = stat['chunking_strategy']
                if strategy not in chunking_stats:
                    chunking_stats[strategy] = []
                chunking_stats[strategy].append(stat['chunks_generated'])
            
            self.logger.info("ğŸ” åˆ†å—ç­–ç•¥ç‹¬ç«‹æ€§éªŒè¯:")
            for strategy, chunks_list in chunking_stats.items():
                self.logger.info(f"   {strategy}: {chunks_list} (chunksæ•°é‡)")
                if len(set(chunks_list)) > 1:
                    self.logger.info(f"     âœ… ä¸åŒembeddingæ¨¡å‹äº§ç”Ÿäº†ä¸åŒçš„chunkæ•°é‡")
                else:
                    self.logger.info(f"     âš ï¸  æ‰€æœ‰embeddingæ¨¡å‹äº§ç”Ÿäº†ç›¸åŒçš„chunkæ•°é‡")
            
            # éªŒè¯ä¸åŒç­–ç•¥äº§ç”Ÿäº†ä¸åŒçš„ç»“æœ
            all_chunks = [stat['chunks_generated'] for stat in system_stats]
            unique_chunks = len(set(all_chunks))
            self.logger.info(f"ğŸ¯ æ€»ä½“ç‹¬ç«‹æ€§: {unique_chunks}/{len(system_stats)} ç§ä¸åŒçš„chunkæ•°é‡")
            
            if unique_chunks > 1:
                self.logger.info("âœ… ç¡®è®¤ï¼šä¸åŒRAGç³»ç»Ÿé…ç½®äº§ç”Ÿäº†ä¸åŒçš„å¤„ç†ç»“æœ")
            else:
                self.logger.warning("âš ï¸  è­¦å‘Šï¼šæ‰€æœ‰ç³»ç»Ÿäº§ç”Ÿäº†ç›¸åŒçš„chunkæ•°é‡ï¼Œè¯·æ£€æŸ¥é…ç½®å·®å¼‚")
                
        self.logger.info("ğŸ‰ æ‰€æœ‰RAGç³»ç»Ÿçš„QACGç”Ÿæˆå®Œæˆ") 
    
    def _process_knowledge_base_for_system(self, system: LlamaIndexRAGSystem, 
                                         raw_knowledge_base: Dict[str, str]) -> Dict[str, str]:
        """
        ä¸ºç‰¹å®šç³»ç»Ÿå¤„ç†çŸ¥è¯†åº“ - ç¡®ä¿ç‹¬ç«‹å¤„ç†
        
        Args:
            system: RAGç³»ç»Ÿå®ä¾‹
            raw_knowledge_base: åŸå§‹çŸ¥è¯†åº“æ•°æ®(JSONLæ–‡ä»¶çš„æ¯è¡Œä½œä¸ºä¸€ä¸ªæ–‡æ¡£)
            
        Returns:
            Dict[str, str]: å¤„ç†åçš„çŸ¥è¯†åº“
        """
        self.logger.info(f"ğŸ”„ ä¸ºç³»ç»Ÿ {system.config.system_name} æ‰§è¡Œç‹¬ç«‹çš„çŸ¥è¯†åº“å¤„ç†")
        self.logger.info(f"ğŸ“Š åŸå§‹çŸ¥è¯†åº“æ–‡æ¡£æ•°é‡: {len(raw_knowledge_base)}")
        self.logger.info(f"âš™ï¸  ç³»ç»Ÿé…ç½®:")
        self.logger.info(f"   - Chunkingç­–ç•¥: {system.config.chunking_strategy}")
        self.logger.info(f"   - Chunkå¤§å°: {system.config.chunk_size}")
        self.logger.info(f"   - Chunké‡å : {system.config.chunk_overlap}")
        self.logger.info(f"   - Embeddingæ¨¡å‹: {system.config.embedding_model}")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†è¿‡çš„çŸ¥è¯†åº“ç¼“å­˜
        cache_dir = f"./knowledge_cache/{system.config.system_name}"
        os.makedirs(cache_dir, exist_ok=True)
        cache_file = os.path.join(cache_dir, "processed_kb.json")
        
        # æ£€æŸ¥ç¼“å­˜
        if os.path.exists(cache_file):
            self.logger.info(f"ğŸ“ æ£€æŸ¥ç¼“å­˜æ–‡ä»¶: {cache_file}")
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_kb = json.load(f)
                
                # éªŒè¯ç¼“å­˜é…ç½®æ˜¯å¦åŒ¹é…
                if self._validate_cache_config(cached_kb.get('config', {}), system.config):
                    self.logger.info(f"âœ… ç¼“å­˜é…ç½®åŒ¹é…ï¼ŒåŠ è½½ç¼“å­˜çš„çŸ¥è¯†åº“")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„chunksæ•°æ®
                    if 'chunks' in cached_kb and cached_kb['chunks']:
                        self.logger.info(f"ğŸ“¦ å‘ç°ç¼“å­˜çš„chunksæ•°æ®: {len(cached_kb['chunks'])} ä¸ªchunks")
                        self.logger.info(f"ğŸš€ ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„chunksï¼Œè·³è¿‡é‡æ–°åˆ†å—")
                        
                        # ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„chunksæ•°æ®æ„å»ºå‘é‡å­˜å‚¨
                        self._load_cached_chunks_and_build_vector_store(system, cached_kb['chunks'])
                        
                        return cached_kb['knowledge_base']
                    else:
                        self.logger.info(f"âš ï¸ ç¼“å­˜ä¸­æ²¡æœ‰chunksæ•°æ®ï¼Œéœ€è¦é‡æ–°å¤„ç†")
                        # é‡æ–°å¤„ç†çŸ¥è¯†åº“ä»¥å»ºç«‹ç´¢å¼•
                        self.logger.info(f"ğŸ—ï¸  é‡æ–°å»ºç«‹å‘é‡ç´¢å¼• (ç¼“å­˜ä¸­æ— chunksæ•°æ®)")
                        system.process_knowledge_base(cached_kb['knowledge_base'])
                        
                        return cached_kb['knowledge_base']
                else:
                    self.logger.info(f"âŒ ç¼“å­˜é…ç½®ä¸åŒ¹é…ï¼Œå°†é‡æ–°å¤„ç†")
            except Exception as e:
                self.logger.warning(f"âš ï¸  è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–é…ç½®ä¸åŒ¹é…ï¼Œé‡æ–°å¤„ç†
        self.logger.info(f"ğŸ”„ å¼€å§‹ç‹¬ç«‹å¤„ç†çŸ¥è¯†åº“")
        self.logger.info(f"ğŸ“ å¤„ç†ç­–ç•¥è¯¦æƒ…:")
        self.logger.info(f"   - æ¯ä¸ªJSONLè¡Œå°†ä½œä¸ºç‹¬ç«‹æ–‡æ¡£å¤„ç†")
        self.logger.info(f"   - ä½¿ç”¨ {system.config.chunking_strategy} åˆ†å—ç­–ç•¥")
        self.logger.info(f"   - ä½¿ç”¨ {system.config.embedding_model} åµŒå…¥æ¨¡å‹")
        
        # ä¸ºå½“å‰ç³»ç»Ÿç‹¬ç«‹å¤„ç†çŸ¥è¯†åº“
        # è¿™é‡Œçš„å…³é”®æ˜¯ï¼šæ¯ä¸ªç³»ç»Ÿéƒ½ä¼šæ ¹æ®è‡ªå·±çš„é…ç½®ç‹¬ç«‹åˆ†å—å’ŒåµŒå…¥
        processing_result = system.process_knowledge_base(raw_knowledge_base)
        
        # è®°å½•å¤„ç†ç»“æœç»Ÿè®¡ä¿¡æ¯å¹¶å‡†å¤‡ç¼“å­˜æ•°æ®
        chunks_data = []
        if hasattr(system, 'nodes') and system.nodes:
            chunk_count = len(system.nodes)
            self.logger.info(f"ğŸ“ˆ ç³»ç»Ÿ {system.config.system_name} å¤„ç†ç»Ÿè®¡:")
            self.logger.info(f"   - è¾“å…¥æ–‡æ¡£æ•°: {len(raw_knowledge_base)}")
            self.logger.info(f"   - ç”Ÿæˆchunksæ•°: {chunk_count}")
            self.logger.info(f"   - å¹³å‡æ¯æ–‡æ¡£chunks: {chunk_count/len(raw_knowledge_base):.2f}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªchunkçš„æ ·æœ¬
            for i, node in enumerate(system.nodes[:3]):
                self.logger.info(f"   - Chunk {i+1} (é•¿åº¦ {len(node.text)}): {node.text[:50]}...")
            
            # å°†nodesè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼ç”¨äºç¼“å­˜
            self.logger.info("ğŸ“¦ å‡†å¤‡ç¼“å­˜chunksæ•°æ®...")
            for i, node in enumerate(system.nodes):
                chunk_data = {
                    "chunk_id": f"{node.metadata.get('doc_id', 'unknown')}_{i}",
                    "content": node.text,
                    "source_doc_id": node.metadata.get("doc_id", "unknown"),
                    "metadata": dict(node.metadata)
                }
                chunks_data.append(chunk_data)
            
            self.logger.info(f"âœ… å·²å‡†å¤‡ {len(chunks_data)} ä¸ªchunksç”¨äºç¼“å­˜")
        
        # ç¼“å­˜å¤„ç†ç»“æœï¼ˆåŒ…å«chunksæ•°æ®ï¼‰
        cache_data = {
            'config': {
                'chunking_strategy': system.config.chunking_strategy,
                'chunk_size': system.config.chunk_size,
                'chunk_overlap': system.config.chunk_overlap,
                'embedding_model': system.config.embedding_model,
                'system_name': system.config.system_name
            },
            'knowledge_base': raw_knowledge_base,
            'chunks': chunks_data,  # æ–°å¢ï¼šä¿å­˜chunksæ•°æ®
            'processing_stats': {
                'input_doc_count': len(raw_knowledge_base),
                'chunk_count': len(chunks_data),
                'processed_at': str(pd.Timestamp.now())
            }
        }
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ çŸ¥è¯†åº“å¤„ç†å®Œæˆå¹¶ç¼“å­˜: {cache_file}")
        self.logger.info(f"âœ… ç³»ç»Ÿ {system.config.system_name} çŸ¥è¯†åº“ç‹¬ç«‹å¤„ç†å®Œæˆ")
        
        return raw_knowledge_base
    
    def _load_cached_chunks_and_build_vector_store(self, system: LlamaIndexRAGSystem, cached_chunks: List[Dict]) -> None:
        """
        ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„chunksæ•°æ®æ„å»ºå‘é‡å­˜å‚¨
        
        Args:
            system: RAGç³»ç»Ÿå®ä¾‹
            cached_chunks: ç¼“å­˜çš„chunksæ•°æ®
        """
        try:
            self.logger.info(f"ğŸ”„ ä¸ºç³»ç»Ÿ {system.config.system_name} ä½¿ç”¨ç¼“å­˜chunksæ„å»ºå‘é‡å­˜å‚¨")
            
            # å°†ç¼“å­˜çš„chunksè½¬æ¢ä¸ºç³»ç»Ÿéœ€è¦çš„æ ¼å¼
            # ç›´æ¥è°ƒç”¨å‘é‡å­˜å‚¨æ„å»ºï¼Œè·³è¿‡åˆ†å—æ­¥éª¤
            system.build_vector_store(cached_chunks, [])  # embeddingså‚æ•°ä¸ºç©ºï¼ŒLlamaIndexä¼šè‡ªåŠ¨å¤„ç†
            
            # è®¾ç½®ç³»ç»ŸçŠ¶æ€ä¸ºå·²ç´¢å¼•
            system.is_indexed = True
            
            self.logger.info(f"âœ… æˆåŠŸä½¿ç”¨ç¼“å­˜chunksæ„å»ºå‘é‡å­˜å‚¨")
            
        except Exception as e:
            self.logger.error(f"âŒ ä½¿ç”¨ç¼“å­˜chunksæ„å»ºå‘é‡å­˜å‚¨å¤±è´¥: {e}")
            self.logger.warning(f"âš ï¸ å›é€€åˆ°æ ‡å‡†å¤„ç†æµç¨‹")
            # å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æµç¨‹
            raise e

    def _validate_cache_config(self, cached_config: Dict, current_config) -> bool:
        """éªŒè¯ç¼“å­˜é…ç½®æ˜¯å¦ä¸å½“å‰é…ç½®åŒ¹é…"""
        key_fields = ['chunking_strategy', 'chunk_size', 'chunk_overlap', 'embedding_model']
        
        for field in key_fields:
            if cached_config.get(field) != getattr(current_config, field):
                return False
        
        return True 