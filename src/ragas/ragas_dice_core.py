#!/usr/bin/env python3
"""
RAGAS DICE æ ¸å¿ƒæ¨¡å—
åŸºäºRAGASæ¡†æ¶çš„ç³»ç»Ÿè¯„åˆ†å’Œæ’å
"""

import json
import logging
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np

# å¯¼å…¥RAGASè¯„ä¼°å™¨
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))
from ragas_evaluator import RagasEvaluator, RagasConfig


@dataclass
class RagasDiceConfig:
    """RAGAS DICEé…ç½®"""
    llm_model: str = "deepseek-chat"
    embeddings_model: str = "BAAI/bge-small-zh-v1.5"  # ä½¿ç”¨æ›´å°çš„æ¨¡å‹èŠ‚çœå†…å­˜
    metrics: List[str] = None
    api_key: str = ""
    base_url: str = "https://api.deepseek.com"
    output_dir: str = "ragas_dice_output"
    max_workers: int = 1
    batch_size: int = 5
    
    def __post_init__(self):
        if self.metrics is None:
            # åŸºäºRAGASåŸè®ºæ–‡çš„ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦
            self.metrics = [
                "faithfulness",
                "answer_relevancy",
                "context_relevance"
            ]
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)


class RagasDiceEvaluator:
    """RAGAS DICEè¯„ä¼°å™¨"""
    
    def __init__(self, config: RagasDiceConfig):
        self.config = config
        self.logger = logging.getLogger("RagasDice")
        self._setup_logger()
        
        # åˆ›å»ºRAGASé…ç½®
        self.ragas_config = RagasConfig(
            llm_model=config.llm_model,
            embeddings_model=config.embeddings_model,
            metrics=config.metrics,
            api_key=config.api_key,
            base_url=config.base_url
        )
        
        # åˆ›å»ºRAGASè¯„ä¼°å™¨
        self.ragas_evaluator = RagasEvaluator(self.ragas_config)
        
        self.logger.info(f"RAGAS DICEè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {config.llm_model}")
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def evaluate_single_system(self, qacg_file: str, system_name: str = None) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªç³»ç»Ÿçš„QACGæ•°æ®
        
        Args:
            qacg_file: QACGæ–‡ä»¶è·¯å¾„
            system_name: ç³»ç»Ÿåç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        qacg_path = Path(qacg_file)
        if not qacg_path.exists():
            raise FileNotFoundError(f"QACGæ–‡ä»¶ä¸å­˜åœ¨: {qacg_file}")
        
        # ç¡®å®šç³»ç»Ÿåç§°
        if system_name is None:
            system_name = qacg_path.stem.replace("qacg_", "")
        
        self.logger.info(f"ğŸ” å¼€å§‹è¯„ä¼°ç³»ç»Ÿ: {system_name}")
        
        # åŠ è½½QACGæ•°æ®
        with open(qacg_file, 'r', encoding='utf-8') as f:
            qacg_data = json.load(f)
        
        self.logger.info(f"ğŸ“Š åŠ è½½äº† {len(qacg_data)} ä¸ªé—®ç­”å¯¹")
        
        # æ‰¹é‡è¯„ä¼°
        all_scores = []
        total_items = len(qacg_data)
        
        self.logger.info(f"âš™ï¸ è¯„ä¼°é…ç½®: {self.config.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹, æ‰¹å¤§å°: {self.config.batch_size}")
        if self.config.max_workers > 1:
            self.logger.info(f"ğŸš€ å¯ç”¨å¹¶å‘æ¨¡å¼ï¼Œé¢„è®¡åŠ é€Ÿ {self.config.max_workers}x")
        else:
            self.logger.info(f"ğŸ”„ ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼ï¼ˆå®‰å…¨æ¨¡å¼ï¼‰")
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for i in range(0, total_items, self.config.batch_size):
            batch = qacg_data[i:i+self.config.batch_size]
            batch_num = i // self.config.batch_size + 1
            total_batches = (total_items + self.config.batch_size - 1) // self.config.batch_size
            
            self.logger.info(f"\n{'='*20} æ‰¹æ¬¡ {batch_num}/{total_batches} {'='*20}")
            self.logger.info(f"â³ å¼€å§‹å¤„ç† {len(batch)} ä¸ªé—®ç­”å¯¹ (é¢˜ç›® {i+1}-{min(i+self.config.batch_size, total_items)})")
            
            # ä½¿ç”¨æ–°çš„å¹¶å‘è¯„ä¼°æ–¹æ³•
            batch_scores = self._evaluate_batch_concurrent(batch, i, system_name, total_items)
            all_scores.extend(batch_scores)
            
            # æ‰¹æ¬¡å®Œæˆæ€»ç»“
            completed = min(i + self.config.batch_size, total_items)
            progress = completed / total_items * 100
            
            # è®¡ç®—æ‰¹æ¬¡ç»Ÿè®¡
            batch_success = len([s for s in batch_scores if "error" not in s])
            batch_avg_score = sum(s["composite_score"] for s in batch_scores if "error" not in s) / max(batch_success, 1)
            
            self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ:")
            self.logger.info(f"    ğŸ“Š æˆåŠŸ: {batch_success}/{len(batch)} é¢˜")
            self.logger.info(f"    ğŸ“ˆ æ‰¹æ¬¡å¹³å‡åˆ†: {batch_avg_score:.4f}")
            self.logger.info(f"    ğŸ¯ æ€»è¿›åº¦: {completed}/{total_items} ({progress:.1f}%)")
            self.logger.info(f"{'='*50}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        system_result = self._calculate_system_statistics(system_name, all_scores)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detail_file = Path(self.config.output_dir) / f"{system_name}_ragas_details.json"
        with open(detail_file, 'w', encoding='utf-8') as f:
            json.dump({
                "system_name": system_name,
                "total_questions": len(all_scores),
                "detailed_scores": all_scores,
                "statistics": system_result
            }, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"âœ… ç³»ç»Ÿ {system_name} è¯„ä¼°å®Œæˆ")
        self.logger.info(f"ğŸ“Š ç»¼åˆå¾—åˆ†: {system_result['composite_score']:.4f}")
        self.logger.info(f"ğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜è‡³: {detail_file}")
        
        return system_result
    
    def _evaluate_single_question(self, qa_item: Dict[str, Any], question_idx: int, total_questions: int, system_name: str) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®ç­”å¯¹"""
        try:
            question = qa_item.get("question", "")[:100]  # æˆªå–å‰100å­—ç¬¦æ˜¾ç¤º
            self.logger.info(f"ğŸ“ é—®é¢˜ {question_idx}/{total_questions}: {question}...")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­
            import threading
            thread_id = threading.current_thread().ident
            is_main_thread = thread_id == threading.main_thread().ident
            
            if not is_main_thread:
                # åœ¨å·¥ä½œçº¿ç¨‹ä¸­ï¼Œåˆ›å»ºç‹¬ç«‹çš„è¯„ä¼°å™¨å®ä¾‹ä»¥é¿å…äº‹ä»¶å¾ªç¯å†²çª
                from ragas_evaluator import RagasEvaluator, RagasConfig
                thread_config = RagasConfig(
                    llm_model=self.config.llm_model,
                    embeddings_model=self.config.embeddings_model,
                    metrics=self.config.metrics,
                    api_key=self.config.api_key,
                    base_url=self.config.base_url
                )
                thread_evaluator = RagasEvaluator(thread_config)
                scores = thread_evaluator.evaluate_single_qacg(qa_item)
                composite_score = thread_evaluator.calculate_composite_score(scores)
            else:
                # åœ¨ä¸»çº¿ç¨‹ä¸­ï¼Œä½¿ç”¨å…±äº«çš„è¯„ä¼°å™¨
                scores = self.ragas_evaluator.evaluate_single_qacg(qa_item)
                composite_score = self.ragas_evaluator.calculate_composite_score(scores)
            
            result = {
                "question": qa_item.get("question", ""),
                "scores": scores,
                "composite_score": composite_score,
                "question_idx": question_idx
            }
            
            # æ‰“å°è¯¦ç»†çš„é¢˜ç›®è¯„ä¼°ç»“æœ
            self._print_question_result(result, system_name, question_idx, total_questions)
            
            return result
            
        except Exception as e:
            import traceback
            error_msg = f"è¯„ä¼°é—®ç­”å¯¹å¤±è´¥: {e}"
            self.logger.error(f"âŒ é—®é¢˜ {question_idx}/{total_questions}: {error_msg}")
            
            # å¦‚æœæ˜¯äº‹ä»¶å¾ªç¯é”™è¯¯ï¼Œç»™å‡ºç‰¹æ®Šæç¤º
            if "event loop" in str(e).lower() or "asyncio" in str(e).lower():
                self.logger.error("âš ï¸ æ£€æµ‹åˆ°å¼‚æ­¥äº‹ä»¶å¾ªç¯å†²çªï¼Œå»ºè®®ä½¿ç”¨ --safe_mode æˆ–å‡å°‘ --max_workers")
            
            # æ·»åŠ é»˜è®¤å¾—åˆ†
            result = {
                "question": qa_item.get("question", ""),
                "scores": {metric: 0.0 for metric in self.config.metrics},
                "composite_score": 0.0,
                "error": str(e),
                "question_idx": question_idx
            }
            
            return result
    
    def _print_question_result(self, result: Dict[str, Any], system_name: str, question_idx: int, total_questions: int):
        """æ‰“å°å•ä¸ªé—®é¢˜çš„è¯„ä¼°ç»“æœ"""
        question = result["question"][:80] + "..." if len(result["question"]) > 80 else result["question"]
        composite_score = result["composite_score"]
        scores = result["scores"]
        
        # æ„å»ºæŒ‡æ ‡å¾—åˆ†å­—ç¬¦ä¸²
        metric_strs = []
        for metric, score in scores.items():
            if score is not None:
                metric_strs.append(f"{metric}={score:.3f}")
            else:
                metric_strs.append(f"{metric}=N/A")
        
        metrics_display = ", ".join(metric_strs)
        
        # æ‰“å°åˆ°æ§åˆ¶å°å’Œæ—¥å¿—
        result_msg = f"âœ… [{system_name}] é—®é¢˜ {question_idx}/{total_questions} å®Œæˆ"
        self.logger.info(result_msg)
        self.logger.info(f"    ğŸ“ é—®é¢˜: {question}")
        self.logger.info(f"    ğŸ“Š ç»¼åˆå¾—åˆ†: {composite_score:.4f}")
        self.logger.info(f"    ğŸ” å„æŒ‡æ ‡: {metrics_display}")
        
        # æ·»åŠ åˆ†éš”çº¿ï¼ˆæ¯10é¢˜ï¼‰
        if question_idx % 10 == 0:
            progress = question_idx / total_questions * 100
            self.logger.info(f"    ğŸ“ˆ [{system_name}] è¿›åº¦: {question_idx}/{total_questions} ({progress:.1f}%)")
            self.logger.info(f"    {'â”€' * 60}")
    
    def _evaluate_batch_concurrent(self, batch: List[Dict[str, Any]], batch_start_idx: int, system_name: str, total_questions: int) -> List[Dict[str, Any]]:
        """å¹¶å‘è¯„ä¼°ä¸€æ‰¹é—®ç­”å¯¹"""
        if self.config.max_workers <= 1:
            # å•çº¿ç¨‹æ¨¡å¼
            batch_scores = []
            for i, qa_item in enumerate(batch):
                question_idx = batch_start_idx + i + 1
                result = self._evaluate_single_question(qa_item, question_idx, total_questions, system_name)
                batch_scores.append(result)
            return batch_scores
        
        # å¤šçº¿ç¨‹æ¨¡å¼ - æ·»åŠ é”™è¯¯ç›‘æ§å’Œè‡ªåŠ¨é™çº§
        batch_scores = [None] * len(batch)
        asyncio_errors = 0
        max_asyncio_errors = 3  # æœ€å¤šå…è®¸3ä¸ªå¼‚æ­¥é”™è¯¯
        
        try:
            with ThreadPoolExecutor(max_workers=min(self.config.max_workers, len(batch))) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_idx = {}
                for i, qa_item in enumerate(batch):
                    question_idx = batch_start_idx + i + 1
                    future = executor.submit(
                        self._evaluate_single_question, 
                        qa_item, 
                        question_idx, 
                        total_questions, 
                        system_name
                    )
                    future_to_idx[future] = i
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        result = future.result()
                        batch_scores[idx] = result
                    except Exception as e:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚æ­¥ç›¸å…³é”™è¯¯
                        error_str = str(e).lower()
                        if "event loop" in error_str or "asyncio" in error_str or "bound to a different" in error_str:
                            asyncio_errors += 1
                            self.logger.error(f"âš ï¸ å¼‚æ­¥é”™è¯¯ #{asyncio_errors}: {e}")
                            
                            # å¦‚æœå¼‚æ­¥é”™è¯¯è¿‡å¤šï¼Œç«‹å³åˆ‡æ¢åˆ°å®‰å…¨æ¨¡å¼
                            if asyncio_errors >= max_asyncio_errors:
                                self.logger.error(f"ğŸš¨ å¼‚æ­¥é”™è¯¯è¿‡å¤š ({asyncio_errors}æ¬¡)ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å®‰å…¨æ¨¡å¼")
                                self.config.max_workers = 1
                                # å–æ¶ˆå‰©ä½™çš„Futureå¹¶åˆ‡æ¢åˆ°å•çº¿ç¨‹å¤„ç†
                                for remaining_future in future_to_idx:
                                    if not remaining_future.done():
                                        remaining_future.cancel()
                                
                                # å¤„ç†å‰©ä½™çš„æœªå®Œæˆä»»åŠ¡
                                remaining_items = [batch[i] for i, score in enumerate(batch_scores) if score is None]
                                remaining_start = batch_start_idx + len([s for s in batch_scores if s is not None])
                                
                                if remaining_items:
                                    self.logger.info(f"ğŸ”„ å•çº¿ç¨‹æ¨¡å¼å¤„ç†å‰©ä½™ {len(remaining_items)} ä¸ªä»»åŠ¡...")
                                    for i, qa_item in enumerate(remaining_items):
                                        question_idx = remaining_start + i + 1
                                        safe_result = self._evaluate_single_question(qa_item, question_idx, total_questions, system_name)
                                        batch_scores[remaining_start - batch_start_idx + i] = safe_result
                                
                                break
                        
                        # åˆ›å»ºé”™è¯¯ç»“æœ
                        question_idx = batch_start_idx + idx + 1
                        batch_scores[idx] = {
                            "question": batch[idx].get("question", ""),
                            "scores": {metric: 0.0 for metric in self.config.metrics},
                            "composite_score": 0.0,
                            "error": str(e),
                            "question_idx": question_idx
                        }
                        
                        self.logger.error(f"å¹¶å‘ä»»åŠ¡å¤±è´¥: {e}")
        
        except Exception as e:
            self.logger.error(f"ğŸš¨ å¹¶å‘æ‰§è¡Œä¸¥é‡é”™è¯¯ï¼Œåˆ‡æ¢åˆ°å®‰å…¨æ¨¡å¼: {e}")
            self.config.max_workers = 1
            
            # é‡æ–°å•çº¿ç¨‹å¤„ç†æ•´ä¸ªæ‰¹æ¬¡
            batch_scores = []
            for i, qa_item in enumerate(batch):
                question_idx = batch_start_idx + i + 1
                result = self._evaluate_single_question(qa_item, question_idx, total_questions, system_name)
                batch_scores.append(result)
        
        return batch_scores
    
    def _evaluate_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯„ä¼°ä¸€æ‰¹é—®ç­”å¯¹ï¼ˆä¸ºå…¼å®¹æ€§ä¿ç•™çš„æ–¹æ³•ï¼‰"""
        return self._evaluate_batch_concurrent(batch, 0, "unknown", len(batch))
    
    def _calculate_system_statistics(self, system_name: str, all_scores: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not all_scores:
            return {
                "system_name": system_name,
                "composite_score": 0.0,
                "total_questions": 0,
                "metric_averages": {},
                "metric_std": {},
                "valid_questions": 0
            }
        
        # æå–æ‰€æœ‰æœ‰æ•ˆå¾—åˆ†
        valid_scores = [item for item in all_scores if "error" not in item]
        
        # è®¡ç®—å„æŒ‡æ ‡çš„å¹³å‡å€¼
        metric_sums = {metric: 0.0 for metric in self.config.metrics}
        metric_counts = {metric: 0 for metric in self.config.metrics}
        composite_scores = []
        
        for item in valid_scores:
            scores = item["scores"]
            composite_scores.append(item["composite_score"])
            
            for metric in self.config.metrics:
                if metric in scores and scores[metric] is not None:
                    metric_sums[metric] += scores[metric]
                    metric_counts[metric] += 1
        
        # è®¡ç®—å¹³å‡å€¼
        metric_averages = {}
        metric_std = {}
        
        for metric in self.config.metrics:
            if metric_counts[metric] > 0:
                metric_averages[metric] = metric_sums[metric] / metric_counts[metric]
                
                # è®¡ç®—æ ‡å‡†å·®
                if len(valid_scores) > 1:
                    values = [item["scores"].get(metric, 0) for item in valid_scores 
                             if metric in item["scores"] and item["scores"][metric] is not None]
                    if values:
                        metric_std[metric] = float(np.std(values))
                    else:
                        metric_std[metric] = 0.0
                else:
                    metric_std[metric] = 0.0
            else:
                metric_averages[metric] = 0.0
                metric_std[metric] = 0.0
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        if composite_scores:
            overall_composite = sum(composite_scores) / len(composite_scores)
            composite_std = float(np.std(composite_scores)) if len(composite_scores) > 1 else 0.0
        else:
            overall_composite = 0.0
            composite_std = 0.0
        
        return {
            "system_name": system_name,
            "composite_score": overall_composite,
            "composite_std": composite_std,
            "total_questions": len(all_scores),
            "valid_questions": len(valid_scores),
            "metric_averages": metric_averages,
            "metric_std": metric_std,
            "success_rate": len(valid_scores) / len(all_scores) if all_scores else 0.0
        }
    
    def evaluate_multiple_systems(self, qacg_files: List[str]) -> Dict[str, Any]:
        """
        è¯„ä¼°å¤šä¸ªç³»ç»Ÿå¹¶ç”Ÿæˆæ’å
        
        Args:
            qacg_files: QACGæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            è¯„ä¼°å’Œæ’åç»“æœ
        """
        self.logger.info(f"ğŸš€ å¼€å§‹RAGAS DICEå¤šç³»ç»Ÿè¯„ä¼°")
        self.logger.info(f"ğŸ“ å¾…è¯„ä¼°ç³»ç»Ÿæ•°é‡: {len(qacg_files)}")
        
        # æ˜¾ç¤ºç³»ç»Ÿåˆ—è¡¨
        for i, qacg_file in enumerate(qacg_files, 1):
            system_name = Path(qacg_file).stem.replace("qacg_", "")
            self.logger.info(f"  {i}. {system_name}")
        
        # è¯„ä¼°æ¯ä¸ªç³»ç»Ÿ
        system_results = []
        
        for i, qacg_file in enumerate(qacg_files, 1):
            system_name = Path(qacg_file).stem.replace("qacg_", "")
            
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"ğŸ” è¯„ä¼°ç³»ç»Ÿ {i}/{len(qacg_files)}: {system_name}")
            self.logger.info(f"{'='*80}")
            
            try:
                result = self.evaluate_single_system(qacg_file, system_name)
                system_results.append(result)
                
                self.logger.info(f"âœ… ç³»ç»Ÿ {system_name} è¯„ä¼°å®Œæˆ")
                self.logger.info(f"ğŸ“Š å¾—åˆ†: {result['composite_score']:.4f}")
                
            except Exception as e:
                self.logger.error(f"âŒ ç³»ç»Ÿ {system_name} è¯„ä¼°å¤±è´¥: {e}")
                # æ·»åŠ é»˜è®¤ç»“æœ
                system_results.append({
                    "system_name": system_name,
                    "composite_score": 0.0,
                    "total_questions": 0,
                    "error": str(e)
                })
        
        # ç”Ÿæˆæ’å
        ranking_result = self._generate_ranking(system_results)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        output_file = Path(self.config.output_dir) / "ragas_dice_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(ranking_result, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ å®Œæ•´ç»“æœä¿å­˜è‡³: {output_file}")
        
        return ranking_result
    
    def _generate_ranking(self, system_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç³»ç»Ÿæ’å"""
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        valid_results = [r for r in system_results if "error" not in r]
        error_results = [r for r in system_results if "error" in r]
        
        # æ’åº
        ranked_systems = sorted(valid_results, key=lambda x: x["composite_score"], reverse=True)
        
        # ç”Ÿæˆæ’åä¿¡æ¯
        ranking = []
        for i, result in enumerate(ranked_systems, 1):
            ranking.append({
                "rank": i,
                "system_name": result["system_name"],
                "composite_score": result["composite_score"],
                "composite_std": result.get("composite_std", 0.0),
                "total_questions": result["total_questions"],
                "valid_questions": result.get("valid_questions", result["total_questions"]),
                "success_rate": result.get("success_rate", 1.0),
                "metric_averages": result.get("metric_averages", {})
            })
        
        # æ·»åŠ å¤±è´¥çš„ç³»ç»Ÿ
        for result in error_results:
            ranking.append({
                "rank": len(ranked_systems) + 1,
                "system_name": result["system_name"],
                "composite_score": 0.0,
                "error": result["error"]
            })
        
        return {
            "evaluation_type": "RAGAS_DICE",
            "total_systems": len(system_results),
            "successful_systems": len(valid_results),
            "failed_systems": len(error_results),
            "ranking": ranking,
            "config": {
                "llm_model": self.config.llm_model,
                "metrics": self.config.metrics,
                "batch_size": self.config.batch_size
            }
        }
